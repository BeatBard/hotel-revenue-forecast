{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "✓ TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING CNN-LSTM READY DATASET\n",
      "==================================================\n",
      "✓ Transformed data shape: (1458, 65)\n",
      "✓ Target data shape: (1458, 4)\n",
      "✓ Transformed data columns: 65 features\n",
      "✓ Target data columns: 4 target variables\n",
      "Index(['Year', 'CheckTotal', 'is_zero', 'IsRamadan', 'IsEid', 'IsPreRamadan',\n",
      "       'IsPostRamadan', 'IsLast10Ramadan', 'IsDSF', 'IsSummerEvent',\n",
      "       'IsNationalDay', 'IsNewYear', 'IsMarathon', 'IsGITEX', 'IsAirshow',\n",
      "       'IsFoodFestival', 'IsPreEvent', 'IsPostEvent', 'Month_sin', 'Month_cos',\n",
      "       'DayOfWeek_sin', 'DayOfWeek_cos', 'Meal_Breakfast', 'Meal_Dinner',\n",
      "       'Meal_Lunch', 'Event_Dubai-Airshow', 'Event_Dubai-Food-Festival',\n",
      "       'Event_Dubai-Marathon', 'Event_Dubai-Shopping-Festival',\n",
      "       'Event_Dubai-Summer-Surprises', 'Event_Eid-Adha', 'Event_Flag-Day',\n",
      "       'Event_GITEX-Technology-Week', 'Event_New-Year-Celebrations',\n",
      "       'Event_Normal', 'Event_Post-Dubai-Airshow', 'Event_Post-Dubai-Marathon',\n",
      "       'Event_Post-Eid-Adha', 'Event_Post-Flag-Day',\n",
      "       'Event_Post-GITEX-Technology-Week', 'Event_Post-New-Year-Celebrations',\n",
      "       'Event_Post-Ramadan-Recovery', 'Event_Post-Ramadan-Week1',\n",
      "       'Event_Post-Summer-Event', 'Event_Pre-Commemoration-Day',\n",
      "       'Event_Pre-DSF', 'Event_Pre-Dubai-Airshow',\n",
      "       'Event_Pre-Dubai-Food-Festival', 'Event_Pre-Dubai-Marathon',\n",
      "       'Event_Pre-Eid-Adha', 'Event_Pre-Flag-Day',\n",
      "       'Event_Pre-GITEX-Technology-Week', 'Event_Pre-Ramadan-Early',\n",
      "       'Event_Pre-Ramadan-Late', 'Event_Pre-UAE-National-Day',\n",
      "       'Event_Ramadan-First10Days', 'Event_Ramadan-Last10Days',\n",
      "       'Event_Ramadan-Middle', 'Tourism_0', 'Tourism_1', 'Tourism_2',\n",
      "       'Tourism_3', 'Impact_-1', 'Impact_0', 'Impact_1'],\n",
      "      dtype='object')\n",
      "\n",
      "First 3 rows of transformed data:\n",
      "       Year  CheckTotal   is_zero  IsRamadan     IsEid  IsPreRamadan  \\\n",
      "0 -0.575766    0.015624 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "1 -0.575766    2.072745 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "2 -0.575766   -0.155666 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "\n",
      "   IsPostRamadan  IsLast10Ramadan     IsDSF  IsSummerEvent  ...  \\\n",
      "0      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "1      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "2      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "\n",
      "   Event_Ramadan-First10Days  Event_Ramadan-Last10Days  Event_Ramadan-Middle  \\\n",
      "0                  -0.207168                 -0.207168             -0.201706   \n",
      "1                  -0.207168                 -0.207168             -0.201706   \n",
      "2                  -0.207168                 -0.207168             -0.201706   \n",
      "\n",
      "   Tourism_0  Tourism_1  Tourism_2  Tourism_3  Impact_-1  Impact_0  Impact_1  \n",
      "0  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "1  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "2  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "\n",
      "[3 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load the transformed datasets\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING CNN-LSTM READY DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the datasets\n",
    "df_transformed = pd.read_csv('cnn_lstm_ready_dataset.csv')\n",
    "target_data = pd.read_csv('target_data_for_sequences.csv')\n",
    "\n",
    "print(f\"✓ Transformed data shape: {df_transformed.shape}\")\n",
    "print(f\"✓ Target data shape: {target_data.shape}\")\n",
    "print(f\"✓ Transformed data columns: {len(df_transformed.columns)} features\")\n",
    "print(f\"✓ Target data columns: {len(target_data.columns)} target variables\")\n",
    "print(df_transformed.columns)\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 3 rows of transformed data:\")\n",
    "print(df_transformed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_for_cnn_lstm(df_transformed, target_data, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences for CNN-LSTM training from loaded CSV files\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length  # Look back 30 days\n",
    "    FORECAST_HORIZON = forecast_horizon  # Predict next 7 days\n",
    "    \n",
    "    # Sort by date to ensure proper sequence order\n",
    "    df_transformed_sorted = df_transformed.sort_values('Date').reset_index(drop=True)\n",
    "    target_data_sorted = target_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Pivot target data to wide format\n",
    "    target_pivot = target_data_sorted.pivot_table(\n",
    "        index='Date', \n",
    "        columns=['RevenueCenterName', 'MealPeriod'], \n",
    "        values='CheckTotal', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create column names for revenue streams\n",
    "    target_pivot.columns = ['Date'] + [f\"{col[0]}_{col[1]}\" for col in target_pivot.columns[1:]]\n",
    "    \n",
    "    # Ensure same date range\n",
    "    common_dates = set(df_transformed_sorted['Date']).intersection(set(target_pivot['Date']))\n",
    "    df_transformed_sorted = df_transformed_sorted[df_transformed_sorted['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    target_pivot = target_pivot[target_pivot['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    \n",
    "    # Remove Date column from features\n",
    "    feature_columns = [col for col in df_transformed_sorted.columns if col != 'Date']\n",
    "    features = df_transformed_sorted[feature_columns].values\n",
    "    \n",
    "    # Target columns (revenue targets)\n",
    "    target_columns = [col for col in target_pivot.columns if col != 'Date']\n",
    "    targets = target_pivot[target_columns].values\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Number of feature columns: {len(feature_columns)}\")\n",
    "    print(f\"✓ Number of target columns: {len(target_columns)}\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past 30 days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next 7 days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, 30, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, 7, revenue_targets)\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Target normalization functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3A: Target Normalization Functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "def normalize_targets(y_train, y_test, save_scaler=True):\n",
    "    \"\"\"\n",
    "    Normalize target values for better training stability\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"NORMALIZING TARGET VALUES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Original data info\n",
    "    print(f\"📊 Original target ranges:\")\n",
    "    print(f\"  y_train: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "    print(f\"  y_test: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
    "    \n",
    "    # Reshape for normalization: (samples, days, streams) -> (samples*days, streams)\n",
    "    original_train_shape = y_train.shape\n",
    "    original_test_shape = y_test.shape\n",
    "    \n",
    "    y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])  # (samples*days, 3)\n",
    "    y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])     # (samples*days, 3)\n",
    "    \n",
    "    print(f\"✓ Reshaped for scaling:\")\n",
    "    print(f\"  y_train: {original_train_shape} -> {y_train_reshaped.shape}\")\n",
    "    print(f\"  y_test: {original_test_shape} -> {y_test_reshaped.shape}\")\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_normalized = target_scaler.fit_transform(y_train_reshaped)\n",
    "    y_test_normalized = target_scaler.transform(y_test_reshaped)\n",
    "    \n",
    "    # Reshape back to original format\n",
    "    y_train_normalized = y_train_normalized.reshape(original_train_shape)\n",
    "    y_test_normalized = y_test_normalized.reshape(original_test_shape)\n",
    "    \n",
    "    print(f\"✓ Normalized target ranges:\")\n",
    "    print(f\"  y_train: {y_train_normalized.min():.3f} - {y_train_normalized.max():.3f}\")\n",
    "    print(f\"  y_test: {y_test_normalized.min():.3f} - {y_test_normalized.max():.3f}\")\n",
    "    print(f\"  Mean: {y_train_normalized.mean():.3f}, Std: {y_train_normalized.std():.3f}\")\n",
    "    \n",
    "    # Save scaler for later denormalization\n",
    "    if save_scaler:\n",
    "        joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "        print(f\"✅ Target scaler saved to 'target_scaler.pkl'\")\n",
    "    \n",
    "    return y_train_normalized, y_test_normalized, target_scaler\n",
    "\n",
    "def denormalize_predictions(predictions_normalized, target_scaler):\n",
    "    \"\"\"\n",
    "    Convert normalized predictions back to actual dollar amounts\n",
    "    \"\"\"\n",
    "    original_shape = predictions_normalized.shape\n",
    "    \n",
    "    # Reshape for denormalization\n",
    "    pred_reshaped = predictions_normalized.reshape(-1, predictions_normalized.shape[-1])\n",
    "    \n",
    "    # Denormalize\n",
    "    pred_actual = target_scaler.inverse_transform(pred_reshaped)\n",
    "    \n",
    "    # Reshape back\n",
    "    pred_actual = pred_actual.reshape(original_shape)\n",
    "    \n",
    "    return pred_actual\n",
    "\n",
    "print(\"✅ Target normalization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLEANING AND PREPARING DATA FOR CNN-LSTM\n",
      "==================================================\n",
      "Original data info:\n",
      "df_transformed shape: (1458, 65)\n",
      "df_transformed columns: ['Year', 'CheckTotal', 'is_zero', 'IsRamadan', 'IsEid', 'IsPreRamadan', 'IsPostRamadan', 'IsLast10Ramadan', 'IsDSF', 'IsSummerEvent', 'IsNationalDay', 'IsNewYear', 'IsMarathon', 'IsGITEX', 'IsAirshow', 'IsFoodFestival', 'IsPreEvent', 'IsPostEvent', 'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Meal_Breakfast', 'Meal_Dinner', 'Meal_Lunch', 'Event_Dubai-Airshow', 'Event_Dubai-Food-Festival', 'Event_Dubai-Marathon', 'Event_Dubai-Shopping-Festival', 'Event_Dubai-Summer-Surprises', 'Event_Eid-Adha', 'Event_Flag-Day', 'Event_GITEX-Technology-Week', 'Event_New-Year-Celebrations', 'Event_Normal', 'Event_Post-Dubai-Airshow', 'Event_Post-Dubai-Marathon', 'Event_Post-Eid-Adha', 'Event_Post-Flag-Day', 'Event_Post-GITEX-Technology-Week', 'Event_Post-New-Year-Celebrations', 'Event_Post-Ramadan-Recovery', 'Event_Post-Ramadan-Week1', 'Event_Post-Summer-Event', 'Event_Pre-Commemoration-Day', 'Event_Pre-DSF', 'Event_Pre-Dubai-Airshow', 'Event_Pre-Dubai-Food-Festival', 'Event_Pre-Dubai-Marathon', 'Event_Pre-Eid-Adha', 'Event_Pre-Flag-Day', 'Event_Pre-GITEX-Technology-Week', 'Event_Pre-Ramadan-Early', 'Event_Pre-Ramadan-Late', 'Event_Pre-UAE-National-Day', 'Event_Ramadan-First10Days', 'Event_Ramadan-Last10Days', 'Event_Ramadan-Middle', 'Tourism_0', 'Tourism_1', 'Tourism_2', 'Tourism_3', 'Impact_-1', 'Impact_0', 'Impact_1']\n",
      "target_data shape: (1458, 4)\n",
      "target_data columns: ['Date', 'RevenueCenterName', 'MealPeriod', 'CheckTotal']\n",
      "✓ Data lengths match - assuming already aligned by row index\n",
      "\n",
      "🔄 Pivoting target data to wide format...\n",
      "✓ Pivoted target shape: (486, 4)\n",
      "✓ Pivoted target columns: ['day_id', 'Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "📊 Aggregating features to day level...\n",
      "✓ Aggregated features shape: (486, 65)\n",
      "✓ Final aligned shapes:\n",
      "Features: (486, 65)\n",
      "Targets: (486, 3)\n",
      "\n",
      "🧹 Cleaning data types...\n",
      "✅ Final cleaned data:\n",
      "Features shape: (486, 65)\n",
      "Targets shape: (486, 3)\n",
      "Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "Data lengths match: True\n",
      "\n",
      "==================================================\n",
      "CREATING SEQUENCES FOR CNN-LSTM\n",
      "==================================================\n",
      "✓ Feature shape: (486, 65)\n",
      "✓ Target shape: (486, 3)\n",
      "✓ Sequence length: 30 days\n",
      "✓ Forecast horizon: 7 days\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Final X shape: (450, 30, 65)\n",
      "✓ Final y shape: (450, 7, 3)\n",
      "✓ X dtype: float32\n",
      "✓ y dtype: float32\n",
      "✓ Total sequences created: 450\n",
      "\n",
      "📊 Shape interpretation:\n",
      "X: (450 sequences, 30 days history, 65 features)\n",
      "y: (450 sequences, 7 days forecast, 3 revenue streams)\n",
      "\n",
      "🎉 SUCCESS! Sequences created successfully!\n",
      "✓ Input sequences (X): (450, 30, 65)\n",
      "✓ Output sequences (y): (450, 7, 3)\n",
      "✓ Feature columns: 65\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Data types: X=float32, y=float32\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Corrected - Handle data without Date column in features\n",
    "def clean_and_prepare_data_fixed(df_transformed, target_data):\n",
    "    \"\"\"\n",
    "    Clean dataframes when features don't have Date column\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLEANING AND PREPARING DATA FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Check original data\n",
    "    print(\"Original data info:\")\n",
    "    print(f\"df_transformed shape: {df_transformed.shape}\")\n",
    "    print(f\"df_transformed columns: {list(df_transformed.columns)}\")\n",
    "    print(f\"target_data shape: {target_data.shape}\")\n",
    "    print(f\"target_data columns: {list(target_data.columns)}\")\n",
    "    \n",
    "    # Check if data is already aligned by length\n",
    "    if len(df_transformed) == len(target_data):\n",
    "        print(\"✓ Data lengths match - assuming already aligned by row index\")\n",
    "        \n",
    "        # Step 2: Pivot target data from long to wide format\n",
    "        print(\"\\n🔄 Pivoting target data to wide format...\")\n",
    "        \n",
    "        # Add row index to help with pivoting\n",
    "        target_with_index = target_data.copy()\n",
    "        target_with_index['row_index'] = target_with_index.index\n",
    "        \n",
    "        # Create a day identifier (since we know there are 3 meal periods per day)\n",
    "        target_with_index['day_id'] = target_with_index['row_index'] // 3\n",
    "        \n",
    "        target_pivot = target_with_index.pivot_table(\n",
    "            index='day_id', \n",
    "            columns='MealPeriod', \n",
    "            values='CheckTotal', \n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "        \n",
    "        print(f\"✓ Pivoted target shape: {target_pivot.shape}\")\n",
    "        print(f\"✓ Pivoted target columns: {list(target_pivot.columns)}\")\n",
    "        \n",
    "        # Step 3: Aggregate features to day level (average of 3 meal periods per day)\n",
    "        print(\"\\n📊 Aggregating features to day level...\")\n",
    "        \n",
    "        # Add day_id to features\n",
    "        df_features_with_day = df_transformed.copy()\n",
    "        df_features_with_day['day_id'] = df_features_with_day.index // 3\n",
    "        \n",
    "        # Aggregate features by day (mean of the 3 meal periods)\n",
    "        df_features_daily = df_features_with_day.groupby('day_id').mean().reset_index()\n",
    "        df_features_daily = df_features_daily.drop('day_id', axis=1)\n",
    "        \n",
    "        print(f\"✓ Aggregated features shape: {df_features_daily.shape}\")\n",
    "        \n",
    "        # Step 4: Align the data\n",
    "        target_values = target_pivot.drop('day_id', axis=1)\n",
    "        \n",
    "        # Ensure same number of rows\n",
    "        min_rows = min(len(df_features_daily), len(target_values))\n",
    "        df_features_final = df_features_daily.iloc[:min_rows]\n",
    "        target_values_final = target_values.iloc[:min_rows]\n",
    "        \n",
    "        print(f\"✓ Final aligned shapes:\")\n",
    "        print(f\"Features: {df_features_final.shape}\")\n",
    "        print(f\"Targets: {target_values_final.shape}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Data length mismatch: features={len(df_transformed)}, targets={len(target_data)}\")\n",
    "    \n",
    "    # Step 5: Clean data types and handle missing values\n",
    "    print(\"\\n🧹 Cleaning data types...\")\n",
    "    \n",
    "    # Features: ensure all numeric\n",
    "    df_features_clean = df_features_final.select_dtypes(include=[np.number])\n",
    "    df_features_clean = df_features_clean.fillna(0).astype(np.float32)\n",
    "    \n",
    "    # Targets: ensure all numeric\n",
    "    df_targets_clean = target_values_final.fillna(0).astype(np.float32)\n",
    "    \n",
    "    print(f\"✅ Final cleaned data:\")\n",
    "    print(f\"Features shape: {df_features_clean.shape}\")\n",
    "    print(f\"Targets shape: {df_targets_clean.shape}\")\n",
    "    print(f\"Target columns: {list(df_targets_clean.columns)}\")\n",
    "    print(f\"Data lengths match: {len(df_features_clean) == len(df_targets_clean)}\")\n",
    "    \n",
    "    return df_features_clean, df_targets_clean\n",
    "\n",
    "def create_sequences_for_cnn_lstm_corrected(df_features, df_targets, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences from properly aligned and cleaned data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length\n",
    "    FORECAST_HORIZON = forecast_horizon\n",
    "    \n",
    "    # Convert to arrays\n",
    "    features = df_features.values\n",
    "    targets = df_targets.values\n",
    "    feature_columns = df_features.columns.tolist()\n",
    "    target_columns = df_targets.columns.tolist()\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    \n",
    "    # Verify we have enough data\n",
    "    min_data_needed = SEQ_LENGTH + FORECAST_HORIZON\n",
    "    if len(features) < min_data_needed:\n",
    "        raise ValueError(f\"Not enough data. Need at least {min_data_needed} rows, got {len(features)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past SEQ_LENGTH days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next FORECAST_HORIZON days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, sequence_length, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, forecast_horizon, revenue_streams)\n",
    "    print(f\"✓ X dtype: {X.dtype}\")\n",
    "    print(f\"✓ y dtype: {y.dtype}\")\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    # Show example of what each dimension means\n",
    "    print(f\"\\n📊 Shape interpretation:\")\n",
    "    print(f\"X: ({X.shape[0]} sequences, {X.shape[1]} days history, {X.shape[2]} features)\")\n",
    "    print(f\"y: ({y.shape[0]} sequences, {y.shape[1]} days forecast, {y.shape[2]} revenue streams)\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns\n",
    "\n",
    "# Execute the corrected pipeline\n",
    "try:\n",
    "    # Step 1: Clean and prepare data without Date column dependency\n",
    "    df_features_clean, df_targets_clean = clean_and_prepare_data_fixed(df_transformed, target_data)\n",
    "    \n",
    "    # Step 2: Create sequences\n",
    "    X, y, feature_cols, target_cols = create_sequences_for_cnn_lstm_corrected(\n",
    "        df_features_clean, df_targets_clean\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 SUCCESS! Sequences created successfully!\")\n",
    "    print(f\"✓ Input sequences (X): {X.shape}\")\n",
    "    print(f\"✓ Output sequences (y): {y.shape}\")\n",
    "    print(f\"✓ Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"✓ Target columns: {target_cols}\")\n",
    "    print(f\"✓ Data types: X={X.dtype}, y={y.dtype}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5757663   0.6442341  -0.08310281 ... -0.3115533  -2.6150928\n",
      "    4.957716  ]\n",
      "  [-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.01185533 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.2981966  -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.48062024 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.7368157  -0.7176877  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.35829824 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.2725303  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.03529583 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]]\n",
      "----------------------------------------------------------\n",
      "[[[ 473.  4368.   250. ]\n",
      "  [ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  ...\n",
      "  [1994.  2686.   247. ]\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]]\n",
      "\n",
      " [[ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  ...\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]]\n",
      "\n",
      " [[ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  [1994.  2686.   247. ]\n",
      "  ...\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]\n",
      "  [ 745.  1673.   258. ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 335.  2032.9  809.5]\n",
      "  [ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  ...\n",
      "  [1005.  2840.  1224. ]\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]]\n",
      "\n",
      " [[ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  ...\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]]\n",
      "\n",
      " [[ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  [1005.  2840.  1224. ]\n",
      "  ...\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]\n",
      "  [ 566.  2503.  1242. ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAIN-TEST SPLIT\n",
      "==============================\n",
      "✓ Training sequences: 360\n",
      "✓ Testing sequences: 90\n",
      "✓ Input shape per sample: (30, 65)\n",
      "✓ Output shape per sample: (7, 3)\n",
      "✓ X_train dtype: float32\n",
      "✓ y_train dtype: float32\n",
      "✓ X_test dtype: float32\n",
      "✓ y_test dtype: float32\n",
      "\n",
      "✓ Data quality check:\n",
      "X_train NaN count: 0\n",
      "y_train NaN count: 0\n",
      "X_train Inf count: 0\n",
      "y_train Inf count: 0\n",
      "\n",
      "✅ Data is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train-Test Split with clean data\n",
    "print(\"=\"*30)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Time-based split (80% train, 20% test)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"✓ Training sequences: {X_train.shape[0]}\")\n",
    "print(f\"✓ Testing sequences: {X_test.shape[0]}\")\n",
    "print(f\"✓ Input shape per sample: {X_train.shape[1:]}\")\n",
    "print(f\"✓ Output shape per sample: {y_train.shape[1:]}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"✓ X_train dtype: {X_train.dtype}\")\n",
    "print(f\"✓ y_train dtype: {y_train.dtype}\")\n",
    "print(f\"✓ X_test dtype: {X_test.dtype}\")\n",
    "print(f\"✓ y_test dtype: {y_test.dtype}\")\n",
    "\n",
    "# Check for any problematic values\n",
    "print(f\"\\n✓ Data quality check:\")\n",
    "print(f\"X_train NaN count: {np.isnan(X_train).sum()}\")\n",
    "print(f\"y_train NaN count: {np.isnan(y_train).sum()}\")\n",
    "print(f\"X_train Inf count: {np.isinf(X_train).sum()}\")\n",
    "print(f\"y_train Inf count: {np.isinf(y_train).sum()}\")\n",
    "\n",
    "print(f\"\\n✅ Data is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model building function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define CNN-LSTM model architecture\n",
    "def build_cnn_lstm_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Build CNN-LSTM hybrid model for hotel revenue forecasting\n",
    "    \"\"\"\n",
    "    print(f\"✓ Building model with input shape: {input_shape}\")\n",
    "    print(f\"✓ Output shape: {output_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, name='conv1d_1'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_1'),\n",
    "        Dropout(0.2, name='dropout_1'),\n",
    "        \n",
    "        # More CNN layers\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', name='conv1d_3'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_2'),\n",
    "        Dropout(0.2, name='dropout_2'),\n",
    "        \n",
    "        # LSTM layers for temporal patterns\n",
    "        LSTM(100, return_sequences=True, name='lstm_1'),\n",
    "        Dropout(0.3, name='dropout_3'),\n",
    "        LSTM(50, return_sequences=False, name='lstm_2'),\n",
    "        Dropout(0.3, name='dropout_4'),\n",
    "        \n",
    "        # Dense layers for final prediction\n",
    "        Dense(100, activation='relu', name='dense_1'),\n",
    "        Dropout(0.2, name='dropout_5'),\n",
    "        Dense(np.prod(output_shape), activation='linear', name='dense_output'),\n",
    "    ])\n",
    "    \n",
    "    # Reshape output to (forecast_days, revenue_streams)\n",
    "    model.add(tf.keras.layers.Reshape(output_shape, name='reshape_output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model building function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "APPLYING TARGET NORMALIZATION\n",
      "========================================\n",
      "==================================================\n",
      "NORMALIZING TARGET VALUES\n",
      "==================================================\n",
      "📊 Original target ranges:\n",
      "  y_train: $5.00 - $10052.50\n",
      "  y_test: $66.00 - $9657.00\n",
      "✓ Reshaped for scaling:\n",
      "  y_train: (360, 7, 3) -> (2520, 3)\n",
      "  y_test: (90, 7, 3) -> (630, 3)\n",
      "✓ Normalized target ranges:\n",
      "  y_train: -1.588 - 9.795\n",
      "  y_test: -1.334 - 9.494\n",
      "  Mean: -0.000, Std: 1.000\n",
      "✅ Target scaler saved to 'target_scaler.pkl'\n",
      "✅ Target normalization applied!\n",
      "✅ Training will use normalized targets\n",
      "✅ Original targets preserved for comparison\n",
      "\n",
      "📊 Comparison:\n",
      "Original y_train range: $5.00 - $10052.50\n",
      "Normalized y_train range: -1.588 - 9.795\n"
     ]
    }
   ],
   "source": [
    "# Cell 6A: Apply Target Normalization\n",
    "print(\"=\"*40)\n",
    "print(\"APPLYING TARGET NORMALIZATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Store original targets for comparison\n",
    "y_train_original = y_train.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "# Apply normalization\n",
    "y_train_norm, y_test_norm, target_scaler = normalize_targets(y_train, y_test, save_scaler=True)\n",
    "\n",
    "# Update variables for training\n",
    "y_train = y_train_norm\n",
    "y_test = y_test_norm\n",
    "\n",
    "print(f\"✅ Target normalization applied!\")\n",
    "print(f\"✅ Training will use normalized targets\")\n",
    "print(f\"✅ Original targets preserved for comparison\")\n",
    "\n",
    "# Show the difference\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"Original y_train range: ${y_train_original.min():.2f} - ${y_train_original.max():.2f}\")\n",
    "print(f\"Normalized y_train range: {y_train.min():.3f} - {y_train.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "BUILDING MODEL\n",
      "==============================\n",
      "✓ Building model with input shape: (30, 65)\n",
      "✓ Output shape: (7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "MODEL ARCHITECTURE\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,121</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │        \u001b[38;5;34m53,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m5,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │         \u001b[38;5;34m2,121\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (\u001b[38;5;33mReshape\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Total parameters: 121,693\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build and compile the model\n",
    "print(\"=\"*30)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (30, features)\n",
    "output_shape = (y_train.shape[1], y_train.shape[2])  # (7, revenue_streams)\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_lstm_model(input_shape, output_shape)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n✓ Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAINING SETUP\n",
      "==============================\n",
      "✓ Callbacks configured:\n",
      "  - Early stopping (patience=15)\n",
      "  - Learning rate reduction (factor=0.5, patience=5)\n",
      "  - Model checkpoint (best_cnn_lstm_model.h5)\n",
      "✓ Batch size: 32\n",
      "✓ Max epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup training callbacks\n",
    "print(\"=\"*30)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=15, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_lstm_model.h5', \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "print(\"✓ Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=15)\")\n",
    "print(\"  - Learning rate reduction (factor=0.5, patience=5)\")\n",
    "print(\"  - Model checkpoint (best_cnn_lstm_model.h5)\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "COMPREHENSIVE DATA PREPARATION\n",
      "========================================\n",
      "🔧 Cleaning and preparing data...\n",
      "🧹 Handling NaN and infinite values...\n",
      "✓ Final data types:\n",
      "  X_train: float32, shape: (360, 30, 65)\n",
      "  y_train: float32, shape: (360, 7, 3)\n",
      "  X_test: float32, shape: (90, 30, 65)\n",
      "  y_test: float32, shape: (90, 7, 3)\n",
      "✓ Data ranges:\n",
      "  X_train: [-2.615, 22.023]\n",
      "  y_train: [-1.588, 9.795]\n",
      "\n",
      "==============================\n",
      "STARTING TRAINING\n",
      "==============================\n",
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9665 - mae: 0.6664 \n",
      "Epoch 1: val_loss improved from inf to 3.43391, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - loss: 0.9689 - mae: 0.6665 - val_loss: 3.4339 - val_mae: 1.3141 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7425 - mae: 0.6068 \n",
      "Epoch 2: val_loss improved from 3.43391 to 3.04964, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8072 - mae: 0.6184 - val_loss: 3.0496 - val_mae: 1.2144 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8319 - mae: 0.6087\n",
      "Epoch 3: val_loss improved from 3.04964 to 2.46417, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8221 - mae: 0.6059 - val_loss: 2.4642 - val_mae: 1.0717 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6036 - mae: 0.5449\n",
      "Epoch 4: val_loss improved from 2.46417 to 2.25386, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6289 - mae: 0.5532 - val_loss: 2.2539 - val_mae: 1.0263 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8036 - mae: 0.6067\n",
      "Epoch 5: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7809 - mae: 0.6014 - val_loss: 2.3177 - val_mae: 1.0313 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6832 - mae: 0.5590\n",
      "Epoch 6: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6840 - mae: 0.5599 - val_loss: 2.2775 - val_mae: 1.0194 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7059 - mae: 0.5670 \n",
      "Epoch 7: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7049 - mae: 0.5669 - val_loss: 2.7672 - val_mae: 1.1201 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7838 - mae: 0.5843\n",
      "Epoch 8: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7762 - mae: 0.5827 - val_loss: 2.2905 - val_mae: 1.0176 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m11/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6671 - mae: 0.5609\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6679 - mae: 0.5610 - val_loss: 2.3908 - val_mae: 1.0376 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6598 - mae: 0.5575 \n",
      "Epoch 10: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6721 - mae: 0.5612 - val_loss: 2.3306 - val_mae: 1.0250 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6631 - mae: 0.5600 \n",
      "Epoch 11: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6688 - mae: 0.5598 - val_loss: 2.2932 - val_mae: 1.0171 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7363 - mae: 0.5797 \n",
      "Epoch 12: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7027 - mae: 0.5705 - val_loss: 2.3539 - val_mae: 1.0299 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6810 - mae: 0.5632\n",
      "Epoch 13: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6746 - mae: 0.5600 - val_loss: 2.3291 - val_mae: 1.0291 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6555 - mae: 0.5475 \n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6712 - mae: 0.5529 - val_loss: 2.3022 - val_mae: 1.0213 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m11/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6735 - mae: 0.5541\n",
      "Epoch 15: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6724 - mae: 0.5546 - val_loss: 2.2824 - val_mae: 1.0158 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6890 - mae: 0.5615\n",
      "Epoch 16: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6789 - mae: 0.5586 - val_loss: 2.3255 - val_mae: 1.0262 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6285 - mae: 0.5507\n",
      "Epoch 17: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6377 - mae: 0.5516 - val_loss: 2.3099 - val_mae: 1.0213 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6381 - mae: 0.5517\n",
      "Epoch 18: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6569 - mae: 0.5549 - val_loss: 2.2704 - val_mae: 1.0110 - learning_rate: 2.5000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6660 - mae: 0.5476\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6662 - mae: 0.5498 - val_loss: 2.2904 - val_mae: 1.0154 - learning_rate: 2.5000e-04\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "✅ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 Alternative: Comprehensive data cleaning and training\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"COMPREHENSIVE DATA PREPARATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def clean_and_prepare_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning for CNN-LSTM training\n",
    "    \"\"\"\n",
    "    print(\"🔧 Cleaning and preparing data...\")\n",
    "    \n",
    "    # Convert to numpy arrays if not already\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Check for object dtype issues\n",
    "    if X_train.dtype == 'object':\n",
    "        print(\"⚠️  X_train has object dtype - converting...\")\n",
    "        X_train = X_train.astype(np.float64)\n",
    "    \n",
    "    if y_train.dtype == 'object':\n",
    "        print(\"⚠️  y_train has object dtype - converting...\")\n",
    "        y_train = y_train.astype(np.float64)\n",
    "    \n",
    "    if X_test.dtype == 'object':\n",
    "        print(\"⚠️  X_test has object dtype - converting...\")\n",
    "        X_test = X_test.astype(np.float64)\n",
    "    \n",
    "    if y_test.dtype == 'object':\n",
    "        print(\"⚠️  y_test has object dtype - converting...\")\n",
    "        y_test = y_test.astype(np.float64)\n",
    "    \n",
    "    # Handle NaN and infinite values\n",
    "    print(\"🧹 Handling NaN and infinite values...\")\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Convert to float32 (TensorFlow's preferred type)\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Final verification\n",
    "    print(f\"✓ Final data types:\")\n",
    "    print(f\"  X_train: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.dtype}, shape: {y_train.shape}\")\n",
    "    print(f\"  X_test: {X_test.dtype}, shape: {X_test.shape}\")\n",
    "    print(f\"  y_test: {y_test.dtype}, shape: {y_test.shape}\")\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(f\"✓ Data ranges:\")\n",
    "    print(f\"  X_train: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "    print(f\"  y_train: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Clean the data\n",
    "X_train_clean, y_train_clean, X_test_clean, y_test_clean = clean_and_prepare_data(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Train with cleaned data\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train_clean, y_train_clean,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_test_clean, y_test_clean),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    \n",
    "    # Update variables for next cells\n",
    "    X_train, y_train = X_train_clean, y_train_clean\n",
    "    X_test, y_test = X_test_clean, y_test_clean\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training still failed: {e}\")\n",
    "    print(\"\\n🔍 Additional debugging:\")\n",
    "    \n",
    "    # More detailed debugging\n",
    "    print(f\"X_train unique dtypes: {set(str(x.dtype) for x in X_train.flatten()[:100])}\")\n",
    "    print(f\"Sample X_train values: {X_train[0, 0, :10]}\")\n",
    "    print(f\"Sample y_train values: {y_train[0, 0, :10]}\")\n",
    "    \n",
    "    # Check if data contains any strings\n",
    "    sample_x = X_train[0, 0, :]\n",
    "    print(f\"Sample X contains strings: {any(isinstance(x, str) for x in sample_x.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "==============================\n",
      "📊 NOTE: Model trained on normalized targets\n",
      "📊 Denormalizing predictions to actual dollar amounts\n",
      "✅ Predictions denormalized successfully\n",
      "\n",
      "🔍 Shape Debugging:\n",
      "X_test shape: (90, 30, 65)\n",
      "y_test_actual shape: (90, 7, 3)\n",
      "y_pred_actual shape: (90, 7, 3)\n",
      "\n",
      "📊 Revenue streams: ['Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "💰 Denormalized Value Ranges:\n",
      "  Actual revenue: $66.00 - $9657.00\n",
      "  Predicted revenue: $481.21 - $5506.60\n",
      "\n",
      "✅ Overall Test Metrics (in USD):\n",
      "  MAE: $870.10\n",
      "  RMSE: $1273.38\n",
      "  MAPE: 75.66%\n",
      "\n",
      "✅ Performance by Revenue Stream:\n",
      "  Breakfast: MAE = $776.84, Correlation = 0.484\n",
      "  Dinner: MAE = $1218.92, Correlation = 0.678\n",
      "  Lunch: MAE = $614.53, Correlation = 0.546\n",
      "\n",
      "✅ Sample Predictions (First sequence - in USD):\n",
      "Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\n",
      "-----------------------------------------------------------------------------------------------\n",
      " 1  | $    2466.00     | $  1767.07     | $ 6548.00     | $5111.87     | $2912.00     | $1255.50\n",
      " 2  | $    2586.80     | $  2115.33     | $ 4300.00     | $5330.62     | $2686.00     | $1078.78\n",
      " 3  | $    1639.60     | $  1803.46     | $ 6378.00     | $4956.40     | $2600.00     | $1613.31\n",
      " 4  | $    2079.60     | $  1908.16     | $ 6710.00     | $4726.28     | $2188.00     | $1252.44\n",
      " 5  | $    1232.00     | $  2034.98     | $ 4523.00     | $5506.60     | $2206.00     | $1357.37\n",
      " 6  | $    1390.00     | $  2018.57     | $ 3990.00     | $5366.19     | $1710.00     | $1633.34\n",
      " 7  | $    1426.00     | $  2167.11     | $ 5762.00     | $4556.55     | $ 682.00     | $1353.80\n",
      "\n",
      "✅ Evaluation complete with denormalized predictions!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Model Evaluation with Denormalization\n",
    "print(\"=\"*30)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# IMPORTANT: Model was trained on NORMALIZED targets\n",
    "# We need to denormalize predictions for evaluation\n",
    "print(\"📊 NOTE: Model trained on normalized targets\")\n",
    "print(\"📊 Denormalizing predictions to actual dollar amounts\")\n",
    "\n",
    "# Make predictions on normalized test set\n",
    "y_pred_normalized = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Load scaler and denormalize predictions\n",
    "try:\n",
    "    target_scaler = joblib.load('target_scaler.pkl')\n",
    "    y_pred_actual = denormalize_predictions(y_pred_normalized, target_scaler)\n",
    "    y_test_actual = y_test_original  # Use original non-normalized test targets\n",
    "    \n",
    "    print(f\"✅ Predictions denormalized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load scaler: {e}\")\n",
    "    print(f\"📊 Using normalized predictions for evaluation\")\n",
    "    y_pred_actual = y_pred_normalized\n",
    "    y_test_actual = y_test\n",
    "\n",
    "# Debug shapes\n",
    "print(f\"\\n🔍 Shape Debugging:\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test_actual shape: {y_test_actual.shape}\")\n",
    "print(f\"y_pred_actual shape: {y_pred_actual.shape}\")\n",
    "\n",
    "# Define revenue stream names\n",
    "revenue_streams = ['Breakfast', 'Dinner', 'Lunch']\n",
    "print(f\"\\n📊 Revenue streams: {revenue_streams}\")\n",
    "\n",
    "# Show data ranges (should be in dollars after denormalization)\n",
    "print(f\"\\n💰 Denormalized Value Ranges:\")\n",
    "print(f\"  Actual revenue: ${y_test_actual.min():.2f} - ${y_test_actual.max():.2f}\")\n",
    "print(f\"  Predicted revenue: ${y_pred_actual.min():.2f} - ${y_pred_actual.max():.2f}\")\n",
    "\n",
    "# Calculate metrics on actual dollar amounts\n",
    "y_test_flat = y_test_actual.reshape(-1)\n",
    "y_pred_flat = y_pred_actual.reshape(-1)\n",
    "\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test_flat - y_pred_flat) / (np.abs(y_test_flat) + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n✅ Overall Test Metrics (in USD):\")\n",
    "print(f\"  MAE: ${mae:.2f}\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Performance by revenue stream\n",
    "print(f\"\\n✅ Performance by Revenue Stream:\")\n",
    "for stream_idx, stream_name in enumerate(revenue_streams):\n",
    "    stream_mae = mean_absolute_error(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1), \n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )\n",
    "    stream_corr = np.corrcoef(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1),\n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )[0, 1]\n",
    "    print(f\"  {stream_name}: MAE = ${stream_mae:.2f}, Correlation = {stream_corr:.3f}\")\n",
    "\n",
    "# Sample predictions\n",
    "print(f\"\\n✅ Sample Predictions (First sequence - in USD):\")\n",
    "print(\"Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\")\n",
    "print(\"-\" * 95)\n",
    "for day in range(min(7, y_test_actual.shape[1])):\n",
    "    print(f\"{day+1:2d}  | ${y_test_actual[0, day, 0]:11.2f}     | ${y_pred_actual[0, day, 0]:9.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 1]:8.2f}     | ${y_pred_actual[0, day, 1]:6.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 2]:7.2f}     | ${y_pred_actual[0, day, 2]:5.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete with denormalized predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code is to check the most relavent features and remove the unwanted ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE RELEVANCE ANALYSIS FOR CNN-LSTM MODEL\n",
      "================================================================================\n",
      "Analyzing data shapes:\n",
      "X shape: (450, 30, 65)\n",
      "y shape: (450, 7, 3)\n",
      "Number of features: 65\n",
      "Using normalized targets for analysis\n",
      "Analysis target shape: (360, 7, 3)\n",
      "\n",
      "Flattening sequences for feature analysis...\n",
      "Flattened shapes: X_flat=(10800, 65), y_flat=(2520, 3)\n",
      "ERROR: Shape mismatch detected!\n",
      "X_flat samples: 10800\n",
      "y_flat samples: 2520\n",
      "Fixed shapes: X_flat=(2520, 65), y_flat=(2520, 3)\n",
      "\n",
      "============================================================\n",
      "1. RANDOM FOREST FEATURE IMPORTANCE\n",
      "============================================================\n",
      "\n",
      "Analyzing Breakfast revenue...\n",
      "Top 10 features for Breakfast:\n",
      "  20. Month_cos                           0.4697\n",
      "   2. CheckTotal                          0.1208\n",
      "  59. Tourism_0                           0.1020\n",
      "  21. DayOfWeek_sin                       0.0752\n",
      "  22. DayOfWeek_cos                       0.0382\n",
      "  63. Impact_-1                           0.0325\n",
      "  56. Event_Ramadan-First10Days           0.0312\n",
      "   8. IsLast10Ramadan                     0.0280\n",
      "  57. Event_Ramadan-Last10Days            0.0253\n",
      "  64. Impact_0                            0.0207\n",
      "\n",
      "Analyzing Dinner revenue...\n",
      "Top 10 features for Dinner:\n",
      "  20. Month_cos                           0.3493\n",
      "  60. Tourism_1                           0.2563\n",
      "   2. CheckTotal                          0.1075\n",
      "  63. Impact_-1                           0.0572\n",
      "  21. DayOfWeek_sin                       0.0571\n",
      "  64. Impact_0                            0.0450\n",
      "  59. Tourism_0                           0.0281\n",
      "  22. DayOfWeek_cos                       0.0254\n",
      "  56. Event_Ramadan-First10Days           0.0113\n",
      "  43. Event_Post-Ramadan-Week1            0.0104\n",
      "\n",
      "Analyzing Lunch revenue...\n",
      "Top 10 features for Lunch:\n",
      "  20. Month_cos                           0.4509\n",
      "   2. CheckTotal                          0.2097\n",
      "  21. DayOfWeek_sin                       0.1286\n",
      "  22. DayOfWeek_cos                       0.0300\n",
      "  59. Tourism_0                           0.0298\n",
      "  63. Impact_-1                           0.0296\n",
      "  58. Event_Ramadan-Middle                0.0282\n",
      "  64. Impact_0                            0.0168\n",
      "  60. Tourism_1                           0.0138\n",
      "  56. Event_Ramadan-First10Days           0.0101\n",
      "\n",
      "============================================================\n",
      "2. CORRELATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Calculating correlations for Breakfast...\n",
      "Top 10 correlations for Breakfast:\n",
      "  59. Tourism_0                           0.3227\n",
      "  60. Tourism_1                           0.3220\n",
      "  57. Event_Ramadan-Last10Days            0.2981\n",
      "   8. IsLast10Ramadan                     0.2981\n",
      "  20. Month_cos                           0.2613\n",
      "  63. Impact_-1                           0.2030\n",
      "  64. Impact_0                            0.2024\n",
      "  58. Event_Ramadan-Middle                0.1949\n",
      "   4. IsRamadan                           0.1931\n",
      "   2. CheckTotal                          0.1663\n",
      "\n",
      "Calculating correlations for Dinner...\n",
      "Top 10 correlations for Dinner:\n",
      "  60. Tourism_1                           0.3613\n",
      "  59. Tourism_0                           0.3602\n",
      "   8. IsLast10Ramadan                     0.2997\n",
      "  57. Event_Ramadan-Last10Days            0.2997\n",
      "  20. Month_cos                           0.2654\n",
      "  64. Impact_0                            0.2437\n",
      "  63. Impact_-1                           0.2425\n",
      "  58. Event_Ramadan-Middle                0.2018\n",
      "   4. IsRamadan                           0.1936\n",
      "   2. CheckTotal                          0.1755\n",
      "\n",
      "Calculating correlations for Lunch...\n",
      "Top 10 correlations for Lunch:\n",
      "  59. Tourism_0                           0.2359\n",
      "  60. Tourism_1                           0.2349\n",
      "  20. Month_cos                           0.2053\n",
      "  57. Event_Ramadan-Last10Days            0.1980\n",
      "   8. IsLast10Ramadan                     0.1980\n",
      "  63. Impact_-1                           0.1580\n",
      "  64. Impact_0                            0.1570\n",
      "  58. Event_Ramadan-Middle                0.1461\n",
      "   4. IsRamadan                           0.1373\n",
      "   2. CheckTotal                          0.1164\n",
      "\n",
      "============================================================\n",
      "3. FEATURE VARIANCE ANALYSIS\n",
      "============================================================\n",
      "Top 10 features by variance:\n",
      "  27. Event_Dubai-Food-Festival           5.3811\n",
      "  16. IsFoodFestival                      4.7036\n",
      "  48. Event_Pre-Dubai-Food-Festival       4.5913\n",
      "  54. Event_Pre-Ramadan-Late              2.7304\n",
      "  53. Event_Pre-Ramadan-Early             2.5581\n",
      "  56. Event_Ramadan-First10Days           2.4402\n",
      "   6. IsPreRamadan                        2.3749\n",
      "  63. Impact_-1                           1.7677\n",
      "  58. Event_Ramadan-Middle                1.6013\n",
      "  59. Tourism_0                           1.5332\n",
      "\n",
      "Bottom 10 features by variance (potentially redundant):\n",
      "  51. Event_Pre-Flag-Day                  0.0000\n",
      "  45. Event_Pre-Commemoration-Day         0.0000\n",
      "  55. Event_Pre-UAE-National-Day          0.0000\n",
      "  32. Event_Flag-Day                      0.0000\n",
      "  11. IsNationalDay                       0.0000\n",
      "  36. Event_Post-Dubai-Airshow            0.0000\n",
      "  40. Event_Post-GITEX-Technology-Week    0.0000\n",
      "  25. Meal_Lunch                          0.0000\n",
      "  24. Meal_Dinner                         0.0000\n",
      "  23. Meal_Breakfast                      0.0000\n",
      "\n",
      "============================================================\n",
      "4. COMBINED FEATURE RANKING\n",
      "============================================================\n",
      "\n",
      "Combined analysis for Breakfast:\n",
      "Top 15 features by combined ranking:\n",
      "   3. Tourism_0                           Rank: 4.7\n",
      "   6. Impact_-1                           Rank: 6.7\n",
      "   7. Event_Ramadan-First10Days           Rank: 8.7\n",
      "  10. Impact_0                            Rank: 9.7\n",
      "   9. Event_Ramadan-Last10Days            Rank: 10.0\n",
      "   1. Month_cos                           Rank: 10.3\n",
      "   8. IsLast10Ramadan                     Rank: 10.3\n",
      "  16. Event_Ramadan-Middle                Rank: 11.0\n",
      "  11. Tourism_1                           Rank: 11.7\n",
      "   2. CheckTotal                          Rank: 13.7\n",
      "  22. IsFoodFestival                      Rank: 14.0\n",
      "  19. Event_Dubai-Food-Festival           Rank: 14.3\n",
      "  20. Event_Normal                        Rank: 15.0\n",
      "  12. Event_Pre-Ramadan-Late              Rank: 15.0\n",
      "  30. Event_Pre-Ramadan-Early             Rank: 16.3\n",
      "\n",
      "Combined analysis for Dinner:\n",
      "Top 15 features by combined ranking:\n",
      "   4. Impact_-1                           Rank: 6.3\n",
      "   7. Tourism_0                           Rank: 6.3\n",
      "   6. Impact_0                            Rank: 8.0\n",
      "   2. Tourism_1                           Rank: 8.3\n",
      "   9. Event_Ramadan-First10Days           Rank: 9.3\n",
      "  12. Event_Ramadan-Middle                Rank: 9.7\n",
      "   1. Month_cos                           Rank: 10.3\n",
      "  14. Event_Ramadan-Last10Days            Rank: 12.0\n",
      "  18. Event_Pre-Ramadan-Early             Rank: 12.7\n",
      "  16. IsLast10Ramadan                     Rank: 12.7\n",
      "   3. CheckTotal                          Rank: 14.0\n",
      "  22. Event_Dubai-Food-Festival           Rank: 14.3\n",
      "  17. Event_Pre-Ramadan-Late              Rank: 14.7\n",
      "  21. Event_Normal                        Rank: 15.3\n",
      "  25. IsFoodFestival                      Rank: 15.3\n",
      "\n",
      "Combined analysis for Lunch:\n",
      "Top 15 features by combined ranking:\n",
      "   5. Tourism_0                           Rank: 5.3\n",
      "   6. Impact_-1                           Rank: 6.7\n",
      "   7. Event_Ramadan-Middle                Rank: 8.0\n",
      "   8. Impact_0                            Rank: 9.0\n",
      "   1. Month_cos                           Rank: 9.7\n",
      "  10. Event_Ramadan-First10Days           Rank: 10.3\n",
      "   9. Tourism_1                           Rank: 11.0\n",
      "  12. Event_Pre-Ramadan-Late              Rank: 11.7\n",
      "  15. IsLast10Ramadan                     Rank: 13.0\n",
      "  16. IsPreRamadan                        Rank: 13.0\n",
      "   2. CheckTotal                          Rank: 13.7\n",
      "  18. IsPreEvent                          Rank: 14.3\n",
      "  20. Event_Normal                        Rank: 15.0\n",
      "  21. Event_Dubai-Food-Festival           Rank: 15.0\n",
      "  22. Event_Pre-Ramadan-Early             Rank: 15.0\n",
      "\n",
      "============================================================\n",
      "5. FEATURE CATEGORY ANALYSIS\n",
      "============================================================\n",
      "Feature count by category:\n",
      "  Events         : 33 features\n",
      "  Event_Flags    : 15 features\n",
      "  Cyclical       :  4 features\n",
      "  Tourism        :  4 features\n",
      "  Core           :  3 features\n",
      "  Meal_Period    :  3 features\n",
      "  Revenue_Impact :  3 features\n",
      "\n",
      "============================================================\n",
      "6. FEATURE REDUCTION RECOMMENDATIONS\n",
      "============================================================\n",
      "RECOMMENDED FEATURE SET (22 features):\n",
      "Features ranked in top 20 for at least one revenue stream:\n",
      "   1. IsRamadan                           ★★★ (3/3)\n",
      "   2. Tourism_0                           ★★★ (3/3)\n",
      "   3. Month_cos                           ★★★ (3/3)\n",
      "   4. IsFoodFestival                      ★★★ (3/3)\n",
      "   5. Event_Ramadan-Middle                ★★★ (3/3)\n",
      "   6. IsPreRamadan                        ★★★ (3/3)\n",
      "   7. Impact_0                            ★★★ (3/3)\n",
      "   8. DayOfWeek_sin                       ★★★ (3/3)\n",
      "   9. Event_Normal                        ★★★ (3/3)\n",
      "  10. Event_Pre-Ramadan-Late              ★★★ (3/3)\n",
      "  11. IsLast10Ramadan                     ★★★ (3/3)\n",
      "  12. Impact_-1                           ★★★ (3/3)\n",
      "  13. Event_Pre-Ramadan-Early             ★★★ (3/3)\n",
      "  14. Event_Ramadan-First10Days           ★★★ (3/3)\n",
      "  15. Tourism_1                           ★★★ (3/3)\n",
      "  16. Event_Dubai-Food-Festival           ★★★ (3/3)\n",
      "  17. CheckTotal                          ★★★ (3/3)\n",
      "  18. Event_Ramadan-Last10Days            ★★★ (3/3)\n",
      "  19. is_zero                             ★★ (2/3)\n",
      "  20. IsPreEvent                          ★★ (2/3)\n",
      "  21. DayOfWeek_cos                       ★ (1/3)\n",
      "  22. Event_Pre-Dubai-Food-Festival       ★ (1/3)\n",
      "\n",
      "============================================================\n",
      "7. FEATURES LIKELY TO REMOVE\n",
      "============================================================\n",
      "Features consistently ranked low:\n",
      "  - Event_Dubai-Airshow                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Dubai-Shopping-Festival       (bottom 15 for 3/3 targets)\n",
      "  - Event_Dubai-Summer-Surprises        (bottom 15 for 3/3 targets)\n",
      "  - Event_Eid-Adha                      (bottom 15 for 3/3 targets)\n",
      "  - Event_Flag-Day                      (bottom 15 for 3/3 targets)\n",
      "  - Event_GITEX-Technology-Week         (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Dubai-Airshow            (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Eid-Adha                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Flag-Day                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-GITEX-Technology-Week    (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Ramadan-Recovery         (bottom 15 for 3/3 targets)\n",
      "  - IsNationalDay                       (bottom 15 for 3/3 targets)\n",
      "  - Meal_Breakfast                      (bottom 15 for 3/3 targets)\n",
      "  - Meal_Dinner                         (bottom 15 for 3/3 targets)\n",
      "  - Meal_Lunch                          (bottom 15 for 3/3 targets)\n",
      "\n",
      "============================================================\n",
      "8. SUMMARY AND RECOMMENDATIONS\n",
      "============================================================\n",
      "Current model:\n",
      "  Total features: 65\n",
      "  Training sequences: 360\n",
      "  Samples per feature: 38.8\n",
      "\n",
      "Recommended optimization:\n",
      "  Keep top features: 22\n",
      "  Remove features: 43\n",
      "  Reduction: 66.2%\n",
      "  New samples per feature: 114.5\n",
      "\n",
      "TOP 22 FEATURES TO KEEP:\n",
      "   1. IsRamadan\n",
      "   2. Tourism_0\n",
      "   3. Month_cos\n",
      "   4. IsFoodFestival\n",
      "   5. Event_Ramadan-Middle\n",
      "   6. IsPreRamadan\n",
      "   7. Impact_0\n",
      "   8. DayOfWeek_sin\n",
      "   9. Event_Normal\n",
      "  10. Event_Pre-Ramadan-Late\n",
      "  11. IsLast10Ramadan\n",
      "  12. Impact_-1\n",
      "  13. Event_Pre-Ramadan-Early\n",
      "  14. Event_Ramadan-First10Days\n",
      "  15. Tourism_1\n",
      "  16. Event_Dubai-Food-Festival\n",
      "  17. CheckTotal\n",
      "  18. Event_Ramadan-Last10Days\n",
      "  19. is_zero\n",
      "  20. IsPreEvent\n",
      "  21. DayOfWeek_cos\n",
      "  22. Event_Pre-Dubai-Food-Festival\n",
      "\n",
      "============================================================\n",
      "9. CODE TO IMPLEMENT FEATURE REDUCTION\n",
      "============================================================\n",
      "# Copy this code to implement feature reduction:\n",
      "recommended_features = ['IsRamadan', 'Tourism_0', 'Month_cos', 'IsFoodFestival', 'Event_Ramadan-Middle', 'IsPreRamadan', 'Impact_0', 'DayOfWeek_sin', 'Event_Normal', 'Event_Pre-Ramadan-Late', 'IsLast10Ramadan', 'Impact_-1', 'Event_Pre-Ramadan-Early', 'Event_Ramadan-First10Days', 'Tourism_1', 'Event_Dubai-Food-Festival', 'CheckTotal', 'Event_Ramadan-Last10Days', 'is_zero', 'IsPreEvent', 'DayOfWeek_cos', 'Event_Pre-Dubai-Food-Festival']\n",
      "\n",
      "# Get indices of recommended features\n",
      "feature_indices = [feature_cols.index(f) for f in recommended_features if f in feature_cols]\n",
      "print(f'Found {len(feature_indices)} feature indices')\n",
      "\n",
      "# Reduce your dataset\n",
      "X_train_reduced = X_train[:, :, feature_indices]\n",
      "X_test_reduced = X_test[:, :, feature_indices]\n",
      "feature_cols_reduced = [feature_cols[i] for i in feature_indices]\n",
      "print(f'Reduced from {X_train.shape} to {X_train_reduced.shape}')\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "Use the recommended_features list above to reduce your feature set.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FEATURE RELEVANCE ANALYSIS - Fixed version for your CNN-LSTM notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE RELEVANCE ANALYSIS FOR CNN-LSTM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Debug the data shapes first\n",
    "print(f\"Analyzing data shapes:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Check if we're using normalized targets\n",
    "if 'y_train_norm' in locals():\n",
    "    print(\"Using normalized targets for analysis\")\n",
    "    y_analysis = y_train_norm\n",
    "else:\n",
    "    print(\"Using original targets for analysis\")\n",
    "    y_analysis = y_train\n",
    "\n",
    "print(f\"Analysis target shape: {y_analysis.shape}\")\n",
    "\n",
    "# Flatten the data for analysis - CORRECTED VERSION\n",
    "print(\"\\nFlattening sequences for feature analysis...\")\n",
    "\n",
    "# Use only training data for consistency\n",
    "X_flat = X_train.reshape(-1, X_train.shape[-1])  # (train_sequences*timesteps, features)\n",
    "y_flat = y_analysis.reshape(-1, y_analysis.shape[-1])  # (train_sequences*timesteps, targets)\n",
    "\n",
    "print(f\"Flattened shapes: X_flat={X_flat.shape}, y_flat={y_flat.shape}\")\n",
    "\n",
    "# Verify shapes match\n",
    "if X_flat.shape[0] != y_flat.shape[0]:\n",
    "    print(f\"ERROR: Shape mismatch detected!\")\n",
    "    print(f\"X_flat samples: {X_flat.shape[0]}\")\n",
    "    print(f\"y_flat samples: {y_flat.shape[0]}\")\n",
    "    \n",
    "    # Fix by using the minimum length\n",
    "    min_samples = min(X_flat.shape[0], y_flat.shape[0])\n",
    "    X_flat = X_flat[:min_samples]\n",
    "    y_flat = y_flat[:min_samples]\n",
    "    print(f\"Fixed shapes: X_flat={X_flat.shape}, y_flat={y_flat.shape}\")\n",
    "\n",
    "target_names = ['Breakfast', 'Dinner', 'Lunch']\n",
    "\n",
    "# 1. RANDOM FOREST FEATURE IMPORTANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. RANDOM FOREST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_results = {}\n",
    "for i, target_name in enumerate(target_names):\n",
    "    print(f\"\\nAnalyzing {target_name} revenue...\")\n",
    "    \n",
    "    try:\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=50,  # Reduced for speed\n",
    "            random_state=42, \n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_flat, y_flat[:, i])\n",
    "        \n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        rf_results[target_name] = rf_importance\n",
    "        \n",
    "        print(f\"Top 10 features for {target_name}:\")\n",
    "        for idx, row in rf_importance.head(10).iterrows():\n",
    "            print(f\"  {idx+1:2d}. {row['feature']:<35} {row['importance']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Random Forest for {target_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 2. CORRELATION ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "corr_results = {}\n",
    "for i, target_name in enumerate(target_names):\n",
    "    print(f\"\\nCalculating correlations for {target_name}...\")\n",
    "    \n",
    "    try:\n",
    "        correlations = []\n",
    "        for j in range(len(feature_cols)):\n",
    "            # Handle any NaN values\n",
    "            feature_vals = X_flat[:, j]\n",
    "            target_vals = y_flat[:, i]\n",
    "            \n",
    "            # Remove NaN pairs\n",
    "            mask = ~(np.isnan(feature_vals) | np.isnan(target_vals))\n",
    "            if mask.sum() > 10:  # Need at least 10 valid pairs\n",
    "                corr, _ = pearsonr(feature_vals[mask], target_vals[mask])\n",
    "                correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "            else:\n",
    "                correlations.append(0)\n",
    "        \n",
    "        corr_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'abs_correlation': correlations\n",
    "        }).sort_values('abs_correlation', ascending=False)\n",
    "        \n",
    "        corr_results[target_name] = corr_importance\n",
    "        \n",
    "        print(f\"Top 10 correlations for {target_name}:\")\n",
    "        for idx, row in corr_importance.head(10).iterrows():\n",
    "            print(f\"  {idx+1:2d}. {row['feature']:<35} {row['abs_correlation']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in correlation analysis for {target_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 3. SIMPLE VARIANCE ANALYSIS (Alternative to LASSO)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. FEATURE VARIANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate feature variance (low variance = less informative)\n",
    "feature_variances = np.var(X_flat, axis=0)\n",
    "variance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'variance': feature_variances\n",
    "}).sort_values('variance', ascending=False)\n",
    "\n",
    "print(\"Top 10 features by variance:\")\n",
    "for idx, row in variance_df.head(10).iterrows():\n",
    "    print(f\"  {idx+1:2d}. {row['feature']:<35} {row['variance']:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 10 features by variance (potentially redundant):\")\n",
    "for idx, row in variance_df.tail(10).iterrows():\n",
    "    print(f\"  {idx+1:2d}. {row['feature']:<35} {row['variance']:.4f}\")\n",
    "\n",
    "# 4. COMBINED RANKING ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. COMBINED FEATURE RANKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "combined_rankings = {}\n",
    "\n",
    "for target_name in target_names:\n",
    "    if target_name not in rf_results or target_name not in corr_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nCombined analysis for {target_name}:\")\n",
    "    \n",
    "    # Get rankings from each method\n",
    "    rf_rank = rf_results[target_name].reset_index(drop=True)\n",
    "    rf_rank['rf_rank'] = rf_rank.index + 1\n",
    "    \n",
    "    corr_rank = corr_results[target_name].reset_index(drop=True)\n",
    "    corr_rank['corr_rank'] = corr_rank.index + 1\n",
    "    \n",
    "    # Add variance ranking\n",
    "    var_rank = variance_df.reset_index(drop=True)\n",
    "    var_rank['var_rank'] = var_rank.index + 1\n",
    "    \n",
    "    # Merge all rankings\n",
    "    combined = rf_rank[['feature', 'rf_rank', 'importance']].merge(\n",
    "        corr_rank[['feature', 'corr_rank', 'abs_correlation']], on='feature'\n",
    "    ).merge(\n",
    "        var_rank[['feature', 'var_rank', 'variance']], on='feature'\n",
    "    )\n",
    "    \n",
    "    # Calculate average rank (lower is better)\n",
    "    combined['avg_rank'] = combined[['rf_rank', 'corr_rank', 'var_rank']].mean(axis=1)\n",
    "    combined = combined.sort_values('avg_rank')\n",
    "    \n",
    "    combined_rankings[target_name] = combined\n",
    "    \n",
    "    print(\"Top 15 features by combined ranking:\")\n",
    "    for idx, row in combined.head(15).iterrows():\n",
    "        print(f\"  {idx+1:2d}. {row['feature']:<35} Rank: {row['avg_rank']:.1f}\")\n",
    "\n",
    "# 5. FEATURE CATEGORY ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. FEATURE CATEGORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def categorize_feature(feature_name):\n",
    "    if feature_name.startswith('Meal_'):\n",
    "        return 'Meal_Period'\n",
    "    elif feature_name.startswith('Event_'):\n",
    "        return 'Events'\n",
    "    elif feature_name.startswith('Tourism_'):\n",
    "        return 'Tourism'\n",
    "    elif feature_name.startswith('Impact_'):\n",
    "        return 'Revenue_Impact'\n",
    "    elif feature_name.endswith(('_sin', '_cos')):\n",
    "        return 'Cyclical'\n",
    "    elif feature_name.startswith('Is'):\n",
    "        return 'Event_Flags'\n",
    "    else:\n",
    "        return 'Core'\n",
    "\n",
    "# Analyze feature categories\n",
    "all_features_df = pd.DataFrame({'feature': feature_cols})\n",
    "all_features_df['category'] = all_features_df['feature'].apply(categorize_feature)\n",
    "\n",
    "category_counts = all_features_df['category'].value_counts()\n",
    "print(\"Feature count by category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category:<15}: {count:2d} features\")\n",
    "\n",
    "# 6. OVERALL FEATURE RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. FEATURE REDUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find top features across all targets\n",
    "all_top_features = set()\n",
    "top_counts = Counter()\n",
    "\n",
    "for target_name in target_names:\n",
    "    if target_name in combined_rankings:\n",
    "        top_20 = combined_rankings[target_name].head(20)['feature'].tolist()\n",
    "        all_top_features.update(top_20)\n",
    "        for feature in top_20:\n",
    "            top_counts[feature] += 1\n",
    "\n",
    "# Sort by how many targets find this feature important\n",
    "recommended_features = sorted(all_top_features, key=lambda x: top_counts[x], reverse=True)\n",
    "\n",
    "print(f\"RECOMMENDED FEATURE SET ({len(recommended_features)} features):\")\n",
    "print(\"Features ranked in top 20 for at least one revenue stream:\")\n",
    "\n",
    "for i, feature in enumerate(recommended_features[:25], 1):  # Show top 25\n",
    "    count = top_counts[feature]\n",
    "    stars = \"★\" * count\n",
    "    print(f\"  {i:2d}. {feature:<35} {stars} ({count}/3)\")\n",
    "\n",
    "# 7. FEATURES TO REMOVE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. FEATURES LIKELY TO REMOVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Features with consistently low rankings\n",
    "bottom_features = []\n",
    "for target_name in target_names:\n",
    "    if target_name in combined_rankings:\n",
    "        bottom_15 = combined_rankings[target_name].tail(15)['feature'].tolist()\n",
    "        bottom_features.extend(bottom_15)\n",
    "\n",
    "# Features that appear in bottom rankings multiple times\n",
    "bottom_counts = Counter(bottom_features)\n",
    "likely_redundant = [feature for feature, count in bottom_counts.items() if count >= 2]\n",
    "\n",
    "print(\"Features consistently ranked low:\")\n",
    "for feature in sorted(likely_redundant)[:15]:  # Show top 15 candidates for removal\n",
    "    count = bottom_counts[feature]\n",
    "    print(f\"  - {feature:<35} (bottom 15 for {count}/3 targets)\")\n",
    "\n",
    "# 8. SUMMARY AND RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8. SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Current model:\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "print(f\"  Training sequences: {X_train.shape[0]}\")\n",
    "print(f\"  Samples per feature: {X_flat.shape[0] / len(feature_cols):.1f}\")\n",
    "\n",
    "optimal_features = min(30, len(recommended_features))\n",
    "print(f\"\\nRecommended optimization:\")\n",
    "print(f\"  Keep top features: {optimal_features}\")\n",
    "print(f\"  Remove features: {len(feature_cols) - optimal_features}\")\n",
    "print(f\"  Reduction: {(len(feature_cols) - optimal_features) / len(feature_cols) * 100:.1f}%\")\n",
    "print(f\"  New samples per feature: {X_flat.shape[0] / optimal_features:.1f}\")\n",
    "\n",
    "print(f\"\\nTOP {optimal_features} FEATURES TO KEEP:\")\n",
    "for i, feature in enumerate(recommended_features[:optimal_features], 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# 9. CREATE FEATURE SELECTION CODE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"9. CODE TO IMPLEMENT FEATURE REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features_list = recommended_features[:optimal_features]\n",
    "print(\"# Copy this code to implement feature reduction:\")\n",
    "print(f\"recommended_features = {top_features_list}\")\n",
    "print(\"\\n# Get indices of recommended features\")\n",
    "print(\"feature_indices = [feature_cols.index(f) for f in recommended_features if f in feature_cols]\")\n",
    "print(f\"print(f'Found {{len(feature_indices)}} feature indices')\")\n",
    "print(\"\\n# Reduce your dataset\")\n",
    "print(\"X_train_reduced = X_train[:, :, feature_indices]\")\n",
    "print(\"X_test_reduced = X_test[:, :, feature_indices]\")\n",
    "print(\"feature_cols_reduced = [feature_cols[i] for i in feature_indices]\")\n",
    "print(f\"print(f'Reduced from {{X_train.shape}} to {{X_train_reduced.shape}}')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"Use the recommended_features list above to reduce your feature set.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "IMPLEMENTING FEATURE REDUCTION BASED ON RELEVANCE ANALYSIS\n",
      "================================================================================\n",
      "Original features: 65\n",
      "Recommended features: 22\n",
      "Reduction: 66.2%\n",
      "✓ Found 22 matching features\n",
      "✓ Reduced feature set: ['IsRamadan', 'Tourism_0', 'Month_cos', 'IsFoodFestival', 'Event_Ramadan-Middle', 'IsPreRamadan', 'Impact_0', 'DayOfWeek_sin', 'Event_Normal', 'Event_Pre-Ramadan-Late', 'IsLast10Ramadan', 'Impact_-1', 'Event_Pre-Ramadan-Early', 'Event_Ramadan-First10Days', 'Tourism_1', 'Event_Dubai-Food-Festival', 'CheckTotal', 'Event_Ramadan-Last10Days', 'is_zero', 'IsPreEvent', 'DayOfWeek_cos', 'Event_Pre-Dubai-Food-Festival']\n",
      "\n",
      "==================================================\n",
      "REDUCING FEATURE DIMENSIONS\n",
      "==================================================\n",
      "Original shapes:\n",
      "  X_train: (360, 30, 65)\n",
      "  X_test: (90, 30, 65)\n",
      "Reduced shapes:\n",
      "  X_train_reduced: (360, 30, 22)\n",
      "  X_test_reduced: (90, 30, 22)\n",
      "\n",
      "Feature reduction analysis:\n",
      "  Original samples per feature: 5.5\n",
      "  New samples per feature: 16.4\n",
      "  Improvement factor: 3.0x\n",
      "✓ Building optimized model with input shape: (30, 22)\n",
      "✓ Output shape: (7, 3)\n",
      "\n",
      "==================================================\n",
      "OPTIMIZED MODEL ARCHITECTURE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_4                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">693</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m2,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_1                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m3,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_2                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m20,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_3                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_norm_4                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │           \u001b[38;5;34m693\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (\u001b[38;5;33mReshape\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,541</span> (177.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,541\u001b[0m (177.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,189</span> (176.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m45,189\u001b[0m (176.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model complexity comparison:\n",
      "  Optimized model parameters: 45,189\n",
      "  Parameters per training sample: 125.5\n",
      "\n",
      "==================================================\n",
      "SETTING UP ENHANCED TRAINING\n",
      "==================================================\n",
      "✓ Enhanced callbacks configured\n",
      "  - EarlyStopping: patience=25\n",
      "  - ReduceLROnPlateau: factor=0.5, patience=10\n",
      "  - ModelCheckpoint: saves best model\n",
      "\n",
      "============================================================\n",
      "TRAINING OPTIMIZED MODEL WITH REDUCED FEATURES\n",
      "============================================================\n",
      "Training configuration:\n",
      "  Batch size: 16\n",
      "  Max epochs: 200\n",
      "  Learning rate: 0.0003\n",
      "  Loss function: Huber (delta=1.0)\n",
      "Epoch 1/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6635 - mae: 1.0610\n",
      "Epoch 1: val_loss improved from inf to 0.91881, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - loss: 0.6630 - mae: 1.0604 - val_loss: 0.9188 - val_mae: 1.3209 - learning_rate: 3.0000e-04\n",
      "Epoch 2/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6109 - mae: 1.0014\n",
      "Epoch 2: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6103 - mae: 1.0004 - val_loss: 0.9193 - val_mae: 1.3215 - learning_rate: 3.0000e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5889 - mae: 0.9764\n",
      "Epoch 3: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5880 - mae: 0.9752 - val_loss: 0.9194 - val_mae: 1.3216 - learning_rate: 3.0000e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5546 - mae: 0.9390\n",
      "Epoch 4: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5565 - mae: 0.9411 - val_loss: 0.9199 - val_mae: 1.3221 - learning_rate: 3.0000e-04\n",
      "Epoch 5/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5393 - mae: 0.9181 \n",
      "Epoch 5: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5342 - mae: 0.9127 - val_loss: 0.9207 - val_mae: 1.3228 - learning_rate: 3.0000e-04\n",
      "Epoch 6/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5223 - mae: 0.9023\n",
      "Epoch 6: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.5198 - mae: 0.8990 - val_loss: 0.9209 - val_mae: 1.3228 - learning_rate: 3.0000e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4986 - mae: 0.8686\n",
      "Epoch 7: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4974 - mae: 0.8678 - val_loss: 0.9216 - val_mae: 1.3233 - learning_rate: 3.0000e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4541 - mae: 0.8212\n",
      "Epoch 8: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4552 - mae: 0.8225 - val_loss: 0.9237 - val_mae: 1.3253 - learning_rate: 3.0000e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4845 - mae: 0.8582\n",
      "Epoch 9: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4805 - mae: 0.8530 - val_loss: 0.9245 - val_mae: 1.3261 - learning_rate: 3.0000e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4969 - mae: 0.8657\n",
      "Epoch 10: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4932 - mae: 0.8617 - val_loss: 0.9238 - val_mae: 1.3253 - learning_rate: 3.0000e-04\n",
      "Epoch 11/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4248 - mae: 0.7846\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4254 - mae: 0.7853 - val_loss: 0.9275 - val_mae: 1.3289 - learning_rate: 3.0000e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4364 - mae: 0.7977\n",
      "Epoch 12: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4356 - mae: 0.7968 - val_loss: 0.9288 - val_mae: 1.3301 - learning_rate: 1.5000e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4264 - mae: 0.7881\n",
      "Epoch 13: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4267 - mae: 0.7883 - val_loss: 0.9289 - val_mae: 1.3297 - learning_rate: 1.5000e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4229 - mae: 0.7849\n",
      "Epoch 14: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4231 - mae: 0.7850 - val_loss: 0.9277 - val_mae: 1.3279 - learning_rate: 1.5000e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4360 - mae: 0.8006\n",
      "Epoch 15: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4344 - mae: 0.7986 - val_loss: 0.9254 - val_mae: 1.3254 - learning_rate: 1.5000e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4131 - mae: 0.7728\n",
      "Epoch 16: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4135 - mae: 0.7732 - val_loss: 0.9233 - val_mae: 1.3227 - learning_rate: 1.5000e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.4486 - mae: 0.8097\n",
      "Epoch 17: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4407 - mae: 0.8011 - val_loss: 0.9214 - val_mae: 1.3211 - learning_rate: 1.5000e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4246 - mae: 0.7864\n",
      "Epoch 18: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4231 - mae: 0.7846 - val_loss: 0.9211 - val_mae: 1.3210 - learning_rate: 1.5000e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4127 - mae: 0.7719\n",
      "Epoch 19: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4125 - mae: 0.7716 - val_loss: 0.9199 - val_mae: 1.3188 - learning_rate: 1.5000e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3936 - mae: 0.7476\n",
      "Epoch 20: val_loss did not improve from 0.91881\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3939 - mae: 0.7478 - val_loss: 0.9196 - val_mae: 1.3181 - learning_rate: 1.5000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3836 - mae: 0.7373\n",
      "Epoch 21: val_loss improved from 0.91881 to 0.91674, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3858 - mae: 0.7394 - val_loss: 0.9167 - val_mae: 1.3144 - learning_rate: 1.5000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3924 - mae: 0.7459\n",
      "Epoch 22: val_loss improved from 0.91674 to 0.91523, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3926 - mae: 0.7460 - val_loss: 0.9152 - val_mae: 1.3123 - learning_rate: 1.5000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4150 - mae: 0.7723\n",
      "Epoch 23: val_loss improved from 0.91523 to 0.91061, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.4137 - mae: 0.7709 - val_loss: 0.9106 - val_mae: 1.3076 - learning_rate: 1.5000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4023 - mae: 0.7510\n",
      "Epoch 24: val_loss improved from 0.91061 to 0.91010, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3988 - mae: 0.7478 - val_loss: 0.9101 - val_mae: 1.3069 - learning_rate: 1.5000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3826 - mae: 0.7339\n",
      "Epoch 25: val_loss improved from 0.91010 to 0.90695, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3828 - mae: 0.7340 - val_loss: 0.9069 - val_mae: 1.3034 - learning_rate: 1.5000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3634 - mae: 0.7147\n",
      "Epoch 26: val_loss improved from 0.90695 to 0.90596, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3702 - mae: 0.7219 - val_loss: 0.9060 - val_mae: 1.3018 - learning_rate: 1.5000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3818 - mae: 0.7303\n",
      "Epoch 27: val_loss did not improve from 0.90596\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3821 - mae: 0.7308 - val_loss: 0.9087 - val_mae: 1.3045 - learning_rate: 1.5000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3684 - mae: 0.7188\n",
      "Epoch 28: val_loss did not improve from 0.90596\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3721 - mae: 0.7225 - val_loss: 0.9074 - val_mae: 1.3030 - learning_rate: 1.5000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3937 - mae: 0.7445\n",
      "Epoch 29: val_loss improved from 0.90596 to 0.90467, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3890 - mae: 0.7396 - val_loss: 0.9047 - val_mae: 1.2990 - learning_rate: 1.5000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3355 - mae: 0.6823\n",
      "Epoch 30: val_loss improved from 0.90467 to 0.90323, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3453 - mae: 0.6933 - val_loss: 0.9032 - val_mae: 1.2965 - learning_rate: 1.5000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3978 - mae: 0.7499\n",
      "Epoch 31: val_loss improved from 0.90323 to 0.90321, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3910 - mae: 0.7424 - val_loss: 0.9032 - val_mae: 1.2965 - learning_rate: 1.5000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3808 - mae: 0.7306\n",
      "Epoch 32: val_loss did not improve from 0.90321\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3804 - mae: 0.7302 - val_loss: 0.9041 - val_mae: 1.2979 - learning_rate: 1.5000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3640 - mae: 0.7085\n",
      "Epoch 33: val_loss improved from 0.90321 to 0.90308, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3643 - mae: 0.7089 - val_loss: 0.9031 - val_mae: 1.2969 - learning_rate: 1.5000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3926 - mae: 0.7405\n",
      "Epoch 34: val_loss improved from 0.90308 to 0.90296, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3916 - mae: 0.7395 - val_loss: 0.9030 - val_mae: 1.2963 - learning_rate: 1.5000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3683 - mae: 0.7185\n",
      "Epoch 35: val_loss improved from 0.90296 to 0.90242, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3683 - mae: 0.7184 - val_loss: 0.9024 - val_mae: 1.2958 - learning_rate: 1.5000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.4051 - mae: 0.7563\n",
      "Epoch 36: val_loss did not improve from 0.90242\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4036 - mae: 0.7547 - val_loss: 0.9038 - val_mae: 1.2973 - learning_rate: 1.5000e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4112 - mae: 0.7598\n",
      "Epoch 37: val_loss did not improve from 0.90242\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4050 - mae: 0.7535 - val_loss: 0.9033 - val_mae: 1.2967 - learning_rate: 1.5000e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3612 - mae: 0.7059\n",
      "Epoch 38: val_loss did not improve from 0.90242\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3613 - mae: 0.7060 - val_loss: 0.9040 - val_mae: 1.2974 - learning_rate: 1.5000e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3739 - mae: 0.7219\n",
      "Epoch 39: val_loss did not improve from 0.90242\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3703 - mae: 0.7176 - val_loss: 0.9027 - val_mae: 1.2963 - learning_rate: 1.5000e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3556 - mae: 0.6990\n",
      "Epoch 40: val_loss improved from 0.90242 to 0.90117, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3557 - mae: 0.6997 - val_loss: 0.9012 - val_mae: 1.2940 - learning_rate: 1.5000e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3580 - mae: 0.7047\n",
      "Epoch 41: val_loss did not improve from 0.90117\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3576 - mae: 0.7042 - val_loss: 0.9017 - val_mae: 1.2936 - learning_rate: 1.5000e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3347 - mae: 0.6790\n",
      "Epoch 42: val_loss did not improve from 0.90117\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3361 - mae: 0.6802 - val_loss: 0.9020 - val_mae: 1.2937 - learning_rate: 1.5000e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3259 - mae: 0.6700\n",
      "Epoch 43: val_loss did not improve from 0.90117\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3348 - mae: 0.6790 - val_loss: 0.9014 - val_mae: 1.2929 - learning_rate: 1.5000e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3292 - mae: 0.6748\n",
      "Epoch 44: val_loss improved from 0.90117 to 0.89655, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3324 - mae: 0.6779 - val_loss: 0.8965 - val_mae: 1.2878 - learning_rate: 1.5000e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3430 - mae: 0.6858\n",
      "Epoch 45: val_loss improved from 0.89655 to 0.89628, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3433 - mae: 0.6862 - val_loss: 0.8963 - val_mae: 1.2873 - learning_rate: 1.5000e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3273 - mae: 0.6669\n",
      "Epoch 46: val_loss improved from 0.89628 to 0.89585, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3306 - mae: 0.6703 - val_loss: 0.8959 - val_mae: 1.2870 - learning_rate: 1.5000e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3415 - mae: 0.6845\n",
      "Epoch 47: val_loss improved from 0.89585 to 0.89371, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3418 - mae: 0.6848 - val_loss: 0.8937 - val_mae: 1.2845 - learning_rate: 1.5000e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3428 - mae: 0.6825\n",
      "Epoch 48: val_loss improved from 0.89371 to 0.89317, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3415 - mae: 0.6812 - val_loss: 0.8932 - val_mae: 1.2833 - learning_rate: 1.5000e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3508 - mae: 0.6930\n",
      "Epoch 49: val_loss improved from 0.89317 to 0.89259, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3502 - mae: 0.6925 - val_loss: 0.8926 - val_mae: 1.2827 - learning_rate: 1.5000e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.3525 - mae: 0.6943\n",
      "Epoch 50: val_loss improved from 0.89259 to 0.89244, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3519 - mae: 0.6936 - val_loss: 0.8924 - val_mae: 1.2827 - learning_rate: 1.5000e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3253 - mae: 0.6650\n",
      "Epoch 51: val_loss improved from 0.89244 to 0.88865, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3260 - mae: 0.6657 - val_loss: 0.8887 - val_mae: 1.2777 - learning_rate: 1.5000e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3350 - mae: 0.6751\n",
      "Epoch 52: val_loss improved from 0.88865 to 0.88590, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3362 - mae: 0.6767 - val_loss: 0.8859 - val_mae: 1.2745 - learning_rate: 1.5000e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3199 - mae: 0.6579 \n",
      "Epoch 53: val_loss improved from 0.88590 to 0.88304, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3224 - mae: 0.6606 - val_loss: 0.8830 - val_mae: 1.2715 - learning_rate: 1.5000e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3335 - mae: 0.6691\n",
      "Epoch 54: val_loss improved from 0.88304 to 0.88034, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3341 - mae: 0.6710 - val_loss: 0.8803 - val_mae: 1.2685 - learning_rate: 1.5000e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3239 - mae: 0.6603\n",
      "Epoch 55: val_loss improved from 0.88034 to 0.87554, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.3245 - mae: 0.6610 - val_loss: 0.8755 - val_mae: 1.2634 - learning_rate: 1.5000e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3411 - mae: 0.6777\n",
      "Epoch 56: val_loss did not improve from 0.87554\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3403 - mae: 0.6769 - val_loss: 0.8766 - val_mae: 1.2644 - learning_rate: 1.5000e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3521 - mae: 0.6931 \n",
      "Epoch 57: val_loss improved from 0.87554 to 0.87331, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3466 - mae: 0.6871 - val_loss: 0.8733 - val_mae: 1.2609 - learning_rate: 1.5000e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3276 - mae: 0.6665\n",
      "Epoch 58: val_loss did not improve from 0.87331\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3272 - mae: 0.6661 - val_loss: 0.8741 - val_mae: 1.2620 - learning_rate: 1.5000e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3335 - mae: 0.6728\n",
      "Epoch 59: val_loss improved from 0.87331 to 0.87215, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3329 - mae: 0.6723 - val_loss: 0.8722 - val_mae: 1.2601 - learning_rate: 1.5000e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3274 - mae: 0.6619\n",
      "Epoch 60: val_loss improved from 0.87215 to 0.86236, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3258 - mae: 0.6603 - val_loss: 0.8624 - val_mae: 1.2495 - learning_rate: 1.5000e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3222 - mae: 0.6589\n",
      "Epoch 61: val_loss improved from 0.86236 to 0.85948, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3189 - mae: 0.6554 - val_loss: 0.8595 - val_mae: 1.2461 - learning_rate: 1.5000e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3358 - mae: 0.6732\n",
      "Epoch 62: val_loss improved from 0.85948 to 0.85683, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3334 - mae: 0.6702 - val_loss: 0.8568 - val_mae: 1.2420 - learning_rate: 1.5000e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3034 - mae: 0.6355\n",
      "Epoch 63: val_loss improved from 0.85683 to 0.85439, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3047 - mae: 0.6366 - val_loss: 0.8544 - val_mae: 1.2388 - learning_rate: 1.5000e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2884 - mae: 0.6221\n",
      "Epoch 64: val_loss did not improve from 0.85439\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2906 - mae: 0.6243 - val_loss: 0.8566 - val_mae: 1.2415 - learning_rate: 1.5000e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2953 - mae: 0.6277\n",
      "Epoch 65: val_loss improved from 0.85439 to 0.85096, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.3007 - mae: 0.6344 - val_loss: 0.8510 - val_mae: 1.2358 - learning_rate: 1.5000e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2926 - mae: 0.6283\n",
      "Epoch 66: val_loss improved from 0.85096 to 0.84721, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2935 - mae: 0.6293 - val_loss: 0.8472 - val_mae: 1.2317 - learning_rate: 1.5000e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3052 - mae: 0.6416\n",
      "Epoch 67: val_loss improved from 0.84721 to 0.84561, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3062 - mae: 0.6424 - val_loss: 0.8456 - val_mae: 1.2293 - learning_rate: 1.5000e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m15/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3072 - mae: 0.6435\n",
      "Epoch 68: val_loss improved from 0.84561 to 0.83796, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3084 - mae: 0.6449 - val_loss: 0.8380 - val_mae: 1.2202 - learning_rate: 1.5000e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3529 - mae: 0.6900\n",
      "Epoch 69: val_loss did not improve from 0.83796\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3435 - mae: 0.6800 - val_loss: 0.8404 - val_mae: 1.2222 - learning_rate: 1.5000e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3050 - mae: 0.6391\n",
      "Epoch 70: val_loss did not improve from 0.83796\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3068 - mae: 0.6410 - val_loss: 0.8471 - val_mae: 1.2299 - learning_rate: 1.5000e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2855 - mae: 0.6167 \n",
      "Epoch 71: val_loss did not improve from 0.83796\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2885 - mae: 0.6200 - val_loss: 0.8455 - val_mae: 1.2281 - learning_rate: 1.5000e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3052 - mae: 0.6403\n",
      "Epoch 72: val_loss did not improve from 0.83796\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3051 - mae: 0.6401 - val_loss: 0.8434 - val_mae: 1.2254 - learning_rate: 1.5000e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3294 - mae: 0.6647\n",
      "Epoch 73: val_loss improved from 0.83796 to 0.83751, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3234 - mae: 0.6583 - val_loss: 0.8375 - val_mae: 1.2190 - learning_rate: 1.5000e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3058 - mae: 0.6384\n",
      "Epoch 74: val_loss improved from 0.83751 to 0.83627, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3057 - mae: 0.6384 - val_loss: 0.8363 - val_mae: 1.2170 - learning_rate: 1.5000e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3130 - mae: 0.6447\n",
      "Epoch 75: val_loss improved from 0.83627 to 0.83254, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.3109 - mae: 0.6427 - val_loss: 0.8325 - val_mae: 1.2127 - learning_rate: 1.5000e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2823 - mae: 0.6123\n",
      "Epoch 76: val_loss improved from 0.83254 to 0.82967, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2859 - mae: 0.6163 - val_loss: 0.8297 - val_mae: 1.2099 - learning_rate: 1.5000e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2898 - mae: 0.6213\n",
      "Epoch 77: val_loss improved from 0.82967 to 0.82394, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2912 - mae: 0.6228 - val_loss: 0.8239 - val_mae: 1.2037 - learning_rate: 1.5000e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2819 - mae: 0.6102\n",
      "Epoch 78: val_loss improved from 0.82394 to 0.82072, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2848 - mae: 0.6131 - val_loss: 0.8207 - val_mae: 1.2004 - learning_rate: 1.5000e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2984 - mae: 0.6295\n",
      "Epoch 79: val_loss improved from 0.82072 to 0.81565, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2987 - mae: 0.6298 - val_loss: 0.8157 - val_mae: 1.1949 - learning_rate: 1.5000e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2914 - mae: 0.6235\n",
      "Epoch 80: val_loss did not improve from 0.81565\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2902 - mae: 0.6219 - val_loss: 0.8173 - val_mae: 1.1963 - learning_rate: 1.5000e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2991 - mae: 0.6301\n",
      "Epoch 81: val_loss did not improve from 0.81565\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2989 - mae: 0.6298 - val_loss: 0.8295 - val_mae: 1.2087 - learning_rate: 1.5000e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2708 - mae: 0.6003\n",
      "Epoch 82: val_loss did not improve from 0.81565\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2760 - mae: 0.6062 - val_loss: 0.8250 - val_mae: 1.2033 - learning_rate: 1.5000e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2835 - mae: 0.6137\n",
      "Epoch 83: val_loss did not improve from 0.81565\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2850 - mae: 0.6151 - val_loss: 0.8223 - val_mae: 1.1991 - learning_rate: 1.5000e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2954 - mae: 0.6248\n",
      "Epoch 84: val_loss improved from 0.81565 to 0.81499, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2953 - mae: 0.6248 - val_loss: 0.8150 - val_mae: 1.1906 - learning_rate: 1.5000e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3005 - mae: 0.6264\n",
      "Epoch 85: val_loss improved from 0.81499 to 0.81320, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3000 - mae: 0.6260 - val_loss: 0.8132 - val_mae: 1.1890 - learning_rate: 1.5000e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2963 - mae: 0.6259\n",
      "Epoch 86: val_loss did not improve from 0.81320\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2931 - mae: 0.6224 - val_loss: 0.8151 - val_mae: 1.1924 - learning_rate: 1.5000e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3077 - mae: 0.6389\n",
      "Epoch 87: val_loss improved from 0.81320 to 0.80591, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3045 - mae: 0.6358 - val_loss: 0.8059 - val_mae: 1.1822 - learning_rate: 1.5000e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m15/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2819 - mae: 0.6138\n",
      "Epoch 88: val_loss improved from 0.80591 to 0.80226, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2841 - mae: 0.6152 - val_loss: 0.8023 - val_mae: 1.1774 - learning_rate: 1.5000e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3010 - mae: 0.6302\n",
      "Epoch 89: val_loss did not improve from 0.80226\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3005 - mae: 0.6301 - val_loss: 0.8105 - val_mae: 1.1866 - learning_rate: 1.5000e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2720 - mae: 0.5973\n",
      "Epoch 90: val_loss did not improve from 0.80226\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2767 - mae: 0.6033 - val_loss: 0.8041 - val_mae: 1.1799 - learning_rate: 1.5000e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2939 - mae: 0.6180\n",
      "Epoch 91: val_loss improved from 0.80226 to 0.80159, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2939 - mae: 0.6181 - val_loss: 0.8016 - val_mae: 1.1780 - learning_rate: 1.5000e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2945 - mae: 0.6204\n",
      "Epoch 92: val_loss improved from 0.80159 to 0.79823, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2915 - mae: 0.6179 - val_loss: 0.7982 - val_mae: 1.1739 - learning_rate: 1.5000e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2715 - mae: 0.5976\n",
      "Epoch 93: val_loss improved from 0.79823 to 0.79752, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2754 - mae: 0.6019 - val_loss: 0.7975 - val_mae: 1.1726 - learning_rate: 1.5000e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2873 - mae: 0.6154\n",
      "Epoch 94: val_loss improved from 0.79752 to 0.79704, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.2880 - mae: 0.6160 - val_loss: 0.7970 - val_mae: 1.1715 - learning_rate: 1.5000e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2666 - mae: 0.5896\n",
      "Epoch 95: val_loss improved from 0.79704 to 0.79454, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2709 - mae: 0.5943 - val_loss: 0.7945 - val_mae: 1.1694 - learning_rate: 1.5000e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2784 - mae: 0.6032\n",
      "Epoch 96: val_loss improved from 0.79454 to 0.79061, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2790 - mae: 0.6043 - val_loss: 0.7906 - val_mae: 1.1654 - learning_rate: 1.5000e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2798 - mae: 0.6046\n",
      "Epoch 97: val_loss improved from 0.79061 to 0.78300, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2810 - mae: 0.6061 - val_loss: 0.7830 - val_mae: 1.1581 - learning_rate: 1.5000e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2959 - mae: 0.6232\n",
      "Epoch 98: val_loss did not improve from 0.78300\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2950 - mae: 0.6221 - val_loss: 0.7844 - val_mae: 1.1599 - learning_rate: 1.5000e-04\n",
      "Epoch 99/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2975 - mae: 0.6236\n",
      "Epoch 99: val_loss did not improve from 0.78300\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2958 - mae: 0.6221 - val_loss: 0.7843 - val_mae: 1.1599 - learning_rate: 1.5000e-04\n",
      "Epoch 100/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2943 - mae: 0.6226\n",
      "Epoch 100: val_loss improved from 0.78300 to 0.76976, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2936 - mae: 0.6218 - val_loss: 0.7698 - val_mae: 1.1435 - learning_rate: 1.5000e-04\n",
      "Epoch 101/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2899 - mae: 0.6117\n",
      "Epoch 101: val_loss improved from 0.76976 to 0.76602, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2891 - mae: 0.6113 - val_loss: 0.7660 - val_mae: 1.1394 - learning_rate: 1.5000e-04\n",
      "Epoch 102/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2827 - mae: 0.6085\n",
      "Epoch 102: val_loss improved from 0.76602 to 0.76503, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2828 - mae: 0.6086 - val_loss: 0.7650 - val_mae: 1.1390 - learning_rate: 1.5000e-04\n",
      "Epoch 103/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2848 - mae: 0.6123 \n",
      "Epoch 103: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2852 - mae: 0.6128 - val_loss: 0.7756 - val_mae: 1.1493 - learning_rate: 1.5000e-04\n",
      "Epoch 104/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3034 - mae: 0.6321\n",
      "Epoch 104: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3021 - mae: 0.6307 - val_loss: 0.7864 - val_mae: 1.1600 - learning_rate: 1.5000e-04\n",
      "Epoch 105/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2968 - mae: 0.6221\n",
      "Epoch 105: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2946 - mae: 0.6199 - val_loss: 0.7935 - val_mae: 1.1673 - learning_rate: 1.5000e-04\n",
      "Epoch 106/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2778 - mae: 0.6029\n",
      "Epoch 106: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2790 - mae: 0.6043 - val_loss: 0.8011 - val_mae: 1.1743 - learning_rate: 1.5000e-04\n",
      "Epoch 107/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2798 - mae: 0.5998\n",
      "Epoch 107: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2792 - mae: 0.6001 - val_loss: 0.7968 - val_mae: 1.1696 - learning_rate: 1.5000e-04\n",
      "Epoch 108/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2802 - mae: 0.6039\n",
      "Epoch 108: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2805 - mae: 0.6042 - val_loss: 0.7972 - val_mae: 1.1700 - learning_rate: 1.5000e-04\n",
      "Epoch 109/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2755 - mae: 0.6023 \n",
      "Epoch 109: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.2768 - mae: 0.6035 - val_loss: 0.8016 - val_mae: 1.1749 - learning_rate: 1.5000e-04\n",
      "Epoch 110/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2746 - mae: 0.5994\n",
      "Epoch 110: val_loss did not improve from 0.76503\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2747 - mae: 0.5995 - val_loss: 0.7855 - val_mae: 1.1589 - learning_rate: 1.5000e-04\n",
      "Epoch 111/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2609 - mae: 0.5833\n",
      "Epoch 111: val_loss improved from 0.76503 to 0.75135, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2640 - mae: 0.5869 - val_loss: 0.7514 - val_mae: 1.1219 - learning_rate: 1.5000e-04\n",
      "Epoch 112/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2746 - mae: 0.6004 \n",
      "Epoch 112: val_loss did not improve from 0.75135\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2752 - mae: 0.6010 - val_loss: 0.7610 - val_mae: 1.1315 - learning_rate: 1.5000e-04\n",
      "Epoch 113/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2895 - mae: 0.6156 \n",
      "Epoch 113: val_loss did not improve from 0.75135\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2883 - mae: 0.6141 - val_loss: 0.7566 - val_mae: 1.1267 - learning_rate: 1.5000e-04\n",
      "Epoch 114/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2653 - mae: 0.5888 \n",
      "Epoch 114: val_loss did not improve from 0.75135\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2680 - mae: 0.5913 - val_loss: 0.7562 - val_mae: 1.1266 - learning_rate: 1.5000e-04\n",
      "Epoch 115/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2962 - mae: 0.6282\n",
      "Epoch 115: val_loss did not improve from 0.75135\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2944 - mae: 0.6258 - val_loss: 0.7594 - val_mae: 1.1302 - learning_rate: 1.5000e-04\n",
      "Epoch 116/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2758 - mae: 0.6010\n",
      "Epoch 116: val_loss improved from 0.75135 to 0.74352, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2751 - mae: 0.5996 - val_loss: 0.7435 - val_mae: 1.1138 - learning_rate: 1.5000e-04\n",
      "Epoch 117/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2816 - mae: 0.6008\n",
      "Epoch 117: val_loss improved from 0.74352 to 0.73199, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2793 - mae: 0.5990 - val_loss: 0.7320 - val_mae: 1.1020 - learning_rate: 1.5000e-04\n",
      "Epoch 118/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2859 - mae: 0.6081\n",
      "Epoch 118: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2855 - mae: 0.6077 - val_loss: 0.7466 - val_mae: 1.1171 - learning_rate: 1.5000e-04\n",
      "Epoch 119/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2657 - mae: 0.5849\n",
      "Epoch 119: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2672 - mae: 0.5867 - val_loss: 0.7623 - val_mae: 1.1336 - learning_rate: 1.5000e-04\n",
      "Epoch 120/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2804 - mae: 0.6070\n",
      "Epoch 120: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2801 - mae: 0.6067 - val_loss: 0.7475 - val_mae: 1.1174 - learning_rate: 1.5000e-04\n",
      "Epoch 121/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2782 - mae: 0.5991\n",
      "Epoch 121: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2767 - mae: 0.5978 - val_loss: 0.7581 - val_mae: 1.1284 - learning_rate: 1.5000e-04\n",
      "Epoch 122/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2705 - mae: 0.5974\n",
      "Epoch 122: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2746 - mae: 0.6006 - val_loss: 0.7795 - val_mae: 1.1494 - learning_rate: 1.5000e-04\n",
      "Epoch 123/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2682 - mae: 0.5915\n",
      "Epoch 123: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2685 - mae: 0.5918 - val_loss: 0.7627 - val_mae: 1.1321 - learning_rate: 1.5000e-04\n",
      "Epoch 124/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2646 - mae: 0.5876\n",
      "Epoch 124: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2681 - mae: 0.5909 - val_loss: 0.7353 - val_mae: 1.1038 - learning_rate: 1.5000e-04\n",
      "Epoch 125/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2608 - mae: 0.5830\n",
      "Epoch 125: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2635 - mae: 0.5855 - val_loss: 0.7355 - val_mae: 1.1039 - learning_rate: 1.5000e-04\n",
      "Epoch 126/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2649 - mae: 0.5863\n",
      "Epoch 126: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2658 - mae: 0.5873 - val_loss: 0.7392 - val_mae: 1.1081 - learning_rate: 1.5000e-04\n",
      "Epoch 127/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2804 - mae: 0.6056\n",
      "Epoch 127: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2804 - mae: 0.6054 - val_loss: 0.7448 - val_mae: 1.1148 - learning_rate: 1.5000e-04\n",
      "Epoch 128/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2788 - mae: 0.6029\n",
      "Epoch 128: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2778 - mae: 0.6017 - val_loss: 0.7437 - val_mae: 1.1136 - learning_rate: 7.5000e-05\n",
      "Epoch 129/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2749 - mae: 0.5951\n",
      "Epoch 129: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2757 - mae: 0.5963 - val_loss: 0.7368 - val_mae: 1.1062 - learning_rate: 7.5000e-05\n",
      "Epoch 130/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2538 - mae: 0.5765\n",
      "Epoch 130: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2587 - mae: 0.5809 - val_loss: 0.7345 - val_mae: 1.1036 - learning_rate: 7.5000e-05\n",
      "Epoch 131/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2704 - mae: 0.5874\n",
      "Epoch 131: val_loss did not improve from 0.73199\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2716 - mae: 0.5903 - val_loss: 0.7351 - val_mae: 1.1040 - learning_rate: 7.5000e-05\n",
      "Epoch 132/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2789 - mae: 0.6014\n",
      "Epoch 132: val_loss improved from 0.73199 to 0.73158, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2786 - mae: 0.6012 - val_loss: 0.7316 - val_mae: 1.1003 - learning_rate: 7.5000e-05\n",
      "Epoch 133/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3072 - mae: 0.6315\n",
      "Epoch 133: val_loss did not improve from 0.73158\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2973 - mae: 0.6210 - val_loss: 0.7337 - val_mae: 1.1026 - learning_rate: 7.5000e-05\n",
      "Epoch 134/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2825 - mae: 0.6090\n",
      "Epoch 134: val_loss improved from 0.73158 to 0.73091, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2804 - mae: 0.6057 - val_loss: 0.7309 - val_mae: 1.0997 - learning_rate: 7.5000e-05\n",
      "Epoch 135/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2515 - mae: 0.5731\n",
      "Epoch 135: val_loss did not improve from 0.73091\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2536 - mae: 0.5750 - val_loss: 0.7334 - val_mae: 1.1021 - learning_rate: 7.5000e-05\n",
      "Epoch 136/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2712 - mae: 0.5889\n",
      "Epoch 136: val_loss did not improve from 0.73091\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2718 - mae: 0.5908 - val_loss: 0.7332 - val_mae: 1.1018 - learning_rate: 7.5000e-05\n",
      "Epoch 137/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2968 - mae: 0.6202\n",
      "Epoch 137: val_loss did not improve from 0.73091\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2960 - mae: 0.6193 - val_loss: 0.7319 - val_mae: 1.1003 - learning_rate: 7.5000e-05\n",
      "Epoch 138/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2698 - mae: 0.5912\n",
      "Epoch 138: val_loss improved from 0.73091 to 0.72543, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2690 - mae: 0.5901 - val_loss: 0.7254 - val_mae: 1.0937 - learning_rate: 7.5000e-05\n",
      "Epoch 139/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2594 - mae: 0.5830\n",
      "Epoch 139: val_loss did not improve from 0.72543\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2643 - mae: 0.5882 - val_loss: 0.7322 - val_mae: 1.1007 - learning_rate: 7.5000e-05\n",
      "Epoch 140/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2603 - mae: 0.5804\n",
      "Epoch 140: val_loss did not improve from 0.72543\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2613 - mae: 0.5815 - val_loss: 0.7344 - val_mae: 1.1030 - learning_rate: 7.5000e-05\n",
      "Epoch 141/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2555 - mae: 0.5738\n",
      "Epoch 141: val_loss did not improve from 0.72543\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2585 - mae: 0.5772 - val_loss: 0.7262 - val_mae: 1.0947 - learning_rate: 7.5000e-05\n",
      "Epoch 142/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2731 - mae: 0.5990\n",
      "Epoch 142: val_loss improved from 0.72543 to 0.71991, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2730 - mae: 0.5988 - val_loss: 0.7199 - val_mae: 1.0874 - learning_rate: 7.5000e-05\n",
      "Epoch 143/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2788 - mae: 0.6021\n",
      "Epoch 143: val_loss improved from 0.71991 to 0.71946, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2770 - mae: 0.6000 - val_loss: 0.7195 - val_mae: 1.0870 - learning_rate: 7.5000e-05\n",
      "Epoch 144/200\n",
      "\u001b[1m16/23\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2568 - mae: 0.5814\n",
      "Epoch 144: val_loss did not improve from 0.71946\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2611 - mae: 0.5848 - val_loss: 0.7262 - val_mae: 1.0939 - learning_rate: 7.5000e-05\n",
      "Epoch 145/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3087 - mae: 0.6332\n",
      "Epoch 145: val_loss did not improve from 0.71946\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3072 - mae: 0.6316 - val_loss: 0.7244 - val_mae: 1.0920 - learning_rate: 7.5000e-05\n",
      "Epoch 146/200\n",
      "\u001b[1m17/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2755 - mae: 0.5964\n",
      "Epoch 146: val_loss did not improve from 0.71946\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2754 - mae: 0.5968 - val_loss: 0.7272 - val_mae: 1.0949 - learning_rate: 7.5000e-05\n",
      "Epoch 147/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2679 - mae: 0.5894\n",
      "Epoch 147: val_loss did not improve from 0.71946\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2690 - mae: 0.5904 - val_loss: 0.7203 - val_mae: 1.0877 - learning_rate: 7.5000e-05\n",
      "Epoch 148/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2728 - mae: 0.5942\n",
      "Epoch 148: val_loss improved from 0.71946 to 0.71754, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2727 - mae: 0.5940 - val_loss: 0.7175 - val_mae: 1.0850 - learning_rate: 7.5000e-05\n",
      "Epoch 149/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2673 - mae: 0.5841\n",
      "Epoch 149: val_loss improved from 0.71754 to 0.71030, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2678 - mae: 0.5850 - val_loss: 0.7103 - val_mae: 1.0777 - learning_rate: 7.5000e-05\n",
      "Epoch 150/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2959 - mae: 0.6170\n",
      "Epoch 150: val_loss improved from 0.71030 to 0.69183, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2919 - mae: 0.6129 - val_loss: 0.6918 - val_mae: 1.0587 - learning_rate: 7.5000e-05\n",
      "Epoch 151/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2509 - mae: 0.5661\n",
      "Epoch 151: val_loss improved from 0.69183 to 0.68991, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2550 - mae: 0.5711 - val_loss: 0.6899 - val_mae: 1.0570 - learning_rate: 7.5000e-05\n",
      "Epoch 152/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2575 - mae: 0.5789\n",
      "Epoch 152: val_loss improved from 0.68991 to 0.67189, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2598 - mae: 0.5811 - val_loss: 0.6719 - val_mae: 1.0383 - learning_rate: 7.5000e-05\n",
      "Epoch 153/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2601 - mae: 0.5788\n",
      "Epoch 153: val_loss improved from 0.67189 to 0.65486, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2616 - mae: 0.5806 - val_loss: 0.6549 - val_mae: 1.0203 - learning_rate: 7.5000e-05\n",
      "Epoch 154/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2902 - mae: 0.6098\n",
      "Epoch 154: val_loss improved from 0.65486 to 0.65362, saving model to best_optimized_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2869 - mae: 0.6068 - val_loss: 0.6536 - val_mae: 1.0187 - learning_rate: 7.5000e-05\n",
      "Epoch 155/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2674 - mae: 0.5902\n",
      "Epoch 155: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2679 - mae: 0.5906 - val_loss: 0.6658 - val_mae: 1.0319 - learning_rate: 7.5000e-05\n",
      "Epoch 156/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2789 - mae: 0.6047\n",
      "Epoch 156: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2778 - mae: 0.6030 - val_loss: 0.6655 - val_mae: 1.0315 - learning_rate: 7.5000e-05\n",
      "Epoch 157/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2525 - mae: 0.5719\n",
      "Epoch 157: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2564 - mae: 0.5760 - val_loss: 0.6801 - val_mae: 1.0472 - learning_rate: 7.5000e-05\n",
      "Epoch 158/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2473 - mae: 0.5659\n",
      "Epoch 158: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2501 - mae: 0.5691 - val_loss: 0.6812 - val_mae: 1.0480 - learning_rate: 7.5000e-05\n",
      "Epoch 159/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2758 - mae: 0.5981\n",
      "Epoch 159: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2745 - mae: 0.5960 - val_loss: 0.6821 - val_mae: 1.0486 - learning_rate: 7.5000e-05\n",
      "Epoch 160/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2590 - mae: 0.5776\n",
      "Epoch 160: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2601 - mae: 0.5789 - val_loss: 0.6899 - val_mae: 1.0568 - learning_rate: 7.5000e-05\n",
      "Epoch 161/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2613 - mae: 0.5823\n",
      "Epoch 161: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2618 - mae: 0.5826 - val_loss: 0.6936 - val_mae: 1.0608 - learning_rate: 7.5000e-05\n",
      "Epoch 162/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2540 - mae: 0.5760\n",
      "Epoch 162: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2559 - mae: 0.5777 - val_loss: 0.6850 - val_mae: 1.0514 - learning_rate: 7.5000e-05\n",
      "Epoch 163/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2511 - mae: 0.5688\n",
      "Epoch 163: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2527 - mae: 0.5706 - val_loss: 0.7065 - val_mae: 1.0731 - learning_rate: 7.5000e-05\n",
      "Epoch 164/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2604 - mae: 0.5826\n",
      "Epoch 164: ReduceLROnPlateau reducing learning rate to 3.7500001781154424e-05.\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2614 - mae: 0.5832 - val_loss: 0.7060 - val_mae: 1.0724 - learning_rate: 7.5000e-05\n",
      "Epoch 165/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2763 - mae: 0.5992\n",
      "Epoch 165: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2755 - mae: 0.5983 - val_loss: 0.7037 - val_mae: 1.0701 - learning_rate: 3.7500e-05\n",
      "Epoch 166/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2449 - mae: 0.5644 \n",
      "Epoch 166: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2493 - mae: 0.5683 - val_loss: 0.7023 - val_mae: 1.0688 - learning_rate: 3.7500e-05\n",
      "Epoch 167/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3004 - mae: 0.6234\n",
      "Epoch 167: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2953 - mae: 0.6179 - val_loss: 0.7016 - val_mae: 1.0681 - learning_rate: 3.7500e-05\n",
      "Epoch 168/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2719 - mae: 0.5952\n",
      "Epoch 168: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2720 - mae: 0.5946 - val_loss: 0.7024 - val_mae: 1.0688 - learning_rate: 3.7500e-05\n",
      "Epoch 169/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2611 - mae: 0.5795\n",
      "Epoch 169: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2615 - mae: 0.5800 - val_loss: 0.6978 - val_mae: 1.0639 - learning_rate: 3.7500e-05\n",
      "Epoch 170/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2931 - mae: 0.6149\n",
      "Epoch 170: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2893 - mae: 0.6110 - val_loss: 0.6885 - val_mae: 1.0544 - learning_rate: 3.7500e-05\n",
      "Epoch 171/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2560 - mae: 0.5730\n",
      "Epoch 171: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2588 - mae: 0.5765 - val_loss: 0.6880 - val_mae: 1.0538 - learning_rate: 3.7500e-05\n",
      "Epoch 172/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2769 - mae: 0.6010\n",
      "Epoch 172: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2755 - mae: 0.5989 - val_loss: 0.6876 - val_mae: 1.0533 - learning_rate: 3.7500e-05\n",
      "Epoch 173/200\n",
      "\u001b[1m19/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2470 - mae: 0.5644\n",
      "Epoch 173: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2502 - mae: 0.5680 - val_loss: 0.6971 - val_mae: 1.0636 - learning_rate: 3.7500e-05\n",
      "Epoch 174/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2860 - mae: 0.6109\n",
      "Epoch 174: ReduceLROnPlateau reducing learning rate to 1.8750000890577212e-05.\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2841 - mae: 0.6085 - val_loss: 0.6990 - val_mae: 1.0659 - learning_rate: 3.7500e-05\n",
      "Epoch 175/200\n",
      "\u001b[1m20/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2612 - mae: 0.5806\n",
      "Epoch 175: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2624 - mae: 0.5820 - val_loss: 0.7013 - val_mae: 1.0684 - learning_rate: 1.8750e-05\n",
      "Epoch 176/200\n",
      "\u001b[1m22/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2745 - mae: 0.5963\n",
      "Epoch 176: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2744 - mae: 0.5962 - val_loss: 0.7010 - val_mae: 1.0683 - learning_rate: 1.8750e-05\n",
      "Epoch 177/200\n",
      "\u001b[1m18/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2609 - mae: 0.5804\n",
      "Epoch 177: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2632 - mae: 0.5835 - val_loss: 0.7030 - val_mae: 1.0701 - learning_rate: 1.8750e-05\n",
      "Epoch 178/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2841 - mae: 0.6072\n",
      "Epoch 178: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2821 - mae: 0.6048 - val_loss: 0.7048 - val_mae: 1.0719 - learning_rate: 1.8750e-05\n",
      "Epoch 179/200\n",
      "\u001b[1m21/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2788 - mae: 0.6001\n",
      "Epoch 179: val_loss did not improve from 0.65362\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2778 - mae: 0.5990 - val_loss: 0.7116 - val_mae: 1.0789 - learning_rate: 1.8750e-05\n",
      "Epoch 179: early stopping\n",
      "Restoring model weights from the end of the best epoch: 154.\n",
      "✅ Training completed!\n",
      "\n",
      "==================================================\n",
      "EVALUATING OPTIMIZED MODEL PERFORMANCE\n",
      "==================================================\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 285ms/step\n",
      "\n",
      "OPTIMIZED MODEL RESULTS:\n",
      "========================================\n",
      "\n",
      "Breakfast:\n",
      "  MAE: $828.02\n",
      "  MAPE: 65.90%\n",
      "  Correlation: 0.489\n",
      "\n",
      "Dinner:\n",
      "  MAE: $1113.41\n",
      "  MAPE: 35.27%\n",
      "  Correlation: 0.703\n",
      "\n",
      "Lunch:\n",
      "  MAE: $607.71\n",
      "  MAPE: 104.58%\n",
      "  Correlation: 0.528\n",
      "\n",
      "========================================\n",
      "OVERALL OPTIMIZED MODEL PERFORMANCE:\n",
      "  MAE: $849.72\n",
      "  MAPE: 68.58%\n",
      "  Correlation: 0.768\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "Model Performance Comparison:\n",
      "----------------------------------------------------------------------\n",
      "Metric          Previous             Optimized            Improvement    \n",
      "----------------------------------------------------------------------\n",
      "MAE             $859.00             $849.72             +1.1%\n",
      "MAPE            69.0%              68.6%              +0.5pp\n",
      "Correlation     0.540              0.768              +42.3%\n",
      "\n",
      "==================================================\n",
      "PLOTTING TRAINING PROGRESS\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMIZATION COMPLETE! 🚀\n",
      "================================================================================\n",
      "✅ IMPROVEMENTS ACHIEVED:\n",
      "   • Features reduced: 65 → 22 (66.2% reduction)\n",
      "   • MAE: $859.00 → $849.72 (+1.1%)\n",
      "   • MAPE: 69.0% → 68.6% (+0.5pp)\n",
      "   • Correlation: 0.540 → 0.768 (+42.3%)\n",
      "\n",
      "📊 FINAL REDUCED FEATURE SET (22 features):\n",
      "    1. IsRamadan\n",
      "    2. Tourism_0\n",
      "    3. Month_cos\n",
      "    4. IsFoodFestival\n",
      "    5. Event_Ramadan-Middle\n",
      "    6. IsPreRamadan\n",
      "    7. Impact_0\n",
      "    8. DayOfWeek_sin\n",
      "    9. Event_Normal\n",
      "   10. Event_Pre-Ramadan-Late\n",
      "   11. IsLast10Ramadan\n",
      "   12. Impact_-1\n",
      "   13. Event_Pre-Ramadan-Early\n",
      "   14. Event_Ramadan-First10Days\n",
      "   15. Tourism_1\n",
      "   16. Event_Dubai-Food-Festival\n",
      "   17. CheckTotal\n",
      "   18. Event_Ramadan-Last10Days\n",
      "   19. is_zero\n",
      "   20. IsPreEvent\n",
      "   21. DayOfWeek_cos\n",
      "   22. Event_Pre-Dubai-Food-Festival\n",
      "\n",
      "🎯 MODEL READY FOR PRODUCTION!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE REDUCTION AND MODEL RETRAINING - FIXED IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, LSTM, Dense, Dropout, MaxPooling1D, \n",
    "    BatchNormalization, Reshape\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"IMPLEMENTING FEATURE REDUCTION BASED ON RELEVANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define the top 22 features identified from relevance analysis\n",
    "recommended_features = [\n",
    "    'IsRamadan', 'Tourism_0', 'Month_cos', 'IsFoodFestival', \n",
    "    'Event_Ramadan-Middle', 'IsPreRamadan', 'Impact_0', 'DayOfWeek_sin', \n",
    "    'Event_Normal', 'Event_Pre-Ramadan-Late', 'IsLast10Ramadan', 'Impact_-1', \n",
    "    'Event_Pre-Ramadan-Early', 'Event_Ramadan-First10Days', 'Tourism_1', \n",
    "    'Event_Dubai-Food-Festival', 'CheckTotal', 'Event_Ramadan-Last10Days', \n",
    "    'is_zero', 'IsPreEvent', 'DayOfWeek_cos', 'Event_Pre-Dubai-Food-Festival'\n",
    "]\n",
    "\n",
    "print(f\"Original features: {len(feature_cols)}\")\n",
    "print(f\"Recommended features: {len(recommended_features)}\")\n",
    "print(f\"Reduction: {((len(feature_cols) - len(recommended_features)) / len(feature_cols)) * 100:.1f}%\")\n",
    "\n",
    "# Get indices of recommended features\n",
    "feature_indices = []\n",
    "for feature in recommended_features:\n",
    "    if feature in feature_cols:\n",
    "        feature_indices.append(feature_cols.index(feature))\n",
    "    else:\n",
    "        print(f\"Warning: Feature '{feature}' not found in feature_cols\")\n",
    "\n",
    "print(f\"✓ Found {len(feature_indices)} matching features\")\n",
    "\n",
    "# Create reduced feature list\n",
    "feature_cols_reduced = [feature_cols[i] for i in feature_indices]\n",
    "print(f\"✓ Reduced feature set: {feature_cols_reduced}\")\n",
    "\n",
    "# Apply feature reduction to training and test sets\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REDUCING FEATURE DIMENSIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "X_train_reduced = X_train[:, :, feature_indices]\n",
    "X_test_reduced = X_test[:, :, feature_indices]\n",
    "\n",
    "print(f\"Original shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"Reduced shapes:\")\n",
    "print(f\"  X_train_reduced: {X_train_reduced.shape}\")\n",
    "print(f\"  X_test_reduced: {X_test_reduced.shape}\")\n",
    "\n",
    "# Calculate new parameters ratio\n",
    "original_features = X_train.shape[2]\n",
    "reduced_features = X_train_reduced.shape[2]\n",
    "training_samples = X_train.shape[0]\n",
    "\n",
    "print(f\"\\nFeature reduction analysis:\")\n",
    "print(f\"  Original samples per feature: {training_samples / original_features:.1f}\")\n",
    "print(f\"  New samples per feature: {training_samples / reduced_features:.1f}\")\n",
    "print(f\"  Improvement factor: {(training_samples / reduced_features) / (training_samples / original_features):.1f}x\")\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD OPTIMIZED MODEL WITH REDUCED FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def build_optimized_cnn_lstm_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Build optimized CNN-LSTM model with:\n",
    "    - Batch normalization for stability\n",
    "    - Reduced complexity to prevent overfitting\n",
    "    - Better regularization\n",
    "    \"\"\"\n",
    "    print(f\"✓ Building optimized model with input shape: {input_shape}\")\n",
    "    print(f\"✓ Output shape: {output_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # First CNN block with batch normalization\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape, name='conv1d_1'),\n",
    "        BatchNormalization(name='batch_norm_1'),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', name='conv1d_2'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_1'),\n",
    "        Dropout(0.25, name='dropout_1'),\n",
    "        \n",
    "        # Second CNN block (reduced complexity)\n",
    "        Conv1D(filters=16, kernel_size=3, activation='relu', name='conv1d_3'),\n",
    "        BatchNormalization(name='batch_norm_2'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_2'),\n",
    "        Dropout(0.25, name='dropout_2'),\n",
    "        \n",
    "        # LSTM layers (reduced size to prevent overfitting)\n",
    "        LSTM(64, return_sequences=True, name='lstm_1'),\n",
    "        BatchNormalization(name='batch_norm_3'),\n",
    "        Dropout(0.3, name='dropout_3'),\n",
    "        LSTM(32, return_sequences=False, name='lstm_2'),\n",
    "        Dropout(0.3, name='dropout_4'),\n",
    "        \n",
    "        # Dense layers with batch normalization\n",
    "        Dense(64, activation='relu', name='dense_1'),\n",
    "        BatchNormalization(name='batch_norm_4'),\n",
    "        Dropout(0.25, name='dropout_5'),\n",
    "        Dense(32, activation='relu', name='dense_2'),\n",
    "        Dropout(0.2, name='dropout_6'),\n",
    "        Dense(np.prod(output_shape), activation='linear', name='dense_output'),\n",
    "    ])\n",
    "    \n",
    "    # Reshape output to (forecast_days, revenue_streams)\n",
    "    model.add(Reshape(output_shape, name='reshape_output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the optimized model\n",
    "input_shape = (X_train_reduced.shape[1], X_train_reduced.shape[2])  # (30, 22)\n",
    "output_shape = (y_train.shape[1], y_train.shape[2])  # (7, 3)\n",
    "\n",
    "model_optimized = build_optimized_cnn_lstm_model(input_shape, output_shape)\n",
    "\n",
    "# Compile with Huber loss and lower learning rate\n",
    "model_optimized.compile(\n",
    "    optimizer=Adam(learning_rate=0.0003),  # Lower learning rate\n",
    "    loss=Huber(delta=1.0),  # Better for outliers than MSE\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZED MODEL ARCHITECTURE\")\n",
    "print(\"=\"*50)\n",
    "model_optimized.summary()\n",
    "\n",
    "# Calculate parameter reduction\n",
    "def count_parameters(model):\n",
    "    return sum([np.prod(tf.keras.backend.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "optimized_params = count_parameters(model_optimized)\n",
    "print(f\"\\nModel complexity comparison:\")\n",
    "print(f\"  Optimized model parameters: {optimized_params:,}\")\n",
    "print(f\"  Parameters per training sample: {optimized_params / training_samples:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# ENHANCED TRAINING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SETTING UP ENHANCED TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,  # More patience for stable training\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_optimized_cnn_lstm_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Enhanced callbacks configured\")\n",
    "print(\"  - EarlyStopping: patience=25\")\n",
    "print(\"  - ReduceLROnPlateau: factor=0.5, patience=10\")\n",
    "print(\"  - ModelCheckpoint: saves best model\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN THE OPTIMIZED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING OPTIMIZED MODEL WITH REDUCED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train with smaller batch size for better gradient updates\n",
    "batch_size = 16  # Smaller batch size\n",
    "epochs = 200     # More epochs with early stopping\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Max epochs: {epochs}\")\n",
    "print(f\"  Learning rate: 0.0003\")\n",
    "print(f\"  Loss function: Huber (delta=1.0)\")\n",
    "\n",
    "# Start training\n",
    "history_optimized = model_optimized.fit(\n",
    "    X_train_reduced, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test_reduced, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"✅ Training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE OPTIMIZED MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING OPTIMIZED MODEL PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load best model\n",
    "model_optimized.load_weights('best_optimized_cnn_lstm_model.h5')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_optimized = model_optimized.predict(X_test_reduced)\n",
    "\n",
    "# Denormalize predictions and targets for evaluation\n",
    "y_test_denorm = target_scaler.inverse_transform(\n",
    "    y_test.reshape(-1, y_test.shape[-1])\n",
    ").reshape(y_test.shape)\n",
    "\n",
    "y_pred_denorm_optimized = target_scaler.inverse_transform(\n",
    "    y_pred_optimized.reshape(-1, y_pred_optimized.shape[-1])\n",
    ").reshape(y_pred_optimized.shape)\n",
    "\n",
    "# Calculate metrics for optimized model\n",
    "print(\"\\nOPTIMIZED MODEL RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "revenue_streams = ['Breakfast', 'Dinner', 'Lunch']\n",
    "optimized_results = {}\n",
    "\n",
    "for i, stream in enumerate(revenue_streams):\n",
    "    y_true = y_test_denorm[:, :, i].flatten()\n",
    "    y_pred = y_pred_denorm_optimized[:, :, i].flatten()\n",
    "    \n",
    "    # Remove any potential NaN values\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
    "    correlation, _ = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    optimized_results[stream] = {\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'Correlation': correlation\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{stream}:\")\n",
    "    print(f\"  MAE: ${mae:.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  Correlation: {correlation:.3f}\")\n",
    "\n",
    "# Overall metrics\n",
    "all_true = y_test_denorm.flatten()\n",
    "all_pred = y_pred_denorm_optimized.flatten()\n",
    "mask = ~(np.isnan(all_true) | np.isnan(all_pred))\n",
    "all_true = all_true[mask]\n",
    "all_pred = all_pred[mask]\n",
    "\n",
    "overall_mae = np.mean(np.abs(all_true - all_pred))\n",
    "overall_mape = np.mean(np.abs((all_true - all_pred) / (all_true + 1e-8))) * 100\n",
    "overall_correlation, _ = pearsonr(all_true, all_pred)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*40)\n",
    "print(\"OVERALL OPTIMIZED MODEL PERFORMANCE:\")\n",
    "print(f\"  MAE: ${overall_mae:.2f}\")\n",
    "print(f\"  MAPE: {overall_mape:.2f}%\")\n",
    "print(f\"  Correlation: {overall_correlation:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON WITH PREVIOUS MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Previous model results (from your summary)\n",
    "previous_results = {\n",
    "    'Overall': {'MAE': 859, 'MAPE': 69.04, 'Correlation': 0.54}  # Average of your results\n",
    "}\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<15} {'Previous':<20} {'Optimized':<20} {'Improvement':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "prev = previous_results['Overall']\n",
    "\n",
    "# MAE comparison\n",
    "mae_improvement = ((prev['MAE'] - overall_mae) / prev['MAE']) * 100\n",
    "print(f\"{'MAE':<15} ${prev['MAE']:.2f}{'':>12} ${overall_mae:.2f}{'':>12} {mae_improvement:+.1f}%\")\n",
    "\n",
    "# MAPE comparison\n",
    "mape_improvement = prev['MAPE'] - overall_mape\n",
    "print(f\"{'MAPE':<15} {prev['MAPE']:.1f}%{'':>13} {overall_mape:.1f}%{'':>13} {mape_improvement:+.1f}pp\")\n",
    "\n",
    "# Correlation comparison\n",
    "corr_improvement = ((overall_correlation - prev['Correlation']) / prev['Correlation']) * 100\n",
    "print(f\"{'Correlation':<15} {prev['Correlation']:.3f}{'':>13} {overall_correlation:.3f}{'':>13} {corr_improvement:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZE TRAINING PROGRESS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PLOTTING TRAINING PROGRESS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history_optimized.history['loss'], label='Training Loss', alpha=0.8)\n",
    "plt.plot(history_optimized.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "plt.title('Model Loss (Huber)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training MAE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history_optimized.history['mae'], label='Training MAE', alpha=0.8)\n",
    "plt.plot(history_optimized.history['val_mae'], label='Validation MAE', alpha=0.8)\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "if 'lr' in history_optimized.history:\n",
    "    plt.plot(history_optimized.history['lr'], alpha=0.8, color='red')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.yscale('log')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Learning Rate\\nNot Recorded', \n",
    "             transform=plt.gca().transAxes, ha='center', va='center')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZATION COMPLETE! 🚀\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"✅ IMPROVEMENTS ACHIEVED:\")\n",
    "print(f\"   • Features reduced: 65 → {len(feature_indices)} ({((65-len(feature_indices))/65)*100:.1f}% reduction)\")\n",
    "print(f\"   • MAE: ${prev['MAE']:.2f} → ${overall_mae:.2f} ({mae_improvement:+.1f}%)\")\n",
    "print(f\"   • MAPE: {prev['MAPE']:.1f}% → {overall_mape:.1f}% ({mape_improvement:+.1f}pp)\")\n",
    "print(f\"   • Correlation: {prev['Correlation']:.3f} → {overall_correlation:.3f} ({corr_improvement:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n📊 FINAL REDUCED FEATURE SET ({len(feature_cols_reduced)} features):\")\n",
    "for i, feature in enumerate(feature_cols_reduced, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(\"\\n🎯 MODEL READY FOR PRODUCTION!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
