{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "✓ TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING CNN-LSTM READY DATASET\n",
      "==================================================\n",
      "✓ Transformed data shape: (1458, 65)\n",
      "✓ Target data shape: (1458, 4)\n",
      "✓ Transformed data columns: 65 features\n",
      "✓ Target data columns: 4 target variables\n",
      "\n",
      "First 3 rows of transformed data:\n",
      "       Year  CheckTotal   is_zero  IsRamadan     IsEid  IsPreRamadan  \\\n",
      "0 -0.575766    0.015624 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "1 -0.575766    2.072745 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "2 -0.575766   -0.155666 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "\n",
      "   IsPostRamadan  IsLast10Ramadan     IsDSF  IsSummerEvent  ...  \\\n",
      "0      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "1      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "2      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "\n",
      "   Event_Ramadan-First10Days  Event_Ramadan-Last10Days  Event_Ramadan-Middle  \\\n",
      "0                  -0.207168                 -0.207168             -0.201706   \n",
      "1                  -0.207168                 -0.207168             -0.201706   \n",
      "2                  -0.207168                 -0.207168             -0.201706   \n",
      "\n",
      "   Tourism_0  Tourism_1  Tourism_2  Tourism_3  Impact_-1  Impact_0  Impact_1  \n",
      "0  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "1  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "2  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "\n",
      "[3 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load the transformed datasets\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING CNN-LSTM READY DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the datasets\n",
    "df_transformed = pd.read_csv('cnn_lstm_ready_dataset.csv')\n",
    "target_data = pd.read_csv('target_data_for_sequences.csv')\n",
    "\n",
    "print(f\"✓ Transformed data shape: {df_transformed.shape}\")\n",
    "print(f\"✓ Target data shape: {target_data.shape}\")\n",
    "print(f\"✓ Transformed data columns: {len(df_transformed.columns)} features\")\n",
    "print(f\"✓ Target data columns: {len(target_data.columns)} target variables\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 3 rows of transformed data:\")\n",
    "print(df_transformed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_for_cnn_lstm(df_transformed, target_data, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences for CNN-LSTM training from loaded CSV files\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length  # Look back 30 days\n",
    "    FORECAST_HORIZON = forecast_horizon  # Predict next 7 days\n",
    "    \n",
    "    # Sort by date to ensure proper sequence order\n",
    "    df_transformed_sorted = df_transformed.sort_values('Date').reset_index(drop=True)\n",
    "    target_data_sorted = target_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Pivot target data to wide format\n",
    "    target_pivot = target_data_sorted.pivot_table(\n",
    "        index='Date', \n",
    "        columns=['RevenueCenterName', 'MealPeriod'], \n",
    "        values='CheckTotal', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create column names for revenue streams\n",
    "    target_pivot.columns = ['Date'] + [f\"{col[0]}_{col[1]}\" for col in target_pivot.columns[1:]]\n",
    "    \n",
    "    # Ensure same date range\n",
    "    common_dates = set(df_transformed_sorted['Date']).intersection(set(target_pivot['Date']))\n",
    "    df_transformed_sorted = df_transformed_sorted[df_transformed_sorted['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    target_pivot = target_pivot[target_pivot['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    \n",
    "    # Remove Date column from features\n",
    "    feature_columns = [col for col in df_transformed_sorted.columns if col != 'Date']\n",
    "    features = df_transformed_sorted[feature_columns].values\n",
    "    \n",
    "    # Target columns (revenue targets)\n",
    "    target_columns = [col for col in target_pivot.columns if col != 'Date']\n",
    "    targets = target_pivot[target_columns].values\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Number of feature columns: {len(feature_columns)}\")\n",
    "    print(f\"✓ Number of target columns: {len(target_columns)}\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past 30 days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next 7 days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, 30, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, 7, revenue_targets)\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Target normalization functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3A: Target Normalization Functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "def normalize_targets(y_train, y_test, save_scaler=True):\n",
    "    \"\"\"\n",
    "    Normalize target values for better training stability\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"NORMALIZING TARGET VALUES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Original data info\n",
    "    print(f\"📊 Original target ranges:\")\n",
    "    print(f\"  y_train: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "    print(f\"  y_test: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
    "    \n",
    "    # Reshape for normalization: (samples, days, streams) -> (samples*days, streams)\n",
    "    original_train_shape = y_train.shape\n",
    "    original_test_shape = y_test.shape\n",
    "    \n",
    "    y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])  # (samples*days, 3)\n",
    "    y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])     # (samples*days, 3)\n",
    "    \n",
    "    print(f\"✓ Reshaped for scaling:\")\n",
    "    print(f\"  y_train: {original_train_shape} -> {y_train_reshaped.shape}\")\n",
    "    print(f\"  y_test: {original_test_shape} -> {y_test_reshaped.shape}\")\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_normalized = target_scaler.fit_transform(y_train_reshaped)\n",
    "    y_test_normalized = target_scaler.transform(y_test_reshaped)\n",
    "    \n",
    "    # Reshape back to original format\n",
    "    y_train_normalized = y_train_normalized.reshape(original_train_shape)\n",
    "    y_test_normalized = y_test_normalized.reshape(original_test_shape)\n",
    "    \n",
    "    print(f\"✓ Normalized target ranges:\")\n",
    "    print(f\"  y_train: {y_train_normalized.min():.3f} - {y_train_normalized.max():.3f}\")\n",
    "    print(f\"  y_test: {y_test_normalized.min():.3f} - {y_test_normalized.max():.3f}\")\n",
    "    print(f\"  Mean: {y_train_normalized.mean():.3f}, Std: {y_train_normalized.std():.3f}\")\n",
    "    \n",
    "    # Save scaler for later denormalization\n",
    "    if save_scaler:\n",
    "        joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "        print(f\"✅ Target scaler saved to 'target_scaler.pkl'\")\n",
    "    \n",
    "    return y_train_normalized, y_test_normalized, target_scaler\n",
    "\n",
    "def denormalize_predictions(predictions_normalized, target_scaler):\n",
    "    \"\"\"\n",
    "    Convert normalized predictions back to actual dollar amounts\n",
    "    \"\"\"\n",
    "    original_shape = predictions_normalized.shape\n",
    "    \n",
    "    # Reshape for denormalization\n",
    "    pred_reshaped = predictions_normalized.reshape(-1, predictions_normalized.shape[-1])\n",
    "    \n",
    "    # Denormalize\n",
    "    pred_actual = target_scaler.inverse_transform(pred_reshaped)\n",
    "    \n",
    "    # Reshape back\n",
    "    pred_actual = pred_actual.reshape(original_shape)\n",
    "    \n",
    "    return pred_actual\n",
    "\n",
    "print(\"✅ Target normalization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLEANING AND PREPARING DATA FOR CNN-LSTM\n",
      "==================================================\n",
      "Original data info:\n",
      "df_transformed shape: (1458, 65)\n",
      "df_transformed columns: ['Year', 'CheckTotal', 'is_zero', 'IsRamadan', 'IsEid', 'IsPreRamadan', 'IsPostRamadan', 'IsLast10Ramadan', 'IsDSF', 'IsSummerEvent', 'IsNationalDay', 'IsNewYear', 'IsMarathon', 'IsGITEX', 'IsAirshow', 'IsFoodFestival', 'IsPreEvent', 'IsPostEvent', 'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Meal_Breakfast', 'Meal_Dinner', 'Meal_Lunch', 'Event_Dubai-Airshow', 'Event_Dubai-Food-Festival', 'Event_Dubai-Marathon', 'Event_Dubai-Shopping-Festival', 'Event_Dubai-Summer-Surprises', 'Event_Eid-Adha', 'Event_Flag-Day', 'Event_GITEX-Technology-Week', 'Event_New-Year-Celebrations', 'Event_Normal', 'Event_Post-Dubai-Airshow', 'Event_Post-Dubai-Marathon', 'Event_Post-Eid-Adha', 'Event_Post-Flag-Day', 'Event_Post-GITEX-Technology-Week', 'Event_Post-New-Year-Celebrations', 'Event_Post-Ramadan-Recovery', 'Event_Post-Ramadan-Week1', 'Event_Post-Summer-Event', 'Event_Pre-Commemoration-Day', 'Event_Pre-DSF', 'Event_Pre-Dubai-Airshow', 'Event_Pre-Dubai-Food-Festival', 'Event_Pre-Dubai-Marathon', 'Event_Pre-Eid-Adha', 'Event_Pre-Flag-Day', 'Event_Pre-GITEX-Technology-Week', 'Event_Pre-Ramadan-Early', 'Event_Pre-Ramadan-Late', 'Event_Pre-UAE-National-Day', 'Event_Ramadan-First10Days', 'Event_Ramadan-Last10Days', 'Event_Ramadan-Middle', 'Tourism_0', 'Tourism_1', 'Tourism_2', 'Tourism_3', 'Impact_-1', 'Impact_0', 'Impact_1']\n",
      "target_data shape: (1458, 4)\n",
      "target_data columns: ['Date', 'RevenueCenterName', 'MealPeriod', 'CheckTotal']\n",
      "✓ Data lengths match - assuming already aligned by row index\n",
      "\n",
      "🔄 Pivoting target data to wide format...\n",
      "✓ Pivoted target shape: (486, 4)\n",
      "✓ Pivoted target columns: ['day_id', 'Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "📊 Aggregating features to day level...\n",
      "✓ Aggregated features shape: (486, 65)\n",
      "✓ Final aligned shapes:\n",
      "Features: (486, 65)\n",
      "Targets: (486, 3)\n",
      "\n",
      "🧹 Cleaning data types...\n",
      "✅ Final cleaned data:\n",
      "Features shape: (486, 65)\n",
      "Targets shape: (486, 3)\n",
      "Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "Data lengths match: True\n",
      "\n",
      "==================================================\n",
      "CREATING SEQUENCES FOR CNN-LSTM\n",
      "==================================================\n",
      "✓ Feature shape: (486, 65)\n",
      "✓ Target shape: (486, 3)\n",
      "✓ Sequence length: 30 days\n",
      "✓ Forecast horizon: 7 days\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Final X shape: (450, 30, 65)\n",
      "✓ Final y shape: (450, 7, 3)\n",
      "✓ X dtype: float32\n",
      "✓ y dtype: float32\n",
      "✓ Total sequences created: 450\n",
      "\n",
      "📊 Shape interpretation:\n",
      "X: (450 sequences, 30 days history, 65 features)\n",
      "y: (450 sequences, 7 days forecast, 3 revenue streams)\n",
      "\n",
      "🎉 SUCCESS! Sequences created successfully!\n",
      "✓ Input sequences (X): (450, 30, 65)\n",
      "✓ Output sequences (y): (450, 7, 3)\n",
      "✓ Feature columns: 65\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Data types: X=float32, y=float32\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Corrected - Handle data without Date column in features\n",
    "def clean_and_prepare_data_fixed(df_transformed, target_data):\n",
    "    \"\"\"\n",
    "    Clean dataframes when features don't have Date column\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLEANING AND PREPARING DATA FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Check original data\n",
    "    print(\"Original data info:\")\n",
    "    print(f\"df_transformed shape: {df_transformed.shape}\")\n",
    "    print(f\"df_transformed columns: {list(df_transformed.columns)}\")\n",
    "    print(f\"target_data shape: {target_data.shape}\")\n",
    "    print(f\"target_data columns: {list(target_data.columns)}\")\n",
    "    \n",
    "    # Check if data is already aligned by length\n",
    "    if len(df_transformed) == len(target_data):\n",
    "        print(\"✓ Data lengths match - assuming already aligned by row index\")\n",
    "        \n",
    "        # Step 2: Pivot target data from long to wide format\n",
    "        print(\"\\n🔄 Pivoting target data to wide format...\")\n",
    "        \n",
    "        # Add row index to help with pivoting\n",
    "        target_with_index = target_data.copy()\n",
    "        target_with_index['row_index'] = target_with_index.index\n",
    "        \n",
    "        # Create a day identifier (since we know there are 3 meal periods per day)\n",
    "        target_with_index['day_id'] = target_with_index['row_index'] // 3\n",
    "        \n",
    "        target_pivot = target_with_index.pivot_table(\n",
    "            index='day_id', \n",
    "            columns='MealPeriod', \n",
    "            values='CheckTotal', \n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "        \n",
    "        print(f\"✓ Pivoted target shape: {target_pivot.shape}\")\n",
    "        print(f\"✓ Pivoted target columns: {list(target_pivot.columns)}\")\n",
    "        \n",
    "        # Step 3: Aggregate features to day level (average of 3 meal periods per day)\n",
    "        print(\"\\n📊 Aggregating features to day level...\")\n",
    "        \n",
    "        # Add day_id to features\n",
    "        df_features_with_day = df_transformed.copy()\n",
    "        df_features_with_day['day_id'] = df_features_with_day.index // 3\n",
    "        \n",
    "        # Aggregate features by day (mean of the 3 meal periods)\n",
    "        df_features_daily = df_features_with_day.groupby('day_id').mean().reset_index()\n",
    "        df_features_daily = df_features_daily.drop('day_id', axis=1)\n",
    "        \n",
    "        print(f\"✓ Aggregated features shape: {df_features_daily.shape}\")\n",
    "        \n",
    "        # Step 4: Align the data\n",
    "        target_values = target_pivot.drop('day_id', axis=1)\n",
    "        \n",
    "        # Ensure same number of rows\n",
    "        min_rows = min(len(df_features_daily), len(target_values))\n",
    "        df_features_final = df_features_daily.iloc[:min_rows]\n",
    "        target_values_final = target_values.iloc[:min_rows]\n",
    "        \n",
    "        print(f\"✓ Final aligned shapes:\")\n",
    "        print(f\"Features: {df_features_final.shape}\")\n",
    "        print(f\"Targets: {target_values_final.shape}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Data length mismatch: features={len(df_transformed)}, targets={len(target_data)}\")\n",
    "    \n",
    "    # Step 5: Clean data types and handle missing values\n",
    "    print(\"\\n🧹 Cleaning data types...\")\n",
    "    \n",
    "    # Features: ensure all numeric\n",
    "    df_features_clean = df_features_final.select_dtypes(include=[np.number])\n",
    "    df_features_clean = df_features_clean.fillna(0).astype(np.float32)\n",
    "    \n",
    "    # Targets: ensure all numeric\n",
    "    df_targets_clean = target_values_final.fillna(0).astype(np.float32)\n",
    "    \n",
    "    print(f\"✅ Final cleaned data:\")\n",
    "    print(f\"Features shape: {df_features_clean.shape}\")\n",
    "    print(f\"Targets shape: {df_targets_clean.shape}\")\n",
    "    print(f\"Target columns: {list(df_targets_clean.columns)}\")\n",
    "    print(f\"Data lengths match: {len(df_features_clean) == len(df_targets_clean)}\")\n",
    "    \n",
    "    return df_features_clean, df_targets_clean\n",
    "\n",
    "def create_sequences_for_cnn_lstm_corrected(df_features, df_targets, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences from properly aligned and cleaned data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length\n",
    "    FORECAST_HORIZON = forecast_horizon\n",
    "    \n",
    "    # Convert to arrays\n",
    "    features = df_features.values\n",
    "    targets = df_targets.values\n",
    "    feature_columns = df_features.columns.tolist()\n",
    "    target_columns = df_targets.columns.tolist()\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    \n",
    "    # Verify we have enough data\n",
    "    min_data_needed = SEQ_LENGTH + FORECAST_HORIZON\n",
    "    if len(features) < min_data_needed:\n",
    "        raise ValueError(f\"Not enough data. Need at least {min_data_needed} rows, got {len(features)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past SEQ_LENGTH days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next FORECAST_HORIZON days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, sequence_length, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, forecast_horizon, revenue_streams)\n",
    "    print(f\"✓ X dtype: {X.dtype}\")\n",
    "    print(f\"✓ y dtype: {y.dtype}\")\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    # Show example of what each dimension means\n",
    "    print(f\"\\n📊 Shape interpretation:\")\n",
    "    print(f\"X: ({X.shape[0]} sequences, {X.shape[1]} days history, {X.shape[2]} features)\")\n",
    "    print(f\"y: ({y.shape[0]} sequences, {y.shape[1]} days forecast, {y.shape[2]} revenue streams)\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns\n",
    "\n",
    "# Execute the corrected pipeline\n",
    "try:\n",
    "    # Step 1: Clean and prepare data without Date column dependency\n",
    "    df_features_clean, df_targets_clean = clean_and_prepare_data_fixed(df_transformed, target_data)\n",
    "    \n",
    "    # Step 2: Create sequences\n",
    "    X, y, feature_cols, target_cols = create_sequences_for_cnn_lstm_corrected(\n",
    "        df_features_clean, df_targets_clean\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 SUCCESS! Sequences created successfully!\")\n",
    "    print(f\"✓ Input sequences (X): {X.shape}\")\n",
    "    print(f\"✓ Output sequences (y): {y.shape}\")\n",
    "    print(f\"✓ Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"✓ Target columns: {target_cols}\")\n",
    "    print(f\"✓ Data types: X={X.dtype}, y={y.dtype}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5757663   0.6442341  -0.08310281 ... -0.3115533  -2.6150928\n",
      "    4.957716  ]\n",
      "  [-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.01185533 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.2981966  -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.48062024 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.7368157  -0.7176877  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.35829824 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.2725303  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.03529583 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]]\n",
      "----------------------------------------------------------\n",
      "[[[ 473.  4368.   250. ]\n",
      "  [ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  ...\n",
      "  [1994.  2686.   247. ]\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]]\n",
      "\n",
      " [[ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  ...\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]]\n",
      "\n",
      " [[ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  [1994.  2686.   247. ]\n",
      "  ...\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]\n",
      "  [ 745.  1673.   258. ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 335.  2032.9  809.5]\n",
      "  [ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  ...\n",
      "  [1005.  2840.  1224. ]\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]]\n",
      "\n",
      " [[ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  ...\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]]\n",
      "\n",
      " [[ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  [1005.  2840.  1224. ]\n",
      "  ...\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]\n",
      "  [ 566.  2503.  1242. ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAIN-TEST SPLIT\n",
      "==============================\n",
      "✓ Training sequences: 360\n",
      "✓ Testing sequences: 90\n",
      "✓ Input shape per sample: (30, 65)\n",
      "✓ Output shape per sample: (7, 3)\n",
      "✓ X_train dtype: float32\n",
      "✓ y_train dtype: float32\n",
      "✓ X_test dtype: float32\n",
      "✓ y_test dtype: float32\n",
      "\n",
      "✓ Data quality check:\n",
      "X_train NaN count: 0\n",
      "y_train NaN count: 0\n",
      "X_train Inf count: 0\n",
      "y_train Inf count: 0\n",
      "\n",
      "✅ Data is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train-Test Split with clean data\n",
    "print(\"=\"*30)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Time-based split (80% train, 20% test)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"✓ Training sequences: {X_train.shape[0]}\")\n",
    "print(f\"✓ Testing sequences: {X_test.shape[0]}\")\n",
    "print(f\"✓ Input shape per sample: {X_train.shape[1:]}\")\n",
    "print(f\"✓ Output shape per sample: {y_train.shape[1:]}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"✓ X_train dtype: {X_train.dtype}\")\n",
    "print(f\"✓ y_train dtype: {y_train.dtype}\")\n",
    "print(f\"✓ X_test dtype: {X_test.dtype}\")\n",
    "print(f\"✓ y_test dtype: {y_test.dtype}\")\n",
    "\n",
    "# Check for any problematic values\n",
    "print(f\"\\n✓ Data quality check:\")\n",
    "print(f\"X_train NaN count: {np.isnan(X_train).sum()}\")\n",
    "print(f\"y_train NaN count: {np.isnan(y_train).sum()}\")\n",
    "print(f\"X_train Inf count: {np.isinf(X_train).sum()}\")\n",
    "print(f\"y_train Inf count: {np.isinf(y_train).sum()}\")\n",
    "\n",
    "print(f\"\\n✅ Data is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model building function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define CNN-LSTM model architecture\n",
    "def build_cnn_lstm_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Build CNN-LSTM hybrid model for hotel revenue forecasting\n",
    "    \"\"\"\n",
    "    print(f\"✓ Building model with input shape: {input_shape}\")\n",
    "    print(f\"✓ Output shape: {output_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, name='conv1d_1'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_1'),\n",
    "        Dropout(0.2, name='dropout_1'),\n",
    "        \n",
    "        # More CNN layers\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', name='conv1d_3'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_2'),\n",
    "        Dropout(0.2, name='dropout_2'),\n",
    "        \n",
    "        # LSTM layers for temporal patterns\n",
    "        LSTM(100, return_sequences=True, name='lstm_1'),\n",
    "        Dropout(0.3, name='dropout_3'),\n",
    "        LSTM(50, return_sequences=False, name='lstm_2'),\n",
    "        Dropout(0.3, name='dropout_4'),\n",
    "        \n",
    "        # Dense layers for final prediction\n",
    "        Dense(100, activation='relu', name='dense_1'),\n",
    "        Dropout(0.2, name='dropout_5'),\n",
    "        Dense(np.prod(output_shape), activation='linear', name='dense_output'),\n",
    "    ])\n",
    "    \n",
    "    # Reshape output to (forecast_days, revenue_streams)\n",
    "    model.add(tf.keras.layers.Reshape(output_shape, name='reshape_output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model building function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "APPLYING TARGET NORMALIZATION\n",
      "========================================\n",
      "==================================================\n",
      "NORMALIZING TARGET VALUES\n",
      "==================================================\n",
      "📊 Original target ranges:\n",
      "  y_train: $5.00 - $10052.50\n",
      "  y_test: $66.00 - $9657.00\n",
      "✓ Reshaped for scaling:\n",
      "  y_train: (360, 7, 3) -> (2520, 3)\n",
      "  y_test: (90, 7, 3) -> (630, 3)\n",
      "✓ Normalized target ranges:\n",
      "  y_train: -1.588 - 9.795\n",
      "  y_test: -1.334 - 9.494\n",
      "  Mean: -0.000, Std: 1.000\n",
      "✅ Target scaler saved to 'target_scaler.pkl'\n",
      "✅ Target normalization applied!\n",
      "✅ Training will use normalized targets\n",
      "✅ Original targets preserved for comparison\n",
      "\n",
      "📊 Comparison:\n",
      "Original y_train range: $5.00 - $10052.50\n",
      "Normalized y_train range: -1.588 - 9.795\n"
     ]
    }
   ],
   "source": [
    "# Cell 6A: Apply Target Normalization\n",
    "print(\"=\"*40)\n",
    "print(\"APPLYING TARGET NORMALIZATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Store original targets for comparison\n",
    "y_train_original = y_train.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "# Apply normalization\n",
    "y_train_norm, y_test_norm, target_scaler = normalize_targets(y_train, y_test, save_scaler=True)\n",
    "\n",
    "# Update variables for training\n",
    "y_train = y_train_norm\n",
    "y_test = y_test_norm\n",
    "\n",
    "print(f\"✅ Target normalization applied!\")\n",
    "print(f\"✅ Training will use normalized targets\")\n",
    "print(f\"✅ Original targets preserved for comparison\")\n",
    "\n",
    "# Show the difference\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"Original y_train range: ${y_train_original.min():.2f} - ${y_train_original.max():.2f}\")\n",
    "print(f\"Normalized y_train range: {y_train.min():.3f} - {y_train.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "BUILDING MODEL\n",
      "==============================\n",
      "✓ Building model with input shape: (30, 65)\n",
      "✓ Output shape: (7, 3)\n",
      "\n",
      "==============================\n",
      "MODEL ARCHITECTURE\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,121</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │        \u001b[38;5;34m53,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m5,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │         \u001b[38;5;34m2,121\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (\u001b[38;5;33mReshape\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Total parameters: 121,693\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build and compile the model\n",
    "print(\"=\"*30)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (30, features)\n",
    "output_shape = (y_train.shape[1], y_train.shape[2])  # (7, revenue_streams)\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_lstm_model(input_shape, output_shape)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n✓ Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAINING SETUP\n",
      "==============================\n",
      "✓ Callbacks configured:\n",
      "  - Early stopping (patience=15)\n",
      "  - Learning rate reduction (factor=0.5, patience=5)\n",
      "  - Model checkpoint (best_cnn_lstm_model.h5)\n",
      "✓ Batch size: 32\n",
      "✓ Max epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup training callbacks\n",
    "print(\"=\"*30)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=15, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_lstm_model.h5', \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "print(\"✓ Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=15)\")\n",
    "print(\"  - Learning rate reduction (factor=0.5, patience=5)\")\n",
    "print(\"  - Model checkpoint (best_cnn_lstm_model.h5)\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "COMPREHENSIVE DATA PREPARATION\n",
      "========================================\n",
      "🔧 Cleaning and preparing data...\n",
      "🧹 Handling NaN and infinite values...\n",
      "✓ Final data types:\n",
      "  X_train: float32, shape: (360, 30, 65)\n",
      "  y_train: float32, shape: (360, 7, 3)\n",
      "  X_test: float32, shape: (90, 30, 65)\n",
      "  y_test: float32, shape: (90, 7, 3)\n",
      "✓ Data ranges:\n",
      "  X_train: [-2.615, 22.023]\n",
      "  y_train: [-1.588, 9.795]\n",
      "\n",
      "==============================\n",
      "STARTING TRAINING\n",
      "==============================\n",
      "Epoch 1/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.9150 - mae: 0.6489\n",
      "Epoch 1: val_loss improved from inf to 3.45716, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 0.9346 - mae: 0.6528 - val_loss: 3.4572 - val_mae: 1.3137 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8669 - mae: 0.6297\n",
      "Epoch 2: val_loss improved from 3.45716 to 3.23934, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8725 - mae: 0.6306 - val_loss: 3.2393 - val_mae: 1.2468 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8015 - mae: 0.5953\n",
      "Epoch 3: val_loss improved from 3.23934 to 2.74908, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8233 - mae: 0.6006 - val_loss: 2.7491 - val_mae: 1.1306 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.9111 - mae: 0.6246 \n",
      "Epoch 4: val_loss improved from 2.74908 to 2.70749, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.8503 - mae: 0.6109 - val_loss: 2.7075 - val_mae: 1.1235 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6866 - mae: 0.5781\n",
      "Epoch 5: val_loss did not improve from 2.70749\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6882 - mae: 0.5786 - val_loss: 2.8854 - val_mae: 1.1613 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6817 - mae: 0.5653 \n",
      "Epoch 6: val_loss did not improve from 2.70749\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6904 - mae: 0.5679 - val_loss: 2.7517 - val_mae: 1.1256 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7296 - mae: 0.5729 \n",
      "Epoch 7: val_loss did not improve from 2.70749\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7133 - mae: 0.5701 - val_loss: 2.8940 - val_mae: 1.1538 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5902 - mae: 0.5366 \n",
      "Epoch 8: val_loss did not improve from 2.70749\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6242 - mae: 0.5491 - val_loss: 2.7996 - val_mae: 1.1320 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6274 - mae: 0.5418 \n",
      "Epoch 9: val_loss improved from 2.70749 to 2.57517, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6511 - mae: 0.5498 - val_loss: 2.5752 - val_mae: 1.0838 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7231 - mae: 0.5658 \n",
      "Epoch 10: val_loss did not improve from 2.57517\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7034 - mae: 0.5646 - val_loss: 2.6387 - val_mae: 1.1019 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6717 - mae: 0.5625\n",
      "Epoch 11: val_loss did not improve from 2.57517\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6653 - mae: 0.5619 - val_loss: 2.6595 - val_mae: 1.0966 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7415 - mae: 0.5768 \n",
      "Epoch 12: val_loss did not improve from 2.57517\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7268 - mae: 0.5722 - val_loss: 2.6057 - val_mae: 1.0867 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7469 - mae: 0.5886 \n",
      "Epoch 13: val_loss did not improve from 2.57517\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7173 - mae: 0.5781 - val_loss: 2.6137 - val_mae: 1.0868 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7902 - mae: 0.5808 \n",
      "Epoch 14: val_loss improved from 2.57517 to 2.54685, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7484 - mae: 0.5729 - val_loss: 2.5468 - val_mae: 1.0686 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6204 - mae: 0.5312 \n",
      "Epoch 15: val_loss improved from 2.54685 to 2.39348, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6424 - mae: 0.5395 - val_loss: 2.3935 - val_mae: 1.0381 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6491 - mae: 0.5565 \n",
      "Epoch 16: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6476 - mae: 0.5540 - val_loss: 2.4804 - val_mae: 1.0550 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6420 - mae: 0.5501\n",
      "Epoch 17: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6434 - mae: 0.5502 - val_loss: 2.5618 - val_mae: 1.0740 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7651 - mae: 0.5822\n",
      "Epoch 18: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7082 - mae: 0.5662 - val_loss: 2.5395 - val_mae: 1.0680 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5374 - mae: 0.5271 \n",
      "Epoch 19: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5824 - mae: 0.5364 - val_loss: 2.5007 - val_mae: 1.0617 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6568 - mae: 0.5596 \n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6469 - mae: 0.5533 - val_loss: 2.4254 - val_mae: 1.0476 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6429 - mae: 0.5523 \n",
      "Epoch 21: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6378 - mae: 0.5498 - val_loss: 2.5906 - val_mae: 1.0830 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6050 - mae: 0.5437 \n",
      "Epoch 22: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6132 - mae: 0.5417 - val_loss: 2.6174 - val_mae: 1.0895 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6554 - mae: 0.5530 \n",
      "Epoch 23: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6521 - mae: 0.5517 - val_loss: 2.3971 - val_mae: 1.0421 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m11/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.6221 - mae: 0.5373\n",
      "Epoch 24: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6251 - mae: 0.5385 - val_loss: 2.4291 - val_mae: 1.0496 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6864 - mae: 0.5610\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 25: val_loss did not improve from 2.39348\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6824 - mae: 0.5596 - val_loss: 2.4438 - val_mae: 1.0498 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6385 - mae: 0.5387\n",
      "Epoch 26: val_loss improved from 2.39348 to 2.35862, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6380 - mae: 0.5407 - val_loss: 2.3586 - val_mae: 1.0329 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5526 - mae: 0.5213 \n",
      "Epoch 27: val_loss improved from 2.35862 to 2.26065, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.5869 - mae: 0.5312 - val_loss: 2.2606 - val_mae: 1.0133 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5678 - mae: 0.5221 \n",
      "Epoch 28: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5876 - mae: 0.5288 - val_loss: 2.3077 - val_mae: 1.0244 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6760 - mae: 0.5511 \n",
      "Epoch 29: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6542 - mae: 0.5481 - val_loss: 2.2607 - val_mae: 1.0152 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.7040 - mae: 0.5603 \n",
      "Epoch 30: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6846 - mae: 0.5556 - val_loss: 2.3298 - val_mae: 1.0267 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6001 - mae: 0.5322 \n",
      "Epoch 31: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6017 - mae: 0.5326 - val_loss: 2.3228 - val_mae: 1.0278 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5183 - mae: 0.5050 \n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5533 - mae: 0.5174 - val_loss: 2.3682 - val_mae: 1.0402 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.5977 - mae: 0.5316\n",
      "Epoch 33: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6031 - mae: 0.5334 - val_loss: 2.3206 - val_mae: 1.0300 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5460 - mae: 0.5189 \n",
      "Epoch 34: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5778 - mae: 0.5272 - val_loss: 2.2906 - val_mae: 1.0236 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5806 - mae: 0.5248 \n",
      "Epoch 35: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5976 - mae: 0.5301 - val_loss: 2.2961 - val_mae: 1.0229 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5885 - mae: 0.5245 \n",
      "Epoch 36: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6046 - mae: 0.5301 - val_loss: 2.2978 - val_mae: 1.0222 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.5652 - mae: 0.5234 \n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5898 - mae: 0.5296 - val_loss: 2.3188 - val_mae: 1.0271 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6456 - mae: 0.5450 \n",
      "Epoch 38: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6395 - mae: 0.5441 - val_loss: 2.3242 - val_mae: 1.0296 - learning_rate: 6.2500e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5195 - mae: 0.5196 \n",
      "Epoch 39: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5472 - mae: 0.5242 - val_loss: 2.3338 - val_mae: 1.0314 - learning_rate: 6.2500e-05\n",
      "Epoch 40/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5893 - mae: 0.5324 \n",
      "Epoch 40: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6030 - mae: 0.5341 - val_loss: 2.3333 - val_mae: 1.0314 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.6116 - mae: 0.5309 \n",
      "Epoch 41: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6144 - mae: 0.5325 - val_loss: 2.3515 - val_mae: 1.0345 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.5531 - mae: 0.5184 \n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 42: val_loss did not improve from 2.26065\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5856 - mae: 0.5265 - val_loss: 2.3566 - val_mae: 1.0356 - learning_rate: 6.2500e-05\n",
      "Epoch 42: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "\n",
      "✅ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 Alternative: Comprehensive data cleaning and training\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"COMPREHENSIVE DATA PREPARATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def clean_and_prepare_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning for CNN-LSTM training\n",
    "    \"\"\"\n",
    "    print(\"🔧 Cleaning and preparing data...\")\n",
    "    \n",
    "    # Convert to numpy arrays if not already\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Check for object dtype issues\n",
    "    if X_train.dtype == 'object':\n",
    "        print(\"⚠️  X_train has object dtype - converting...\")\n",
    "        X_train = X_train.astype(np.float64)\n",
    "    \n",
    "    if y_train.dtype == 'object':\n",
    "        print(\"⚠️  y_train has object dtype - converting...\")\n",
    "        y_train = y_train.astype(np.float64)\n",
    "    \n",
    "    if X_test.dtype == 'object':\n",
    "        print(\"⚠️  X_test has object dtype - converting...\")\n",
    "        X_test = X_test.astype(np.float64)\n",
    "    \n",
    "    if y_test.dtype == 'object':\n",
    "        print(\"⚠️  y_test has object dtype - converting...\")\n",
    "        y_test = y_test.astype(np.float64)\n",
    "    \n",
    "    # Handle NaN and infinite values\n",
    "    print(\"🧹 Handling NaN and infinite values...\")\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Convert to float32 (TensorFlow's preferred type)\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Final verification\n",
    "    print(f\"✓ Final data types:\")\n",
    "    print(f\"  X_train: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.dtype}, shape: {y_train.shape}\")\n",
    "    print(f\"  X_test: {X_test.dtype}, shape: {X_test.shape}\")\n",
    "    print(f\"  y_test: {y_test.dtype}, shape: {y_test.shape}\")\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(f\"✓ Data ranges:\")\n",
    "    print(f\"  X_train: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "    print(f\"  y_train: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Clean the data\n",
    "X_train_clean, y_train_clean, X_test_clean, y_test_clean = clean_and_prepare_data(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Train with cleaned data\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train_clean, y_train_clean,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_test_clean, y_test_clean),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    \n",
    "    # Update variables for next cells\n",
    "    X_train, y_train = X_train_clean, y_train_clean\n",
    "    X_test, y_test = X_test_clean, y_test_clean\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training still failed: {e}\")\n",
    "    print(\"\\n🔍 Additional debugging:\")\n",
    "    \n",
    "    # More detailed debugging\n",
    "    print(f\"X_train unique dtypes: {set(str(x.dtype) for x in X_train.flatten()[:100])}\")\n",
    "    print(f\"Sample X_train values: {X_train[0, 0, :10]}\")\n",
    "    print(f\"Sample y_train values: {y_train[0, 0, :10]}\")\n",
    "    \n",
    "    # Check if data contains any strings\n",
    "    sample_x = X_train[0, 0, :]\n",
    "    print(f\"Sample X contains strings: {any(isinstance(x, str) for x in sample_x.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "==============================\n",
      "📊 NOTE: Model trained on normalized targets\n",
      "📊 Denormalizing predictions to actual dollar amounts\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000260B5CB3420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000260B5CB3420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000260B5CB3420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000260B5CB3420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions denormalized successfully\n",
      "\n",
      "🔍 Shape Debugging:\n",
      "X_test shape: (90, 30, 65)\n",
      "y_test_actual shape: (90, 7, 3)\n",
      "y_pred_actual shape: (90, 7, 3)\n",
      "\n",
      "📊 Revenue streams: ['Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "💰 Denormalized Value Ranges:\n",
      "  Actual revenue: $66.00 - $9657.00\n",
      "  Predicted revenue: $299.30 - $6406.11\n",
      "\n",
      "✅ Overall Test Metrics (in USD):\n",
      "  MAE: $859.11\n",
      "  RMSE: $1286.34\n",
      "  MAPE: 69.04%\n",
      "\n",
      "✅ Performance by Revenue Stream:\n",
      "  Breakfast: MAE = $804.04, Correlation = 0.431\n",
      "  Dinner: MAE = $1185.78, Correlation = 0.647\n",
      "  Lunch: MAE = $587.50, Correlation = 0.549\n",
      "\n",
      "✅ Sample Predictions (First sequence - in USD):\n",
      "Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\n",
      "-----------------------------------------------------------------------------------------------\n",
      " 1  | $    2466.00     | $  2360.19     | $ 6548.00     | $5794.64     | $2912.00     | $1564.16\n",
      " 2  | $    2586.80     | $  2358.25     | $ 4300.00     | $5949.73     | $2686.00     | $1637.47\n",
      " 3  | $    1639.60     | $  2282.98     | $ 6378.00     | $6039.08     | $2600.00     | $1753.92\n",
      " 4  | $    2079.60     | $  2346.12     | $ 6710.00     | $6071.00     | $2188.00     | $1690.61\n",
      " 5  | $    1232.00     | $  2397.49     | $ 4523.00     | $6406.11     | $2206.00     | $1960.21\n",
      " 6  | $    1390.00     | $  2354.33     | $ 3990.00     | $6069.86     | $1710.00     | $1964.65\n",
      " 7  | $    1426.00     | $  2315.93     | $ 5762.00     | $5758.33     | $ 682.00     | $1879.93\n",
      "\n",
      "✅ Evaluation complete with denormalized predictions!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Model Evaluation with Denormalization\n",
    "print(\"=\"*30)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# IMPORTANT: Model was trained on NORMALIZED targets\n",
    "# We need to denormalize predictions for evaluation\n",
    "print(\"📊 NOTE: Model trained on normalized targets\")\n",
    "print(\"📊 Denormalizing predictions to actual dollar amounts\")\n",
    "\n",
    "# Make predictions on normalized test set\n",
    "y_pred_normalized = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Load scaler and denormalize predictions\n",
    "try:\n",
    "    target_scaler = joblib.load('target_scaler.pkl')\n",
    "    y_pred_actual = denormalize_predictions(y_pred_normalized, target_scaler)\n",
    "    y_test_actual = y_test_original  # Use original non-normalized test targets\n",
    "    \n",
    "    print(f\"✅ Predictions denormalized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load scaler: {e}\")\n",
    "    print(f\"📊 Using normalized predictions for evaluation\")\n",
    "    y_pred_actual = y_pred_normalized\n",
    "    y_test_actual = y_test\n",
    "\n",
    "# Debug shapes\n",
    "print(f\"\\n🔍 Shape Debugging:\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test_actual shape: {y_test_actual.shape}\")\n",
    "print(f\"y_pred_actual shape: {y_pred_actual.shape}\")\n",
    "\n",
    "# Define revenue stream names\n",
    "revenue_streams = ['Breakfast', 'Dinner', 'Lunch']\n",
    "print(f\"\\n📊 Revenue streams: {revenue_streams}\")\n",
    "\n",
    "# Show data ranges (should be in dollars after denormalization)\n",
    "print(f\"\\n💰 Denormalized Value Ranges:\")\n",
    "print(f\"  Actual revenue: ${y_test_actual.min():.2f} - ${y_test_actual.max():.2f}\")\n",
    "print(f\"  Predicted revenue: ${y_pred_actual.min():.2f} - ${y_pred_actual.max():.2f}\")\n",
    "\n",
    "# Calculate metrics on actual dollar amounts\n",
    "y_test_flat = y_test_actual.reshape(-1)\n",
    "y_pred_flat = y_pred_actual.reshape(-1)\n",
    "\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test_flat - y_pred_flat) / (np.abs(y_test_flat) + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n✅ Overall Test Metrics (in USD):\")\n",
    "print(f\"  MAE: ${mae:.2f}\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Performance by revenue stream\n",
    "print(f\"\\n✅ Performance by Revenue Stream:\")\n",
    "for stream_idx, stream_name in enumerate(revenue_streams):\n",
    "    stream_mae = mean_absolute_error(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1), \n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )\n",
    "    stream_corr = np.corrcoef(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1),\n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )[0, 1]\n",
    "    print(f\"  {stream_name}: MAE = ${stream_mae:.2f}, Correlation = {stream_corr:.3f}\")\n",
    "\n",
    "# Sample predictions\n",
    "print(f\"\\n✅ Sample Predictions (First sequence - in USD):\")\n",
    "print(\"Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\")\n",
    "print(\"-\" * 95)\n",
    "for day in range(min(7, y_test_actual.shape[1])):\n",
    "    print(f\"{day+1:2d}  | ${y_test_actual[0, day, 0]:11.2f}     | ${y_pred_actual[0, day, 0]:9.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 1]:8.2f}     | ${y_pred_actual[0, day, 1]:6.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 2]:7.2f}     | ${y_pred_actual[0, day, 2]:5.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete with denormalized predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
