{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "✓ TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING CNN-LSTM READY DATASET\n",
      "==================================================\n",
      "✓ Transformed data shape: (1458, 65)\n",
      "✓ Target data shape: (1458, 4)\n",
      "✓ Transformed data columns: 65 features\n",
      "✓ Target data columns: 4 target variables\n",
      "Index(['Year', 'CheckTotal', 'is_zero', 'IsRamadan', 'IsEid', 'IsPreRamadan',\n",
      "       'IsPostRamadan', 'IsLast10Ramadan', 'IsDSF', 'IsSummerEvent',\n",
      "       'IsNationalDay', 'IsNewYear', 'IsMarathon', 'IsGITEX', 'IsAirshow',\n",
      "       'IsFoodFestival', 'IsPreEvent', 'IsPostEvent', 'Month_sin', 'Month_cos',\n",
      "       'DayOfWeek_sin', 'DayOfWeek_cos', 'Meal_Breakfast', 'Meal_Dinner',\n",
      "       'Meal_Lunch', 'Event_Dubai-Airshow', 'Event_Dubai-Food-Festival',\n",
      "       'Event_Dubai-Marathon', 'Event_Dubai-Shopping-Festival',\n",
      "       'Event_Dubai-Summer-Surprises', 'Event_Eid-Adha', 'Event_Flag-Day',\n",
      "       'Event_GITEX-Technology-Week', 'Event_New-Year-Celebrations',\n",
      "       'Event_Normal', 'Event_Post-Dubai-Airshow', 'Event_Post-Dubai-Marathon',\n",
      "       'Event_Post-Eid-Adha', 'Event_Post-Flag-Day',\n",
      "       'Event_Post-GITEX-Technology-Week', 'Event_Post-New-Year-Celebrations',\n",
      "       'Event_Post-Ramadan-Recovery', 'Event_Post-Ramadan-Week1',\n",
      "       'Event_Post-Summer-Event', 'Event_Pre-Commemoration-Day',\n",
      "       'Event_Pre-DSF', 'Event_Pre-Dubai-Airshow',\n",
      "       'Event_Pre-Dubai-Food-Festival', 'Event_Pre-Dubai-Marathon',\n",
      "       'Event_Pre-Eid-Adha', 'Event_Pre-Flag-Day',\n",
      "       'Event_Pre-GITEX-Technology-Week', 'Event_Pre-Ramadan-Early',\n",
      "       'Event_Pre-Ramadan-Late', 'Event_Pre-UAE-National-Day',\n",
      "       'Event_Ramadan-First10Days', 'Event_Ramadan-Last10Days',\n",
      "       'Event_Ramadan-Middle', 'Tourism_0', 'Tourism_1', 'Tourism_2',\n",
      "       'Tourism_3', 'Impact_-1', 'Impact_0', 'Impact_1'],\n",
      "      dtype='object')\n",
      "\n",
      "First 3 rows of transformed data:\n",
      "       Year  CheckTotal   is_zero  IsRamadan     IsEid  IsPreRamadan  \\\n",
      "0 -0.575766    0.015624 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "1 -0.575766    2.072745 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "2 -0.575766   -0.155666 -0.083103  -0.645685 -0.184506     -0.307562   \n",
      "\n",
      "   IsPostRamadan  IsLast10Ramadan     IsDSF  IsSummerEvent  ...  \\\n",
      "0      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "1      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "2      -0.307562        -0.207168 -0.261021      -0.389434  ...   \n",
      "\n",
      "   Event_Ramadan-First10Days  Event_Ramadan-Last10Days  Event_Ramadan-Middle  \\\n",
      "0                  -0.207168                 -0.207168             -0.201706   \n",
      "1                  -0.207168                 -0.207168             -0.201706   \n",
      "2                  -0.207168                 -0.207168             -0.201706   \n",
      "\n",
      "   Tourism_0  Tourism_1  Tourism_2  Tourism_3  Impact_-1  Impact_0  Impact_1  \n",
      "0  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "1  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "2  -0.371717  -1.434086  -0.399881   3.705033  -0.311553 -2.615093  4.957716  \n",
      "\n",
      "[3 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load the transformed datasets\n",
    "print(\"=\"*50)\n",
    "print(\"LOADING CNN-LSTM READY DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load the datasets\n",
    "df_transformed = pd.read_csv('cnn_lstm_ready_dataset.csv')\n",
    "target_data = pd.read_csv('target_data_for_sequences.csv')\n",
    "\n",
    "print(f\"✓ Transformed data shape: {df_transformed.shape}\")\n",
    "print(f\"✓ Target data shape: {target_data.shape}\")\n",
    "print(f\"✓ Transformed data columns: {len(df_transformed.columns)} features\")\n",
    "print(f\"✓ Target data columns: {len(target_data.columns)} target variables\")\n",
    "print(df_transformed.columns)\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 3 rows of transformed data:\")\n",
    "print(df_transformed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_for_cnn_lstm(df_transformed, target_data, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences for CNN-LSTM training from loaded CSV files\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length  # Look back 30 days\n",
    "    FORECAST_HORIZON = forecast_horizon  # Predict next 7 days\n",
    "    \n",
    "    # Sort by date to ensure proper sequence order\n",
    "    df_transformed_sorted = df_transformed.sort_values('Date').reset_index(drop=True)\n",
    "    target_data_sorted = target_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Pivot target data to wide format\n",
    "    target_pivot = target_data_sorted.pivot_table(\n",
    "        index='Date', \n",
    "        columns=['RevenueCenterName', 'MealPeriod'], \n",
    "        values='CheckTotal', \n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Create column names for revenue streams\n",
    "    target_pivot.columns = ['Date'] + [f\"{col[0]}_{col[1]}\" for col in target_pivot.columns[1:]]\n",
    "    \n",
    "    # Ensure same date range\n",
    "    common_dates = set(df_transformed_sorted['Date']).intersection(set(target_pivot['Date']))\n",
    "    df_transformed_sorted = df_transformed_sorted[df_transformed_sorted['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    target_pivot = target_pivot[target_pivot['Date'].isin(common_dates)].reset_index(drop=True)\n",
    "    \n",
    "    # Remove Date column from features\n",
    "    feature_columns = [col for col in df_transformed_sorted.columns if col != 'Date']\n",
    "    features = df_transformed_sorted[feature_columns].values\n",
    "    \n",
    "    # Target columns (revenue targets)\n",
    "    target_columns = [col for col in target_pivot.columns if col != 'Date']\n",
    "    targets = target_pivot[target_columns].values\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Number of feature columns: {len(feature_columns)}\")\n",
    "    print(f\"✓ Number of target columns: {len(target_columns)}\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past 30 days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next 7 days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, 30, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, 7, revenue_targets)\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Target normalization functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3A: Target Normalization Functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "def normalize_targets(y_train, y_test, save_scaler=True):\n",
    "    \"\"\"\n",
    "    Normalize target values for better training stability\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"NORMALIZING TARGET VALUES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Original data info\n",
    "    print(f\"📊 Original target ranges:\")\n",
    "    print(f\"  y_train: ${y_train.min():.2f} - ${y_train.max():.2f}\")\n",
    "    print(f\"  y_test: ${y_test.min():.2f} - ${y_test.max():.2f}\")\n",
    "    \n",
    "    # Reshape for normalization: (samples, days, streams) -> (samples*days, streams)\n",
    "    original_train_shape = y_train.shape\n",
    "    original_test_shape = y_test.shape\n",
    "    \n",
    "    y_train_reshaped = y_train.reshape(-1, y_train.shape[-1])  # (samples*days, 3)\n",
    "    y_test_reshaped = y_test.reshape(-1, y_test.shape[-1])     # (samples*days, 3)\n",
    "    \n",
    "    print(f\"✓ Reshaped for scaling:\")\n",
    "    print(f\"  y_train: {original_train_shape} -> {y_train_reshaped.shape}\")\n",
    "    print(f\"  y_test: {original_test_shape} -> {y_test_reshaped.shape}\")\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_normalized = target_scaler.fit_transform(y_train_reshaped)\n",
    "    y_test_normalized = target_scaler.transform(y_test_reshaped)\n",
    "    \n",
    "    # Reshape back to original format\n",
    "    y_train_normalized = y_train_normalized.reshape(original_train_shape)\n",
    "    y_test_normalized = y_test_normalized.reshape(original_test_shape)\n",
    "    \n",
    "    print(f\"✓ Normalized target ranges:\")\n",
    "    print(f\"  y_train: {y_train_normalized.min():.3f} - {y_train_normalized.max():.3f}\")\n",
    "    print(f\"  y_test: {y_test_normalized.min():.3f} - {y_test_normalized.max():.3f}\")\n",
    "    print(f\"  Mean: {y_train_normalized.mean():.3f}, Std: {y_train_normalized.std():.3f}\")\n",
    "    \n",
    "    # Save scaler for later denormalization\n",
    "    if save_scaler:\n",
    "        joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "        print(f\"✅ Target scaler saved to 'target_scaler.pkl'\")\n",
    "    \n",
    "    return y_train_normalized, y_test_normalized, target_scaler\n",
    "\n",
    "def denormalize_predictions(predictions_normalized, target_scaler):\n",
    "    \"\"\"\n",
    "    Convert normalized predictions back to actual dollar amounts\n",
    "    \"\"\"\n",
    "    original_shape = predictions_normalized.shape\n",
    "    \n",
    "    # Reshape for denormalization\n",
    "    pred_reshaped = predictions_normalized.reshape(-1, predictions_normalized.shape[-1])\n",
    "    \n",
    "    # Denormalize\n",
    "    pred_actual = target_scaler.inverse_transform(pred_reshaped)\n",
    "    \n",
    "    # Reshape back\n",
    "    pred_actual = pred_actual.reshape(original_shape)\n",
    "    \n",
    "    return pred_actual\n",
    "\n",
    "print(\"✅ Target normalization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLEANING AND PREPARING DATA FOR CNN-LSTM\n",
      "==================================================\n",
      "Original data info:\n",
      "df_transformed shape: (1458, 65)\n",
      "df_transformed columns: ['Year', 'CheckTotal', 'is_zero', 'IsRamadan', 'IsEid', 'IsPreRamadan', 'IsPostRamadan', 'IsLast10Ramadan', 'IsDSF', 'IsSummerEvent', 'IsNationalDay', 'IsNewYear', 'IsMarathon', 'IsGITEX', 'IsAirshow', 'IsFoodFestival', 'IsPreEvent', 'IsPostEvent', 'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Meal_Breakfast', 'Meal_Dinner', 'Meal_Lunch', 'Event_Dubai-Airshow', 'Event_Dubai-Food-Festival', 'Event_Dubai-Marathon', 'Event_Dubai-Shopping-Festival', 'Event_Dubai-Summer-Surprises', 'Event_Eid-Adha', 'Event_Flag-Day', 'Event_GITEX-Technology-Week', 'Event_New-Year-Celebrations', 'Event_Normal', 'Event_Post-Dubai-Airshow', 'Event_Post-Dubai-Marathon', 'Event_Post-Eid-Adha', 'Event_Post-Flag-Day', 'Event_Post-GITEX-Technology-Week', 'Event_Post-New-Year-Celebrations', 'Event_Post-Ramadan-Recovery', 'Event_Post-Ramadan-Week1', 'Event_Post-Summer-Event', 'Event_Pre-Commemoration-Day', 'Event_Pre-DSF', 'Event_Pre-Dubai-Airshow', 'Event_Pre-Dubai-Food-Festival', 'Event_Pre-Dubai-Marathon', 'Event_Pre-Eid-Adha', 'Event_Pre-Flag-Day', 'Event_Pre-GITEX-Technology-Week', 'Event_Pre-Ramadan-Early', 'Event_Pre-Ramadan-Late', 'Event_Pre-UAE-National-Day', 'Event_Ramadan-First10Days', 'Event_Ramadan-Last10Days', 'Event_Ramadan-Middle', 'Tourism_0', 'Tourism_1', 'Tourism_2', 'Tourism_3', 'Impact_-1', 'Impact_0', 'Impact_1']\n",
      "target_data shape: (1458, 4)\n",
      "target_data columns: ['Date', 'RevenueCenterName', 'MealPeriod', 'CheckTotal']\n",
      "✓ Data lengths match - assuming already aligned by row index\n",
      "\n",
      "🔄 Pivoting target data to wide format...\n",
      "✓ Pivoted target shape: (486, 4)\n",
      "✓ Pivoted target columns: ['day_id', 'Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "📊 Aggregating features to day level...\n",
      "✓ Aggregated features shape: (486, 65)\n",
      "✓ Final aligned shapes:\n",
      "Features: (486, 65)\n",
      "Targets: (486, 3)\n",
      "\n",
      "🧹 Cleaning data types...\n",
      "✅ Final cleaned data:\n",
      "Features shape: (486, 65)\n",
      "Targets shape: (486, 3)\n",
      "Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "Data lengths match: True\n",
      "\n",
      "==================================================\n",
      "CREATING SEQUENCES FOR CNN-LSTM\n",
      "==================================================\n",
      "✓ Feature shape: (486, 65)\n",
      "✓ Target shape: (486, 3)\n",
      "✓ Sequence length: 30 days\n",
      "✓ Forecast horizon: 7 days\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Final X shape: (450, 30, 65)\n",
      "✓ Final y shape: (450, 7, 3)\n",
      "✓ X dtype: float32\n",
      "✓ y dtype: float32\n",
      "✓ Total sequences created: 450\n",
      "\n",
      "📊 Shape interpretation:\n",
      "X: (450 sequences, 30 days history, 65 features)\n",
      "y: (450 sequences, 7 days forecast, 3 revenue streams)\n",
      "\n",
      "🎉 SUCCESS! Sequences created successfully!\n",
      "✓ Input sequences (X): (450, 30, 65)\n",
      "✓ Output sequences (y): (450, 7, 3)\n",
      "✓ Feature columns: 65\n",
      "✓ Target columns: ['Breakfast', 'Dinner', 'Lunch']\n",
      "✓ Data types: X=float32, y=float32\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Corrected - Handle data without Date column in features\n",
    "def clean_and_prepare_data_fixed(df_transformed, target_data):\n",
    "    \"\"\"\n",
    "    Clean dataframes when features don't have Date column\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLEANING AND PREPARING DATA FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Step 1: Check original data\n",
    "    print(\"Original data info:\")\n",
    "    print(f\"df_transformed shape: {df_transformed.shape}\")\n",
    "    print(f\"df_transformed columns: {list(df_transformed.columns)}\")\n",
    "    print(f\"target_data shape: {target_data.shape}\")\n",
    "    print(f\"target_data columns: {list(target_data.columns)}\")\n",
    "    \n",
    "    # Check if data is already aligned by length\n",
    "    if len(df_transformed) == len(target_data):\n",
    "        print(\"✓ Data lengths match - assuming already aligned by row index\")\n",
    "        \n",
    "        # Step 2: Pivot target data from long to wide format\n",
    "        print(\"\\n🔄 Pivoting target data to wide format...\")\n",
    "        \n",
    "        # Add row index to help with pivoting\n",
    "        target_with_index = target_data.copy()\n",
    "        target_with_index['row_index'] = target_with_index.index\n",
    "        \n",
    "        # Create a day identifier (since we know there are 3 meal periods per day)\n",
    "        target_with_index['day_id'] = target_with_index['row_index'] // 3\n",
    "        \n",
    "        target_pivot = target_with_index.pivot_table(\n",
    "            index='day_id', \n",
    "            columns='MealPeriod', \n",
    "            values='CheckTotal', \n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "        \n",
    "        print(f\"✓ Pivoted target shape: {target_pivot.shape}\")\n",
    "        print(f\"✓ Pivoted target columns: {list(target_pivot.columns)}\")\n",
    "        \n",
    "        # Step 3: Aggregate features to day level (average of 3 meal periods per day)\n",
    "        print(\"\\n📊 Aggregating features to day level...\")\n",
    "        \n",
    "        # Add day_id to features\n",
    "        df_features_with_day = df_transformed.copy()\n",
    "        df_features_with_day['day_id'] = df_features_with_day.index // 3\n",
    "        \n",
    "        # Aggregate features by day (mean of the 3 meal periods)\n",
    "        df_features_daily = df_features_with_day.groupby('day_id').mean().reset_index()\n",
    "        df_features_daily = df_features_daily.drop('day_id', axis=1)\n",
    "        \n",
    "        print(f\"✓ Aggregated features shape: {df_features_daily.shape}\")\n",
    "        \n",
    "        # Step 4: Align the data\n",
    "        target_values = target_pivot.drop('day_id', axis=1)\n",
    "        \n",
    "        # Ensure same number of rows\n",
    "        min_rows = min(len(df_features_daily), len(target_values))\n",
    "        df_features_final = df_features_daily.iloc[:min_rows]\n",
    "        target_values_final = target_values.iloc[:min_rows]\n",
    "        \n",
    "        print(f\"✓ Final aligned shapes:\")\n",
    "        print(f\"Features: {df_features_final.shape}\")\n",
    "        print(f\"Targets: {target_values_final.shape}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Data length mismatch: features={len(df_transformed)}, targets={len(target_data)}\")\n",
    "    \n",
    "    # Step 5: Clean data types and handle missing values\n",
    "    print(\"\\n🧹 Cleaning data types...\")\n",
    "    \n",
    "    # Features: ensure all numeric\n",
    "    df_features_clean = df_features_final.select_dtypes(include=[np.number])\n",
    "    df_features_clean = df_features_clean.fillna(0).astype(np.float32)\n",
    "    \n",
    "    # Targets: ensure all numeric\n",
    "    df_targets_clean = target_values_final.fillna(0).astype(np.float32)\n",
    "    \n",
    "    print(f\"✅ Final cleaned data:\")\n",
    "    print(f\"Features shape: {df_features_clean.shape}\")\n",
    "    print(f\"Targets shape: {df_targets_clean.shape}\")\n",
    "    print(f\"Target columns: {list(df_targets_clean.columns)}\")\n",
    "    print(f\"Data lengths match: {len(df_features_clean) == len(df_targets_clean)}\")\n",
    "    \n",
    "    return df_features_clean, df_targets_clean\n",
    "\n",
    "def create_sequences_for_cnn_lstm_corrected(df_features, df_targets, sequence_length=30, forecast_horizon=7):\n",
    "    \"\"\"\n",
    "    Create sequences from properly aligned and cleaned data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CREATING SEQUENCES FOR CNN-LSTM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Parameters\n",
    "    SEQ_LENGTH = sequence_length\n",
    "    FORECAST_HORIZON = forecast_horizon\n",
    "    \n",
    "    # Convert to arrays\n",
    "    features = df_features.values\n",
    "    targets = df_targets.values\n",
    "    feature_columns = df_features.columns.tolist()\n",
    "    target_columns = df_targets.columns.tolist()\n",
    "    \n",
    "    print(f\"✓ Feature shape: {features.shape}\")\n",
    "    print(f\"✓ Target shape: {targets.shape}\")\n",
    "    print(f\"✓ Sequence length: {SEQ_LENGTH} days\")\n",
    "    print(f\"✓ Forecast horizon: {FORECAST_HORIZON} days\")\n",
    "    print(f\"✓ Target columns: {target_columns}\")\n",
    "    \n",
    "    # Verify we have enough data\n",
    "    min_data_needed = SEQ_LENGTH + FORECAST_HORIZON\n",
    "    if len(features) < min_data_needed:\n",
    "        raise ValueError(f\"Not enough data. Need at least {min_data_needed} rows, got {len(features)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(SEQ_LENGTH, len(features) - FORECAST_HORIZON + 1):\n",
    "        # Features: past SEQ_LENGTH days\n",
    "        X.append(features[i-SEQ_LENGTH:i])\n",
    "        \n",
    "        # Targets: next FORECAST_HORIZON days\n",
    "        y.append(targets[i:i+FORECAST_HORIZON])\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    print(f\"✓ Final X shape: {X.shape}\")  # (samples, sequence_length, features)\n",
    "    print(f\"✓ Final y shape: {y.shape}\")  # (samples, forecast_horizon, revenue_streams)\n",
    "    print(f\"✓ X dtype: {X.dtype}\")\n",
    "    print(f\"✓ y dtype: {y.dtype}\")\n",
    "    print(f\"✓ Total sequences created: {len(X)}\")\n",
    "    \n",
    "    # Show example of what each dimension means\n",
    "    print(f\"\\n📊 Shape interpretation:\")\n",
    "    print(f\"X: ({X.shape[0]} sequences, {X.shape[1]} days history, {X.shape[2]} features)\")\n",
    "    print(f\"y: ({y.shape[0]} sequences, {y.shape[1]} days forecast, {y.shape[2]} revenue streams)\")\n",
    "    \n",
    "    return X, y, feature_columns, target_columns\n",
    "\n",
    "# Execute the corrected pipeline\n",
    "try:\n",
    "    # Step 1: Clean and prepare data without Date column dependency\n",
    "    df_features_clean, df_targets_clean = clean_and_prepare_data_fixed(df_transformed, target_data)\n",
    "    \n",
    "    # Step 2: Create sequences\n",
    "    X, y, feature_cols, target_cols = create_sequences_for_cnn_lstm_corrected(\n",
    "        df_features_clean, df_targets_clean\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 SUCCESS! Sequences created successfully!\")\n",
    "    print(f\"✓ Input sequences (X): {X.shape}\")\n",
    "    print(f\"✓ Output sequences (y): {y.shape}\")\n",
    "    print(f\"✓ Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"✓ Target columns: {target_cols}\")\n",
    "    print(f\"✓ Data types: X={X.dtype}, y={y.dtype}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.5757663   0.6442341  -0.08310281 ... -0.3115533  -2.6150928\n",
      "    4.957716  ]\n",
      "  [-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.01185533 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.20041224 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663   0.11598377 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[-0.5757663   0.10293791 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.44010323 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.2981966  -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [-0.5757663  -0.17047678 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663   0.15700552 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [-0.5757663  -0.48062024 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.7368157  -0.7176877  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.35829824 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.75030243 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157  -0.323879   -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]\n",
      "\n",
      " [[ 1.7368157  -0.7354559  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.66974956 -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.2725303  -0.08310281 ...  3.2097237  -2.6150928\n",
      "   -0.20170578]\n",
      "  ...\n",
      "  [ 1.7368157   0.27193794 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.29938522 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]\n",
      "  [ 1.7368157  -0.03529583 -0.08310281 ... -0.3115533   0.38239563\n",
      "   -0.20170578]]]\n",
      "----------------------------------------------------------\n",
      "[[[ 473.  4368.   250. ]\n",
      "  [ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  ...\n",
      "  [1994.  2686.   247. ]\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]]\n",
      "\n",
      " [[ 612.  1527.5  278. ]\n",
      "  [ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  ...\n",
      "  [ 886.4 1466.   569.5]\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]]\n",
      "\n",
      " [[ 223.  2849.   938.5]\n",
      "  [ 968.  1896.   565. ]\n",
      "  [1994.  2686.   247. ]\n",
      "  ...\n",
      "  [1550.4 1952.   125. ]\n",
      "  [1751.4 3700.   969. ]\n",
      "  [ 745.  1673.   258. ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 335.  2032.9  809.5]\n",
      "  [ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  ...\n",
      "  [1005.  2840.  1224. ]\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]]\n",
      "\n",
      " [[ 456.7 3090.5  737.5]\n",
      "  [ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  ...\n",
      "  [ 736.  3199.   543.4]\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]]\n",
      "\n",
      " [[ 421.  2461.5  691. ]\n",
      "  [ 631.  4597.9 1121.5]\n",
      "  [1005.  2840.  1224. ]\n",
      "  ...\n",
      "  [ 863.  2368.  1567. ]\n",
      "  [1064.7 2149.5  366. ]\n",
      "  [ 566.  2503.  1242. ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAIN-TEST SPLIT\n",
      "==============================\n",
      "✓ Training sequences: 360\n",
      "✓ Testing sequences: 90\n",
      "✓ Input shape per sample: (30, 65)\n",
      "✓ Output shape per sample: (7, 3)\n",
      "✓ X_train dtype: float32\n",
      "✓ y_train dtype: float32\n",
      "✓ X_test dtype: float32\n",
      "✓ y_test dtype: float32\n",
      "\n",
      "✓ Data quality check:\n",
      "X_train NaN count: 0\n",
      "y_train NaN count: 0\n",
      "X_train Inf count: 0\n",
      "y_train Inf count: 0\n",
      "\n",
      "✅ Data is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train-Test Split with clean data\n",
    "print(\"=\"*30)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Time-based split (80% train, 20% test)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"✓ Training sequences: {X_train.shape[0]}\")\n",
    "print(f\"✓ Testing sequences: {X_test.shape[0]}\")\n",
    "print(f\"✓ Input shape per sample: {X_train.shape[1:]}\")\n",
    "print(f\"✓ Output shape per sample: {y_train.shape[1:]}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"✓ X_train dtype: {X_train.dtype}\")\n",
    "print(f\"✓ y_train dtype: {y_train.dtype}\")\n",
    "print(f\"✓ X_test dtype: {X_test.dtype}\")\n",
    "print(f\"✓ y_test dtype: {y_test.dtype}\")\n",
    "\n",
    "# Check for any problematic values\n",
    "print(f\"\\n✓ Data quality check:\")\n",
    "print(f\"X_train NaN count: {np.isnan(X_train).sum()}\")\n",
    "print(f\"y_train NaN count: {np.isnan(y_train).sum()}\")\n",
    "print(f\"X_train Inf count: {np.isinf(X_train).sum()}\")\n",
    "print(f\"y_train Inf count: {np.isinf(y_train).sum()}\")\n",
    "\n",
    "print(f\"\\n✅ Data is ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model building function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define CNN-LSTM model architecture\n",
    "def build_cnn_lstm_model(input_shape, output_shape):\n",
    "    \"\"\"\n",
    "    Build CNN-LSTM hybrid model for hotel revenue forecasting\n",
    "    \"\"\"\n",
    "    print(f\"✓ Building model with input shape: {input_shape}\")\n",
    "    print(f\"✓ Output shape: {output_shape}\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, name='conv1d_1'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', name='conv1d_2'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_1'),\n",
    "        Dropout(0.2, name='dropout_1'),\n",
    "        \n",
    "        # More CNN layers\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', name='conv1d_3'),\n",
    "        MaxPooling1D(pool_size=2, name='maxpool_2'),\n",
    "        Dropout(0.2, name='dropout_2'),\n",
    "        \n",
    "        # LSTM layers for temporal patterns\n",
    "        LSTM(100, return_sequences=True, name='lstm_1'),\n",
    "        Dropout(0.3, name='dropout_3'),\n",
    "        LSTM(50, return_sequences=False, name='lstm_2'),\n",
    "        Dropout(0.3, name='dropout_4'),\n",
    "        \n",
    "        # Dense layers for final prediction\n",
    "        Dense(100, activation='relu', name='dense_1'),\n",
    "        Dropout(0.2, name='dropout_5'),\n",
    "        Dense(np.prod(output_shape), activation='linear', name='dense_output'),\n",
    "    ])\n",
    "    \n",
    "    # Reshape output to (forecast_days, revenue_streams)\n",
    "    model.add(tf.keras.layers.Reshape(output_shape, name='reshape_output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✓ Model building function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "APPLYING TARGET NORMALIZATION\n",
      "========================================\n",
      "==================================================\n",
      "NORMALIZING TARGET VALUES\n",
      "==================================================\n",
      "📊 Original target ranges:\n",
      "  y_train: $5.00 - $10052.50\n",
      "  y_test: $66.00 - $9657.00\n",
      "✓ Reshaped for scaling:\n",
      "  y_train: (360, 7, 3) -> (2520, 3)\n",
      "  y_test: (90, 7, 3) -> (630, 3)\n",
      "✓ Normalized target ranges:\n",
      "  y_train: -1.588 - 9.795\n",
      "  y_test: -1.334 - 9.494\n",
      "  Mean: -0.000, Std: 1.000\n",
      "✅ Target scaler saved to 'target_scaler.pkl'\n",
      "✅ Target normalization applied!\n",
      "✅ Training will use normalized targets\n",
      "✅ Original targets preserved for comparison\n",
      "\n",
      "📊 Comparison:\n",
      "Original y_train range: $5.00 - $10052.50\n",
      "Normalized y_train range: -1.588 - 9.795\n"
     ]
    }
   ],
   "source": [
    "# Cell 6A: Apply Target Normalization\n",
    "print(\"=\"*40)\n",
    "print(\"APPLYING TARGET NORMALIZATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Store original targets for comparison\n",
    "y_train_original = y_train.copy()\n",
    "y_test_original = y_test.copy()\n",
    "\n",
    "# Apply normalization\n",
    "y_train_norm, y_test_norm, target_scaler = normalize_targets(y_train, y_test, save_scaler=True)\n",
    "\n",
    "# Update variables for training\n",
    "y_train = y_train_norm\n",
    "y_test = y_test_norm\n",
    "\n",
    "print(f\"✅ Target normalization applied!\")\n",
    "print(f\"✅ Training will use normalized targets\")\n",
    "print(f\"✅ Original targets preserved for comparison\")\n",
    "\n",
    "# Show the difference\n",
    "print(f\"\\n📊 Comparison:\")\n",
    "print(f\"Original y_train range: ${y_train_original.min():.2f} - ${y_train_original.max():.2f}\")\n",
    "print(f\"Normalized y_train range: {y_train.min():.3f} - {y_train.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "BUILDING MODEL\n",
      "==============================\n",
      "✓ Building model with input shape: (30, 65)\n",
      "✓ Output shape: (7, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "MODEL ARCHITECTURE\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,121</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │        \u001b[38;5;34m53,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m30,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m5,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │         \u001b[38;5;34m2,121\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_output (\u001b[38;5;33mReshape\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,693</span> (475.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,693\u001b[0m (475.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Total parameters: 121,693\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Build and compile the model\n",
    "print(\"=\"*30)\n",
    "print(\"BUILDING MODEL\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (30, features)\n",
    "output_shape = (y_train.shape[1], y_train.shape[2])  # (7, revenue_streams)\n",
    "\n",
    "# Build model\n",
    "model = build_cnn_lstm_model(input_shape, output_shape)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*30)\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n✓ Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "TRAINING SETUP\n",
      "==============================\n",
      "✓ Callbacks configured:\n",
      "  - Early stopping (patience=15)\n",
      "  - Learning rate reduction (factor=0.5, patience=5)\n",
      "  - Model checkpoint (best_cnn_lstm_model.h5)\n",
      "✓ Batch size: 32\n",
      "✓ Max epochs: 100\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Setup training callbacks\n",
    "print(\"=\"*30)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=15, \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_lstm_model.h5', \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "print(\"✓ Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=15)\")\n",
    "print(\"  - Learning rate reduction (factor=0.5, patience=5)\")\n",
    "print(\"  - Model checkpoint (best_cnn_lstm_model.h5)\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Max epochs: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "COMPREHENSIVE DATA PREPARATION\n",
      "========================================\n",
      "🔧 Cleaning and preparing data...\n",
      "🧹 Handling NaN and infinite values...\n",
      "✓ Final data types:\n",
      "  X_train: float32, shape: (360, 30, 65)\n",
      "  y_train: float32, shape: (360, 7, 3)\n",
      "  X_test: float32, shape: (90, 30, 65)\n",
      "  y_test: float32, shape: (90, 7, 3)\n",
      "✓ Data ranges:\n",
      "  X_train: [-2.615, 22.023]\n",
      "  y_train: [-1.588, 9.795]\n",
      "\n",
      "==============================\n",
      "STARTING TRAINING\n",
      "==============================\n",
      "Epoch 1/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9665 - mae: 0.6664 \n",
      "Epoch 1: val_loss improved from inf to 3.43391, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - loss: 0.9689 - mae: 0.6665 - val_loss: 3.4339 - val_mae: 1.3141 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7425 - mae: 0.6068 \n",
      "Epoch 2: val_loss improved from 3.43391 to 3.04964, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.8072 - mae: 0.6184 - val_loss: 3.0496 - val_mae: 1.2144 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8319 - mae: 0.6087\n",
      "Epoch 3: val_loss improved from 3.04964 to 2.46417, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8221 - mae: 0.6059 - val_loss: 2.4642 - val_mae: 1.0717 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6036 - mae: 0.5449\n",
      "Epoch 4: val_loss improved from 2.46417 to 2.25386, saving model to best_cnn_lstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.6289 - mae: 0.5532 - val_loss: 2.2539 - val_mae: 1.0263 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.8036 - mae: 0.6067\n",
      "Epoch 5: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7809 - mae: 0.6014 - val_loss: 2.3177 - val_mae: 1.0313 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6832 - mae: 0.5590\n",
      "Epoch 6: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6840 - mae: 0.5599 - val_loss: 2.2775 - val_mae: 1.0194 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7059 - mae: 0.5670 \n",
      "Epoch 7: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.7049 - mae: 0.5669 - val_loss: 2.7672 - val_mae: 1.1201 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.7838 - mae: 0.5843\n",
      "Epoch 8: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7762 - mae: 0.5827 - val_loss: 2.2905 - val_mae: 1.0176 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m11/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6671 - mae: 0.5609\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 9: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6679 - mae: 0.5610 - val_loss: 2.3908 - val_mae: 1.0376 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6598 - mae: 0.5575 \n",
      "Epoch 10: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6721 - mae: 0.5612 - val_loss: 2.3306 - val_mae: 1.0250 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.6631 - mae: 0.5600 \n",
      "Epoch 11: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6688 - mae: 0.5598 - val_loss: 2.2932 - val_mae: 1.0171 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m 8/12\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7363 - mae: 0.5797 \n",
      "Epoch 12: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7027 - mae: 0.5705 - val_loss: 2.3539 - val_mae: 1.0299 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m 6/12\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6810 - mae: 0.5632\n",
      "Epoch 13: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6746 - mae: 0.5600 - val_loss: 2.3291 - val_mae: 1.0291 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6555 - mae: 0.5475 \n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 14: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6712 - mae: 0.5529 - val_loss: 2.3022 - val_mae: 1.0213 - learning_rate: 5.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m11/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6735 - mae: 0.5541\n",
      "Epoch 15: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6724 - mae: 0.5546 - val_loss: 2.2824 - val_mae: 1.0158 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6890 - mae: 0.5615\n",
      "Epoch 16: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6789 - mae: 0.5586 - val_loss: 2.3255 - val_mae: 1.0262 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.6285 - mae: 0.5507\n",
      "Epoch 17: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.6377 - mae: 0.5516 - val_loss: 2.3099 - val_mae: 1.0213 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m 7/12\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.6381 - mae: 0.5517\n",
      "Epoch 18: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6569 - mae: 0.5549 - val_loss: 2.2704 - val_mae: 1.0110 - learning_rate: 2.5000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m 9/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6660 - mae: 0.5476\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 2.25386\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.6662 - mae: 0.5498 - val_loss: 2.2904 - val_mae: 1.0154 - learning_rate: 2.5000e-04\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\n",
      "✅ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 Alternative: Comprehensive data cleaning and training\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"COMPREHENSIVE DATA PREPARATION\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "def clean_and_prepare_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning for CNN-LSTM training\n",
    "    \"\"\"\n",
    "    print(\"🔧 Cleaning and preparing data...\")\n",
    "    \n",
    "    # Convert to numpy arrays if not already\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Check for object dtype issues\n",
    "    if X_train.dtype == 'object':\n",
    "        print(\"⚠️  X_train has object dtype - converting...\")\n",
    "        X_train = X_train.astype(np.float64)\n",
    "    \n",
    "    if y_train.dtype == 'object':\n",
    "        print(\"⚠️  y_train has object dtype - converting...\")\n",
    "        y_train = y_train.astype(np.float64)\n",
    "    \n",
    "    if X_test.dtype == 'object':\n",
    "        print(\"⚠️  X_test has object dtype - converting...\")\n",
    "        X_test = X_test.astype(np.float64)\n",
    "    \n",
    "    if y_test.dtype == 'object':\n",
    "        print(\"⚠️  y_test has object dtype - converting...\")\n",
    "        y_test = y_test.astype(np.float64)\n",
    "    \n",
    "    # Handle NaN and infinite values\n",
    "    print(\"🧹 Handling NaN and infinite values...\")\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Convert to float32 (TensorFlow's preferred type)\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    y_test = y_test.astype(np.float32)\n",
    "    \n",
    "    # Final verification\n",
    "    print(f\"✓ Final data types:\")\n",
    "    print(f\"  X_train: {X_train.dtype}, shape: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.dtype}, shape: {y_train.shape}\")\n",
    "    print(f\"  X_test: {X_test.dtype}, shape: {X_test.shape}\")\n",
    "    print(f\"  y_test: {y_test.dtype}, shape: {y_test.shape}\")\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(f\"✓ Data ranges:\")\n",
    "    print(f\"  X_train: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "    print(f\"  y_train: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Clean the data\n",
    "X_train_clean, y_train_clean, X_test_clean, y_test_clean = clean_and_prepare_data(\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Train with cleaned data\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train_clean, y_train_clean,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_test_clean, y_test_clean),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    \n",
    "    # Update variables for next cells\n",
    "    X_train, y_train = X_train_clean, y_train_clean\n",
    "    X_test, y_test = X_test_clean, y_test_clean\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training still failed: {e}\")\n",
    "    print(\"\\n🔍 Additional debugging:\")\n",
    "    \n",
    "    # More detailed debugging\n",
    "    print(f\"X_train unique dtypes: {set(str(x.dtype) for x in X_train.flatten()[:100])}\")\n",
    "    print(f\"Sample X_train values: {X_train[0, 0, :10]}\")\n",
    "    print(f\"Sample y_train values: {y_train[0, 0, :10]}\")\n",
    "    \n",
    "    # Check if data contains any strings\n",
    "    sample_x = X_train[0, 0, :]\n",
    "    print(f\"Sample X contains strings: {any(isinstance(x, str) for x in sample_x.flatten())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "==============================\n",
      "📊 NOTE: Model trained on normalized targets\n",
      "📊 Denormalizing predictions to actual dollar amounts\n",
      "✅ Predictions denormalized successfully\n",
      "\n",
      "🔍 Shape Debugging:\n",
      "X_test shape: (90, 30, 65)\n",
      "y_test_actual shape: (90, 7, 3)\n",
      "y_pred_actual shape: (90, 7, 3)\n",
      "\n",
      "📊 Revenue streams: ['Breakfast', 'Dinner', 'Lunch']\n",
      "\n",
      "💰 Denormalized Value Ranges:\n",
      "  Actual revenue: $66.00 - $9657.00\n",
      "  Predicted revenue: $481.21 - $5506.60\n",
      "\n",
      "✅ Overall Test Metrics (in USD):\n",
      "  MAE: $870.10\n",
      "  RMSE: $1273.38\n",
      "  MAPE: 75.66%\n",
      "\n",
      "✅ Performance by Revenue Stream:\n",
      "  Breakfast: MAE = $776.84, Correlation = 0.484\n",
      "  Dinner: MAE = $1218.92, Correlation = 0.678\n",
      "  Lunch: MAE = $614.53, Correlation = 0.546\n",
      "\n",
      "✅ Sample Predictions (First sequence - in USD):\n",
      "Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\n",
      "-----------------------------------------------------------------------------------------------\n",
      " 1  | $    2466.00     | $  1767.07     | $ 6548.00     | $5111.87     | $2912.00     | $1255.50\n",
      " 2  | $    2586.80     | $  2115.33     | $ 4300.00     | $5330.62     | $2686.00     | $1078.78\n",
      " 3  | $    1639.60     | $  1803.46     | $ 6378.00     | $4956.40     | $2600.00     | $1613.31\n",
      " 4  | $    2079.60     | $  1908.16     | $ 6710.00     | $4726.28     | $2188.00     | $1252.44\n",
      " 5  | $    1232.00     | $  2034.98     | $ 4523.00     | $5506.60     | $2206.00     | $1357.37\n",
      " 6  | $    1390.00     | $  2018.57     | $ 3990.00     | $5366.19     | $1710.00     | $1633.34\n",
      " 7  | $    1426.00     | $  2167.11     | $ 5762.00     | $4556.55     | $ 682.00     | $1353.80\n",
      "\n",
      "✅ Evaluation complete with denormalized predictions!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Model Evaluation with Denormalization\n",
    "print(\"=\"*30)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# IMPORTANT: Model was trained on NORMALIZED targets\n",
    "# We need to denormalize predictions for evaluation\n",
    "print(\"📊 NOTE: Model trained on normalized targets\")\n",
    "print(\"📊 Denormalizing predictions to actual dollar amounts\")\n",
    "\n",
    "# Make predictions on normalized test set\n",
    "y_pred_normalized = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Load scaler and denormalize predictions\n",
    "try:\n",
    "    target_scaler = joblib.load('target_scaler.pkl')\n",
    "    y_pred_actual = denormalize_predictions(y_pred_normalized, target_scaler)\n",
    "    y_test_actual = y_test_original  # Use original non-normalized test targets\n",
    "    \n",
    "    print(f\"✅ Predictions denormalized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load scaler: {e}\")\n",
    "    print(f\"📊 Using normalized predictions for evaluation\")\n",
    "    y_pred_actual = y_pred_normalized\n",
    "    y_test_actual = y_test\n",
    "\n",
    "# Debug shapes\n",
    "print(f\"\\n🔍 Shape Debugging:\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test_actual shape: {y_test_actual.shape}\")\n",
    "print(f\"y_pred_actual shape: {y_pred_actual.shape}\")\n",
    "\n",
    "# Define revenue stream names\n",
    "revenue_streams = ['Breakfast', 'Dinner', 'Lunch']\n",
    "print(f\"\\n📊 Revenue streams: {revenue_streams}\")\n",
    "\n",
    "# Show data ranges (should be in dollars after denormalization)\n",
    "print(f\"\\n💰 Denormalized Value Ranges:\")\n",
    "print(f\"  Actual revenue: ${y_test_actual.min():.2f} - ${y_test_actual.max():.2f}\")\n",
    "print(f\"  Predicted revenue: ${y_pred_actual.min():.2f} - ${y_pred_actual.max():.2f}\")\n",
    "\n",
    "# Calculate metrics on actual dollar amounts\n",
    "y_test_flat = y_test_actual.reshape(-1)\n",
    "y_pred_flat = y_pred_actual.reshape(-1)\n",
    "\n",
    "mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
    "mse = mean_squared_error(y_test_flat, y_pred_flat)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test_flat - y_pred_flat) / (np.abs(y_test_flat) + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\n✅ Overall Test Metrics (in USD):\")\n",
    "print(f\"  MAE: ${mae:.2f}\")\n",
    "print(f\"  RMSE: ${rmse:.2f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Performance by revenue stream\n",
    "print(f\"\\n✅ Performance by Revenue Stream:\")\n",
    "for stream_idx, stream_name in enumerate(revenue_streams):\n",
    "    stream_mae = mean_absolute_error(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1), \n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )\n",
    "    stream_corr = np.corrcoef(\n",
    "        y_test_actual[:, :, stream_idx].reshape(-1),\n",
    "        y_pred_actual[:, :, stream_idx].reshape(-1)\n",
    "    )[0, 1]\n",
    "    print(f\"  {stream_name}: MAE = ${stream_mae:.2f}, Correlation = {stream_corr:.3f}\")\n",
    "\n",
    "# Sample predictions\n",
    "print(f\"\\n✅ Sample Predictions (First sequence - in USD):\")\n",
    "print(\"Day | Breakfast_Actual | Breakfast_Pred | Dinner_Actual | Dinner_Pred | Lunch_Actual | Lunch_Pred\")\n",
    "print(\"-\" * 95)\n",
    "for day in range(min(7, y_test_actual.shape[1])):\n",
    "    print(f\"{day+1:2d}  | ${y_test_actual[0, day, 0]:11.2f}     | ${y_pred_actual[0, day, 0]:9.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 1]:8.2f}     | ${y_pred_actual[0, day, 1]:6.2f}     | \"\n",
    "          f\"${y_test_actual[0, day, 2]:7.2f}     | ${y_pred_actual[0, day, 2]:5.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Evaluation complete with denormalized predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE RELEVANCE ANALYSIS FOR CNN-LSTM MODEL\n",
      "================================================================================\n",
      "Analyzing data shapes:\n",
      "X shape: (450, 30, 65)\n",
      "y shape: (450, 7, 3)\n",
      "Number of features: 65\n",
      "Using normalized targets for analysis\n",
      "Analysis target shape: (360, 7, 3)\n",
      "\n",
      "Flattening sequences for feature analysis...\n",
      "Flattened shapes: X_flat=(10800, 65), y_flat=(2520, 3)\n",
      "ERROR: Shape mismatch detected!\n",
      "X_flat samples: 10800\n",
      "y_flat samples: 2520\n",
      "Fixed shapes: X_flat=(2520, 65), y_flat=(2520, 3)\n",
      "\n",
      "============================================================\n",
      "1. RANDOM FOREST FEATURE IMPORTANCE\n",
      "============================================================\n",
      "\n",
      "Analyzing Breakfast revenue...\n",
      "Top 10 features for Breakfast:\n",
      "  20. Month_cos                           0.4697\n",
      "   2. CheckTotal                          0.1208\n",
      "  59. Tourism_0                           0.1020\n",
      "  21. DayOfWeek_sin                       0.0752\n",
      "  22. DayOfWeek_cos                       0.0382\n",
      "  63. Impact_-1                           0.0325\n",
      "  56. Event_Ramadan-First10Days           0.0312\n",
      "   8. IsLast10Ramadan                     0.0280\n",
      "  57. Event_Ramadan-Last10Days            0.0253\n",
      "  64. Impact_0                            0.0207\n",
      "\n",
      "Analyzing Dinner revenue...\n",
      "Top 10 features for Dinner:\n",
      "  20. Month_cos                           0.3493\n",
      "  60. Tourism_1                           0.2563\n",
      "   2. CheckTotal                          0.1075\n",
      "  63. Impact_-1                           0.0572\n",
      "  21. DayOfWeek_sin                       0.0571\n",
      "  64. Impact_0                            0.0450\n",
      "  59. Tourism_0                           0.0281\n",
      "  22. DayOfWeek_cos                       0.0254\n",
      "  56. Event_Ramadan-First10Days           0.0113\n",
      "  43. Event_Post-Ramadan-Week1            0.0104\n",
      "\n",
      "Analyzing Lunch revenue...\n",
      "Top 10 features for Lunch:\n",
      "  20. Month_cos                           0.4509\n",
      "   2. CheckTotal                          0.2097\n",
      "  21. DayOfWeek_sin                       0.1286\n",
      "  22. DayOfWeek_cos                       0.0300\n",
      "  59. Tourism_0                           0.0298\n",
      "  63. Impact_-1                           0.0296\n",
      "  58. Event_Ramadan-Middle                0.0282\n",
      "  64. Impact_0                            0.0168\n",
      "  60. Tourism_1                           0.0138\n",
      "  56. Event_Ramadan-First10Days           0.0101\n",
      "\n",
      "============================================================\n",
      "2. CORRELATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Calculating correlations for Breakfast...\n",
      "Top 10 correlations for Breakfast:\n",
      "  59. Tourism_0                           0.3227\n",
      "  60. Tourism_1                           0.3220\n",
      "  57. Event_Ramadan-Last10Days            0.2981\n",
      "   8. IsLast10Ramadan                     0.2981\n",
      "  20. Month_cos                           0.2613\n",
      "  63. Impact_-1                           0.2030\n",
      "  64. Impact_0                            0.2024\n",
      "  58. Event_Ramadan-Middle                0.1949\n",
      "   4. IsRamadan                           0.1931\n",
      "   2. CheckTotal                          0.1663\n",
      "\n",
      "Calculating correlations for Dinner...\n",
      "Top 10 correlations for Dinner:\n",
      "  60. Tourism_1                           0.3613\n",
      "  59. Tourism_0                           0.3602\n",
      "   8. IsLast10Ramadan                     0.2997\n",
      "  57. Event_Ramadan-Last10Days            0.2997\n",
      "  20. Month_cos                           0.2654\n",
      "  64. Impact_0                            0.2437\n",
      "  63. Impact_-1                           0.2425\n",
      "  58. Event_Ramadan-Middle                0.2018\n",
      "   4. IsRamadan                           0.1936\n",
      "   2. CheckTotal                          0.1755\n",
      "\n",
      "Calculating correlations for Lunch...\n",
      "Top 10 correlations for Lunch:\n",
      "  59. Tourism_0                           0.2359\n",
      "  60. Tourism_1                           0.2349\n",
      "  20. Month_cos                           0.2053\n",
      "  57. Event_Ramadan-Last10Days            0.1980\n",
      "   8. IsLast10Ramadan                     0.1980\n",
      "  63. Impact_-1                           0.1580\n",
      "  64. Impact_0                            0.1570\n",
      "  58. Event_Ramadan-Middle                0.1461\n",
      "   4. IsRamadan                           0.1373\n",
      "   2. CheckTotal                          0.1164\n",
      "\n",
      "============================================================\n",
      "3. FEATURE VARIANCE ANALYSIS\n",
      "============================================================\n",
      "Top 10 features by variance:\n",
      "  27. Event_Dubai-Food-Festival           5.3811\n",
      "  16. IsFoodFestival                      4.7036\n",
      "  48. Event_Pre-Dubai-Food-Festival       4.5913\n",
      "  54. Event_Pre-Ramadan-Late              2.7304\n",
      "  53. Event_Pre-Ramadan-Early             2.5581\n",
      "  56. Event_Ramadan-First10Days           2.4402\n",
      "   6. IsPreRamadan                        2.3749\n",
      "  63. Impact_-1                           1.7677\n",
      "  58. Event_Ramadan-Middle                1.6013\n",
      "  59. Tourism_0                           1.5332\n",
      "\n",
      "Bottom 10 features by variance (potentially redundant):\n",
      "  51. Event_Pre-Flag-Day                  0.0000\n",
      "  45. Event_Pre-Commemoration-Day         0.0000\n",
      "  55. Event_Pre-UAE-National-Day          0.0000\n",
      "  32. Event_Flag-Day                      0.0000\n",
      "  11. IsNationalDay                       0.0000\n",
      "  36. Event_Post-Dubai-Airshow            0.0000\n",
      "  40. Event_Post-GITEX-Technology-Week    0.0000\n",
      "  25. Meal_Lunch                          0.0000\n",
      "  24. Meal_Dinner                         0.0000\n",
      "  23. Meal_Breakfast                      0.0000\n",
      "\n",
      "============================================================\n",
      "4. COMBINED FEATURE RANKING\n",
      "============================================================\n",
      "\n",
      "Combined analysis for Breakfast:\n",
      "Top 15 features by combined ranking:\n",
      "   3. Tourism_0                           Rank: 4.7\n",
      "   6. Impact_-1                           Rank: 6.7\n",
      "   7. Event_Ramadan-First10Days           Rank: 8.7\n",
      "  10. Impact_0                            Rank: 9.7\n",
      "   9. Event_Ramadan-Last10Days            Rank: 10.0\n",
      "   1. Month_cos                           Rank: 10.3\n",
      "   8. IsLast10Ramadan                     Rank: 10.3\n",
      "  16. Event_Ramadan-Middle                Rank: 11.0\n",
      "  11. Tourism_1                           Rank: 11.7\n",
      "   2. CheckTotal                          Rank: 13.7\n",
      "  22. IsFoodFestival                      Rank: 14.0\n",
      "  19. Event_Dubai-Food-Festival           Rank: 14.3\n",
      "  20. Event_Normal                        Rank: 15.0\n",
      "  12. Event_Pre-Ramadan-Late              Rank: 15.0\n",
      "  30. Event_Pre-Ramadan-Early             Rank: 16.3\n",
      "\n",
      "Combined analysis for Dinner:\n",
      "Top 15 features by combined ranking:\n",
      "   4. Impact_-1                           Rank: 6.3\n",
      "   7. Tourism_0                           Rank: 6.3\n",
      "   6. Impact_0                            Rank: 8.0\n",
      "   2. Tourism_1                           Rank: 8.3\n",
      "   9. Event_Ramadan-First10Days           Rank: 9.3\n",
      "  12. Event_Ramadan-Middle                Rank: 9.7\n",
      "   1. Month_cos                           Rank: 10.3\n",
      "  14. Event_Ramadan-Last10Days            Rank: 12.0\n",
      "  18. Event_Pre-Ramadan-Early             Rank: 12.7\n",
      "  16. IsLast10Ramadan                     Rank: 12.7\n",
      "   3. CheckTotal                          Rank: 14.0\n",
      "  22. Event_Dubai-Food-Festival           Rank: 14.3\n",
      "  17. Event_Pre-Ramadan-Late              Rank: 14.7\n",
      "  21. Event_Normal                        Rank: 15.3\n",
      "  25. IsFoodFestival                      Rank: 15.3\n",
      "\n",
      "Combined analysis for Lunch:\n",
      "Top 15 features by combined ranking:\n",
      "   5. Tourism_0                           Rank: 5.3\n",
      "   6. Impact_-1                           Rank: 6.7\n",
      "   7. Event_Ramadan-Middle                Rank: 8.0\n",
      "   8. Impact_0                            Rank: 9.0\n",
      "   1. Month_cos                           Rank: 9.7\n",
      "  10. Event_Ramadan-First10Days           Rank: 10.3\n",
      "   9. Tourism_1                           Rank: 11.0\n",
      "  12. Event_Pre-Ramadan-Late              Rank: 11.7\n",
      "  15. IsLast10Ramadan                     Rank: 13.0\n",
      "  16. IsPreRamadan                        Rank: 13.0\n",
      "   2. CheckTotal                          Rank: 13.7\n",
      "  18. IsPreEvent                          Rank: 14.3\n",
      "  20. Event_Normal                        Rank: 15.0\n",
      "  21. Event_Dubai-Food-Festival           Rank: 15.0\n",
      "  22. Event_Pre-Ramadan-Early             Rank: 15.0\n",
      "\n",
      "============================================================\n",
      "5. FEATURE CATEGORY ANALYSIS\n",
      "============================================================\n",
      "Feature count by category:\n",
      "  Events         : 33 features\n",
      "  Event_Flags    : 15 features\n",
      "  Cyclical       :  4 features\n",
      "  Tourism        :  4 features\n",
      "  Core           :  3 features\n",
      "  Meal_Period    :  3 features\n",
      "  Revenue_Impact :  3 features\n",
      "\n",
      "============================================================\n",
      "6. FEATURE REDUCTION RECOMMENDATIONS\n",
      "============================================================\n",
      "RECOMMENDED FEATURE SET (22 features):\n",
      "Features ranked in top 20 for at least one revenue stream:\n",
      "   1. IsRamadan                           ★★★ (3/3)\n",
      "   2. Tourism_0                           ★★★ (3/3)\n",
      "   3. Month_cos                           ★★★ (3/3)\n",
      "   4. IsFoodFestival                      ★★★ (3/3)\n",
      "   5. Event_Ramadan-Middle                ★★★ (3/3)\n",
      "   6. IsPreRamadan                        ★★★ (3/3)\n",
      "   7. Impact_0                            ★★★ (3/3)\n",
      "   8. DayOfWeek_sin                       ★★★ (3/3)\n",
      "   9. Event_Normal                        ★★★ (3/3)\n",
      "  10. Event_Pre-Ramadan-Late              ★★★ (3/3)\n",
      "  11. IsLast10Ramadan                     ★★★ (3/3)\n",
      "  12. Impact_-1                           ★★★ (3/3)\n",
      "  13. Event_Pre-Ramadan-Early             ★★★ (3/3)\n",
      "  14. Event_Ramadan-First10Days           ★★★ (3/3)\n",
      "  15. Tourism_1                           ★★★ (3/3)\n",
      "  16. Event_Dubai-Food-Festival           ★★★ (3/3)\n",
      "  17. CheckTotal                          ★★★ (3/3)\n",
      "  18. Event_Ramadan-Last10Days            ★★★ (3/3)\n",
      "  19. is_zero                             ★★ (2/3)\n",
      "  20. IsPreEvent                          ★★ (2/3)\n",
      "  21. DayOfWeek_cos                       ★ (1/3)\n",
      "  22. Event_Pre-Dubai-Food-Festival       ★ (1/3)\n",
      "\n",
      "============================================================\n",
      "7. FEATURES LIKELY TO REMOVE\n",
      "============================================================\n",
      "Features consistently ranked low:\n",
      "  - Event_Dubai-Airshow                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Dubai-Shopping-Festival       (bottom 15 for 3/3 targets)\n",
      "  - Event_Dubai-Summer-Surprises        (bottom 15 for 3/3 targets)\n",
      "  - Event_Eid-Adha                      (bottom 15 for 3/3 targets)\n",
      "  - Event_Flag-Day                      (bottom 15 for 3/3 targets)\n",
      "  - Event_GITEX-Technology-Week         (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Dubai-Airshow            (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Eid-Adha                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Flag-Day                 (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-GITEX-Technology-Week    (bottom 15 for 3/3 targets)\n",
      "  - Event_Post-Ramadan-Recovery         (bottom 15 for 3/3 targets)\n",
      "  - IsNationalDay                       (bottom 15 for 3/3 targets)\n",
      "  - Meal_Breakfast                      (bottom 15 for 3/3 targets)\n",
      "  - Meal_Dinner                         (bottom 15 for 3/3 targets)\n",
      "  - Meal_Lunch                          (bottom 15 for 3/3 targets)\n",
      "\n",
      "============================================================\n",
      "8. SUMMARY AND RECOMMENDATIONS\n",
      "============================================================\n",
      "Current model:\n",
      "  Total features: 65\n",
      "  Training sequences: 360\n",
      "  Samples per feature: 38.8\n",
      "\n",
      "Recommended optimization:\n",
      "  Keep top features: 22\n",
      "  Remove features: 43\n",
      "  Reduction: 66.2%\n",
      "  New samples per feature: 114.5\n",
      "\n",
      "TOP 22 FEATURES TO KEEP:\n",
      "   1. IsRamadan\n",
      "   2. Tourism_0\n",
      "   3. Month_cos\n",
      "   4. IsFoodFestival\n",
      "   5. Event_Ramadan-Middle\n",
      "   6. IsPreRamadan\n",
      "   7. Impact_0\n",
      "   8. DayOfWeek_sin\n",
      "   9. Event_Normal\n",
      "  10. Event_Pre-Ramadan-Late\n",
      "  11. IsLast10Ramadan\n",
      "  12. Impact_-1\n",
      "  13. Event_Pre-Ramadan-Early\n",
      "  14. Event_Ramadan-First10Days\n",
      "  15. Tourism_1\n",
      "  16. Event_Dubai-Food-Festival\n",
      "  17. CheckTotal\n",
      "  18. Event_Ramadan-Last10Days\n",
      "  19. is_zero\n",
      "  20. IsPreEvent\n",
      "  21. DayOfWeek_cos\n",
      "  22. Event_Pre-Dubai-Food-Festival\n",
      "\n",
      "============================================================\n",
      "9. CODE TO IMPLEMENT FEATURE REDUCTION\n",
      "============================================================\n",
      "# Copy this code to implement feature reduction:\n",
      "recommended_features = ['IsRamadan', 'Tourism_0', 'Month_cos', 'IsFoodFestival', 'Event_Ramadan-Middle', 'IsPreRamadan', 'Impact_0', 'DayOfWeek_sin', 'Event_Normal', 'Event_Pre-Ramadan-Late', 'IsLast10Ramadan', 'Impact_-1', 'Event_Pre-Ramadan-Early', 'Event_Ramadan-First10Days', 'Tourism_1', 'Event_Dubai-Food-Festival', 'CheckTotal', 'Event_Ramadan-Last10Days', 'is_zero', 'IsPreEvent', 'DayOfWeek_cos', 'Event_Pre-Dubai-Food-Festival']\n",
      "\n",
      "# Get indices of recommended features\n",
      "feature_indices = [feature_cols.index(f) for f in recommended_features if f in feature_cols]\n",
      "print(f'Found {len(feature_indices)} feature indices')\n",
      "\n",
      "# Reduce your dataset\n",
      "X_train_reduced = X_train[:, :, feature_indices]\n",
      "X_test_reduced = X_test[:, :, feature_indices]\n",
      "feature_cols_reduced = [feature_cols[i] for i in feature_indices]\n",
      "print(f'Reduced from {X_train.shape} to {X_train_reduced.shape}')\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "Use the recommended_features list above to reduce your feature set.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# FEATURE RELEVANCE ANALYSIS - Fixed version for your CNN-LSTM notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE RELEVANCE ANALYSIS FOR CNN-LSTM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Debug the data shapes first\n",
    "print(f\"Analyzing data shapes:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Check if we're using normalized targets\n",
    "if 'y_train_norm' in locals():\n",
    "    print(\"Using normalized targets for analysis\")\n",
    "    y_analysis = y_train_norm\n",
    "else:\n",
    "    print(\"Using original targets for analysis\")\n",
    "    y_analysis = y_train\n",
    "\n",
    "print(f\"Analysis target shape: {y_analysis.shape}\")\n",
    "\n",
    "# Flatten the data for analysis - CORRECTED VERSION\n",
    "print(\"\\nFlattening sequences for feature analysis...\")\n",
    "\n",
    "# Use only training data for consistency\n",
    "X_flat = X_train.reshape(-1, X_train.shape[-1])  # (train_sequences*timesteps, features)\n",
    "y_flat = y_analysis.reshape(-1, y_analysis.shape[-1])  # (train_sequences*timesteps, targets)\n",
    "\n",
    "print(f\"Flattened shapes: X_flat={X_flat.shape}, y_flat={y_flat.shape}\")\n",
    "\n",
    "# Verify shapes match\n",
    "if X_flat.shape[0] != y_flat.shape[0]:\n",
    "    print(f\"ERROR: Shape mismatch detected!\")\n",
    "    print(f\"X_flat samples: {X_flat.shape[0]}\")\n",
    "    print(f\"y_flat samples: {y_flat.shape[0]}\")\n",
    "    \n",
    "    # Fix by using the minimum length\n",
    "    min_samples = min(X_flat.shape[0], y_flat.shape[0])\n",
    "    X_flat = X_flat[:min_samples]\n",
    "    y_flat = y_flat[:min_samples]\n",
    "    print(f\"Fixed shapes: X_flat={X_flat.shape}, y_flat={y_flat.shape}\")\n",
    "\n",
    "target_names = ['Breakfast', 'Dinner', 'Lunch']\n",
    "\n",
    "# 1. RANDOM FOREST FEATURE IMPORTANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. RANDOM FOREST FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_results = {}\n",
    "for i, target_name in enumerate(target_names):\n",
    "    print(f\"\\nAnalyzing {target_name} revenue...\")\n",
    "    \n",
    "    try:\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=50,  # Reduced for speed\n",
    "            random_state=42, \n",
    "            max_depth=8,\n",
    "            min_samples_split=20,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X_flat, y_flat[:, i])\n",
    "        \n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        rf_results[target_name] = rf_importance\n",
    "        \n",
    "        print(f\"Top 10 features for {target_name}:\")\n",
    "        for idx, row in rf_importance.head(10).iterrows():\n",
    "            print(f\"  {idx+1:2d}. {row['feature']:<35} {row['importance']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Random Forest for {target_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 2. CORRELATION ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "corr_results = {}\n",
    "for i, target_name in enumerate(target_names):\n",
    "    print(f\"\\nCalculating correlations for {target_name}...\")\n",
    "    \n",
    "    try:\n",
    "        correlations = []\n",
    "        for j in range(len(feature_cols)):\n",
    "            # Handle any NaN values\n",
    "            feature_vals = X_flat[:, j]\n",
    "            target_vals = y_flat[:, i]\n",
    "            \n",
    "            # Remove NaN pairs\n",
    "            mask = ~(np.isnan(feature_vals) | np.isnan(target_vals))\n",
    "            if mask.sum() > 10:  # Need at least 10 valid pairs\n",
    "                corr, _ = pearsonr(feature_vals[mask], target_vals[mask])\n",
    "                correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "            else:\n",
    "                correlations.append(0)\n",
    "        \n",
    "        corr_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'abs_correlation': correlations\n",
    "        }).sort_values('abs_correlation', ascending=False)\n",
    "        \n",
    "        corr_results[target_name] = corr_importance\n",
    "        \n",
    "        print(f\"Top 10 correlations for {target_name}:\")\n",
    "        for idx, row in corr_importance.head(10).iterrows():\n",
    "            print(f\"  {idx+1:2d}. {row['feature']:<35} {row['abs_correlation']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in correlation analysis for {target_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 3. SIMPLE VARIANCE ANALYSIS (Alternative to LASSO)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. FEATURE VARIANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate feature variance (low variance = less informative)\n",
    "feature_variances = np.var(X_flat, axis=0)\n",
    "variance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'variance': feature_variances\n",
    "}).sort_values('variance', ascending=False)\n",
    "\n",
    "print(\"Top 10 features by variance:\")\n",
    "for idx, row in variance_df.head(10).iterrows():\n",
    "    print(f\"  {idx+1:2d}. {row['feature']:<35} {row['variance']:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 10 features by variance (potentially redundant):\")\n",
    "for idx, row in variance_df.tail(10).iterrows():\n",
    "    print(f\"  {idx+1:2d}. {row['feature']:<35} {row['variance']:.4f}\")\n",
    "\n",
    "# 4. COMBINED RANKING ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. COMBINED FEATURE RANKING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "combined_rankings = {}\n",
    "\n",
    "for target_name in target_names:\n",
    "    if target_name not in rf_results or target_name not in corr_results:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nCombined analysis for {target_name}:\")\n",
    "    \n",
    "    # Get rankings from each method\n",
    "    rf_rank = rf_results[target_name].reset_index(drop=True)\n",
    "    rf_rank['rf_rank'] = rf_rank.index + 1\n",
    "    \n",
    "    corr_rank = corr_results[target_name].reset_index(drop=True)\n",
    "    corr_rank['corr_rank'] = corr_rank.index + 1\n",
    "    \n",
    "    # Add variance ranking\n",
    "    var_rank = variance_df.reset_index(drop=True)\n",
    "    var_rank['var_rank'] = var_rank.index + 1\n",
    "    \n",
    "    # Merge all rankings\n",
    "    combined = rf_rank[['feature', 'rf_rank', 'importance']].merge(\n",
    "        corr_rank[['feature', 'corr_rank', 'abs_correlation']], on='feature'\n",
    "    ).merge(\n",
    "        var_rank[['feature', 'var_rank', 'variance']], on='feature'\n",
    "    )\n",
    "    \n",
    "    # Calculate average rank (lower is better)\n",
    "    combined['avg_rank'] = combined[['rf_rank', 'corr_rank', 'var_rank']].mean(axis=1)\n",
    "    combined = combined.sort_values('avg_rank')\n",
    "    \n",
    "    combined_rankings[target_name] = combined\n",
    "    \n",
    "    print(\"Top 15 features by combined ranking:\")\n",
    "    for idx, row in combined.head(15).iterrows():\n",
    "        print(f\"  {idx+1:2d}. {row['feature']:<35} Rank: {row['avg_rank']:.1f}\")\n",
    "\n",
    "# 5. FEATURE CATEGORY ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. FEATURE CATEGORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def categorize_feature(feature_name):\n",
    "    if feature_name.startswith('Meal_'):\n",
    "        return 'Meal_Period'\n",
    "    elif feature_name.startswith('Event_'):\n",
    "        return 'Events'\n",
    "    elif feature_name.startswith('Tourism_'):\n",
    "        return 'Tourism'\n",
    "    elif feature_name.startswith('Impact_'):\n",
    "        return 'Revenue_Impact'\n",
    "    elif feature_name.endswith(('_sin', '_cos')):\n",
    "        return 'Cyclical'\n",
    "    elif feature_name.startswith('Is'):\n",
    "        return 'Event_Flags'\n",
    "    else:\n",
    "        return 'Core'\n",
    "\n",
    "# Analyze feature categories\n",
    "all_features_df = pd.DataFrame({'feature': feature_cols})\n",
    "all_features_df['category'] = all_features_df['feature'].apply(categorize_feature)\n",
    "\n",
    "category_counts = all_features_df['category'].value_counts()\n",
    "print(\"Feature count by category:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category:<15}: {count:2d} features\")\n",
    "\n",
    "# 6. OVERALL FEATURE RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. FEATURE REDUCTION RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find top features across all targets\n",
    "all_top_features = set()\n",
    "top_counts = Counter()\n",
    "\n",
    "for target_name in target_names:\n",
    "    if target_name in combined_rankings:\n",
    "        top_20 = combined_rankings[target_name].head(20)['feature'].tolist()\n",
    "        all_top_features.update(top_20)\n",
    "        for feature in top_20:\n",
    "            top_counts[feature] += 1\n",
    "\n",
    "# Sort by how many targets find this feature important\n",
    "recommended_features = sorted(all_top_features, key=lambda x: top_counts[x], reverse=True)\n",
    "\n",
    "print(f\"RECOMMENDED FEATURE SET ({len(recommended_features)} features):\")\n",
    "print(\"Features ranked in top 20 for at least one revenue stream:\")\n",
    "\n",
    "for i, feature in enumerate(recommended_features[:25], 1):  # Show top 25\n",
    "    count = top_counts[feature]\n",
    "    stars = \"★\" * count\n",
    "    print(f\"  {i:2d}. {feature:<35} {stars} ({count}/3)\")\n",
    "\n",
    "# 7. FEATURES TO REMOVE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. FEATURES LIKELY TO REMOVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Features with consistently low rankings\n",
    "bottom_features = []\n",
    "for target_name in target_names:\n",
    "    if target_name in combined_rankings:\n",
    "        bottom_15 = combined_rankings[target_name].tail(15)['feature'].tolist()\n",
    "        bottom_features.extend(bottom_15)\n",
    "\n",
    "# Features that appear in bottom rankings multiple times\n",
    "bottom_counts = Counter(bottom_features)\n",
    "likely_redundant = [feature for feature, count in bottom_counts.items() if count >= 2]\n",
    "\n",
    "print(\"Features consistently ranked low:\")\n",
    "for feature in sorted(likely_redundant)[:15]:  # Show top 15 candidates for removal\n",
    "    count = bottom_counts[feature]\n",
    "    print(f\"  - {feature:<35} (bottom 15 for {count}/3 targets)\")\n",
    "\n",
    "# 8. SUMMARY AND RECOMMENDATIONS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8. SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Current model:\")\n",
    "print(f\"  Total features: {len(feature_cols)}\")\n",
    "print(f\"  Training sequences: {X_train.shape[0]}\")\n",
    "print(f\"  Samples per feature: {X_flat.shape[0] / len(feature_cols):.1f}\")\n",
    "\n",
    "optimal_features = min(30, len(recommended_features))\n",
    "print(f\"\\nRecommended optimization:\")\n",
    "print(f\"  Keep top features: {optimal_features}\")\n",
    "print(f\"  Remove features: {len(feature_cols) - optimal_features}\")\n",
    "print(f\"  Reduction: {(len(feature_cols) - optimal_features) / len(feature_cols) * 100:.1f}%\")\n",
    "print(f\"  New samples per feature: {X_flat.shape[0] / optimal_features:.1f}\")\n",
    "\n",
    "print(f\"\\nTOP {optimal_features} FEATURES TO KEEP:\")\n",
    "for i, feature in enumerate(recommended_features[:optimal_features], 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# 9. CREATE FEATURE SELECTION CODE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"9. CODE TO IMPLEMENT FEATURE REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "top_features_list = recommended_features[:optimal_features]\n",
    "print(\"# Copy this code to implement feature reduction:\")\n",
    "print(f\"recommended_features = {top_features_list}\")\n",
    "print(\"\\n# Get indices of recommended features\")\n",
    "print(\"feature_indices = [feature_cols.index(f) for f in recommended_features if f in feature_cols]\")\n",
    "print(f\"print(f'Found {{len(feature_indices)}} feature indices')\")\n",
    "print(\"\\n# Reduce your dataset\")\n",
    "print(\"X_train_reduced = X_train[:, :, feature_indices]\")\n",
    "print(\"X_test_reduced = X_test[:, :, feature_indices]\")\n",
    "print(\"feature_cols_reduced = [feature_cols[i] for i in feature_indices]\")\n",
    "print(f\"print(f'Reduced from {{X_train.shape}} to {{X_train_reduced.shape}}')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"Use the recommended_features list above to reduce your feature set.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
