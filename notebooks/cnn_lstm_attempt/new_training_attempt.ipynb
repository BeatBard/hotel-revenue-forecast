{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a8b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: LOADING AND EXPLORING RAW DATA\n",
      "================================================================================\n",
      "‚úì Loaded data for 9 revenue centers\n",
      "‚úì Total records: 13,122\n",
      "‚úì Date range: 2023-01-01 00:00:00 to 2024-04-30 00:00:00\n",
      "‚úì Total days: 486\n",
      "\n",
      "üìä Data Quality Overview:\n",
      "  Missing values: 0\n",
      "  Duplicate records: 0\n",
      "  Zero revenue records: 4108\n",
      "  Negative revenue records: 0\n",
      "\n",
      "üí∞ Revenue Distribution (RAW - NO MANIPULATION):\n",
      "  Overall range: $0.00 - $138400.00\n",
      "  Mean: $2499.77\n",
      "  Median: $225.00\n",
      "  Std: $6740.14\n",
      "\n",
      "üçΩÔ∏è By Meal Period (RAW):\n",
      "  Breakfast:\n",
      "    Range: $0.00 - $35640.00\n",
      "    Mean: $2202.28, CV: 2.595\n",
      "  Dinner:\n",
      "    Range: $0.00 - $138400.00\n",
      "    Mean: $4029.75, CV: 2.197\n",
      "  Lunch:\n",
      "    Range: $0.00 - $100760.00\n",
      "    Mean: $1267.29, CV: 3.645\n",
      "\n",
      "üìÖ Temporal Patterns:\n",
      "  Monthly revenue variation (CV): 2.526\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: PROPER DATA LOADING & INITIAL EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_explore_data():\n",
    "    \"\"\"\n",
    "    Load all revenue center data and perform comprehensive exploration\n",
    "    WITHOUT any premature transformations or outlier removal\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING AND EXPLORING RAW DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load all revenue centers\n",
    "    revenue_centers = []\n",
    "    for i in range(1, 10):\n",
    "        df = pd.read_csv(f'../revenue_center_data/RevenueCenter_{i}_data.csv')\n",
    "        df['RevenueCenterID'] = i\n",
    "        revenue_centers.append(df)\n",
    "    \n",
    "    # Combine all revenue centers\n",
    "    df_all = pd.concat(revenue_centers, ignore_index=True)\n",
    "    df_all['Date'] = pd.to_datetime(df_all['Date'])\n",
    "    \n",
    "    print(f\"‚úì Loaded data for {len(revenue_centers)} revenue centers\")\n",
    "    print(f\"‚úì Total records: {len(df_all):,}\")\n",
    "    print(f\"‚úì Date range: {df_all['Date'].min()} to {df_all['Date'].max()}\")\n",
    "    print(f\"‚úì Total days: {(df_all['Date'].max() - df_all['Date'].min()).days + 1}\")\n",
    "    \n",
    "    # Basic data quality checks\n",
    "    print(f\"\\nüìä Data Quality Overview:\")\n",
    "    print(f\"  Missing values: {df_all.isnull().sum().sum()}\")\n",
    "    print(f\"  Duplicate records: {df_all.duplicated().sum()}\")\n",
    "    print(f\"  Zero revenue records: {(df_all['CheckTotal'] == 0).sum()}\")\n",
    "    print(f\"  Negative revenue records: {(df_all['CheckTotal'] < 0).sum()}\")\n",
    "    \n",
    "    # Revenue distribution analysis (HONEST - no manipulation)\n",
    "    print(f\"\\nüí∞ Revenue Distribution (RAW - NO MANIPULATION):\")\n",
    "    print(f\"  Overall range: ${df_all['CheckTotal'].min():.2f} - ${df_all['CheckTotal'].max():.2f}\")\n",
    "    print(f\"  Mean: ${df_all['CheckTotal'].mean():.2f}\")\n",
    "    print(f\"  Median: ${df_all['CheckTotal'].median():.2f}\")\n",
    "    print(f\"  Std: ${df_all['CheckTotal'].std():.2f}\")\n",
    "    \n",
    "    # Per meal period analysis\n",
    "    print(f\"\\nüçΩÔ∏è By Meal Period (RAW):\")\n",
    "    for meal in ['Breakfast', 'Dinner', 'Lunch']:\n",
    "        meal_data = df_all[df_all['MealPeriod'] == meal]['CheckTotal']\n",
    "        print(f\"  {meal}:\")\n",
    "        print(f\"    Range: ${meal_data.min():.2f} - ${meal_data.max():.2f}\")\n",
    "        print(f\"    Mean: ${meal_data.mean():.2f}, CV: {meal_data.std()/meal_data.mean():.3f}\")\n",
    "    \n",
    "    # Temporal patterns\n",
    "    print(f\"\\nüìÖ Temporal Patterns:\")\n",
    "    df_all['DayOfYear'] = df_all['Date'].dt.dayofyear\n",
    "    df_all['WeekOfYear'] = df_all['Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Check for seasonal patterns\n",
    "    monthly_revenue = df_all.groupby(df_all['Date'].dt.month)['CheckTotal'].agg(['mean', 'std'])\n",
    "    print(f\"  Monthly revenue variation (CV): {(monthly_revenue['std'] / monthly_revenue['mean']).mean():.3f}\")\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "# Execute data loading\n",
    "df_raw = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d44c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: PROPER TEMPORAL TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "üìÖ Temporal Split Strategy:\n",
      "  Training period: 2023-01-01 00:00:00 to 2024-01-30 00:00:00\n",
      "  Testing period: 2024-01-31 00:00:00 to 2024-04-30 00:00:00\n",
      "  Test period: 3 months (91 days)\n",
      "\n",
      "‚úÖ Split Results:\n",
      "  Training records: 10,665\n",
      "  Testing records: 2,457\n",
      "  Training days: 395\n",
      "  Testing days: 91\n",
      "  ‚úÖ No temporal leakage confirmed\n",
      "  Training revenue: $23,956,016.15\n",
      "  Testing revenue: $8,846,026.55\n",
      "  Revenue ratio (test/train): 0.369\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: PROPER TEMPORAL TRAIN-TEST SPLIT (NO DATA LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "def create_proper_temporal_split(df, test_months=3):\n",
    "    \"\"\"\n",
    "    Create proper temporal split for time series forecasting\n",
    "    CRITICAL: No future data in training, no overlap between train/test\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 2: PROPER TEMPORAL TRAIN-TEST SPLIT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort by date to ensure temporal order\n",
    "    df_sorted = df.sort_values(['Date', 'RevenueCenterID', 'MealPeriod']).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split date (last N months for testing)\n",
    "    max_date = df_sorted['Date'].max()\n",
    "    split_date = max_date - pd.DateOffset(months=test_months)\n",
    "    \n",
    "    print(f\"üìÖ Temporal Split Strategy:\")\n",
    "    print(f\"  Training period: {df_sorted['Date'].min()} to {split_date}\")\n",
    "    print(f\"  Testing period: {split_date + pd.Timedelta(days=1)} to {max_date}\")\n",
    "    print(f\"  Test period: {test_months} months ({(max_date - split_date).days} days)\")\n",
    "    \n",
    "    # Create temporal split\n",
    "    train_data = df_sorted[df_sorted['Date'] <= split_date].copy()\n",
    "    test_data = df_sorted[df_sorted['Date'] > split_date].copy()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Split Results:\")\n",
    "    print(f\"  Training records: {len(train_data):,}\")\n",
    "    print(f\"  Testing records: {len(test_data):,}\")\n",
    "    print(f\"  Training days: {train_data['Date'].nunique()}\")\n",
    "    print(f\"  Testing days: {test_data['Date'].nunique()}\")\n",
    "    \n",
    "    # Verify no temporal leakage\n",
    "    assert train_data['Date'].max() < test_data['Date'].min(), \"‚ùå TEMPORAL LEAKAGE DETECTED!\"\n",
    "    print(f\"  ‚úÖ No temporal leakage confirmed\")\n",
    "    \n",
    "    # Check data balance\n",
    "    train_revenue = train_data['CheckTotal'].sum()\n",
    "    test_revenue = test_data['CheckTotal'].sum()\n",
    "    print(f\"  Training revenue: ${train_revenue:,.2f}\")\n",
    "    print(f\"  Testing revenue: ${test_revenue:,.2f}\")\n",
    "    print(f\"  Revenue ratio (test/train): {test_revenue/train_revenue:.3f}\")\n",
    "    \n",
    "    return train_data, test_data, split_date\n",
    "\n",
    "# Execute temporal split\n",
    "train_df, test_df, split_date = create_proper_temporal_split(df_raw, test_months=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311356c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: ROBUST FEATURE ENGINEERING\n",
      "================================================================================\n",
      "üîß Engineering features for training data...\n",
      "üîß Applying same transformations to test data...\n",
      "üè∑Ô∏è Creating one-hot encodings...\n",
      "‚úÖ Feature Engineering Complete:\n",
      "  Total features available: 30\n",
      "  Training samples: 10665\n",
      "  Test samples: 2457\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: CONSERVATIVE FEATURE ENGINEERING (NO DATA LEAKAGE)\n",
    "# ============================================================================\n",
    "\n",
    "def create_robust_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Create features using ONLY training data statistics\n",
    "    Apply same transformations to test data (no leakage)\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 3: ROBUST FEATURE ENGINEERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    def engineer_features(df, is_training=True, train_stats=None):\n",
    "        \"\"\"Engineer features for a dataframe\"\"\"\n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # 1. TEMPORAL FEATURES (no leakage)\n",
    "        df_features['Month_sin'] = np.sin(2 * np.pi * df_features['Month'] / 12)\n",
    "        df_features['Month_cos'] = np.cos(2 * np.pi * df_features['Month'] / 12)\n",
    "        df_features['DayOfWeek_sin'] = np.sin(2 * np.pi * df_features['DayOfWeek'] / 7)\n",
    "        df_features['DayOfWeek_cos'] = np.cos(2 * np.pi * df_features['DayOfWeek'] / 7)\n",
    "        \n",
    "        # 2. LAG FEATURES (using only past data)\n",
    "        if is_training:\n",
    "            # Calculate lag statistics from training data only\n",
    "            lag_stats = {}\n",
    "            for center in df_features['RevenueCenterID'].unique():\n",
    "                for meal in df_features['MealPeriod'].unique():\n",
    "                    mask = (df_features['RevenueCenterID'] == center) & (df_features['MealPeriod'] == meal)\n",
    "                    center_meal_data = df_features[mask].sort_values('Date')\n",
    "                    \n",
    "                    # 7-day and 30-day rolling averages (using only past data)\n",
    "                    center_meal_data['Revenue_7d_avg'] = center_meal_data['CheckTotal'].rolling(window=7, min_periods=1).mean().shift(1)\n",
    "                    center_meal_data['Revenue_30d_avg'] = center_meal_data['CheckTotal'].rolling(window=30, min_periods=1).mean().shift(1)\n",
    "                    \n",
    "                    # Store statistics for test data\n",
    "                    lag_stats[(center, meal)] = {\n",
    "                        'mean_7d': center_meal_data['Revenue_7d_avg'].mean(),\n",
    "                        'mean_30d': center_meal_data['Revenue_30d_avg'].mean()\n",
    "                    }\n",
    "                    \n",
    "                    # Update main dataframe\n",
    "                    df_features.loc[mask, 'Revenue_7d_avg'] = center_meal_data['Revenue_7d_avg'].fillna(center_meal_data['CheckTotal'].mean())\n",
    "                    df_features.loc[mask, 'Revenue_30d_avg'] = center_meal_data['Revenue_30d_avg'].fillna(center_meal_data['CheckTotal'].mean())\n",
    "            \n",
    "            return df_features, lag_stats\n",
    "        else:\n",
    "            # Apply training statistics to test data\n",
    "            for center in df_features['RevenueCenterID'].unique():\n",
    "                for meal in df_features['MealPeriod'].unique():\n",
    "                    mask = (df_features['RevenueCenterID'] == center) & (df_features['MealPeriod'] == meal)\n",
    "                    if (center, meal) in train_stats:\n",
    "                        df_features.loc[mask, 'Revenue_7d_avg'] = train_stats[(center, meal)]['mean_7d']\n",
    "                        df_features.loc[mask, 'Revenue_30d_avg'] = train_stats[(center, meal)]['mean_30d']\n",
    "                    else:\n",
    "                        # Fallback for missing combinations\n",
    "                        df_features.loc[mask, 'Revenue_7d_avg'] = df_features.loc[mask, 'CheckTotal'].mean()\n",
    "                        df_features.loc[mask, 'Revenue_30d_avg'] = df_features.loc[mask, 'CheckTotal'].mean()\n",
    "            \n",
    "            return df_features\n",
    "    \n",
    "    # Engineer features for training data\n",
    "    print(\"üîß Engineering features for training data...\")\n",
    "    train_features, lag_stats = engineer_features(train_df, is_training=True)\n",
    "    \n",
    "    # Apply same transformations to test data (no leakage)\n",
    "    print(\"üîß Applying same transformations to test data...\")\n",
    "    test_features = engineer_features(test_df, is_training=False, train_stats=lag_stats)\n",
    "    \n",
    "    # 3. ONE-HOT ENCODING (consistent across train/test)\n",
    "    print(\"üè∑Ô∏è Creating one-hot encodings...\")\n",
    "    \n",
    "    # Get all possible values from training data\n",
    "    categorical_cols = ['MealPeriod', 'IslamicPeriod', 'TourismIntensity', 'RevenueImpact']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # Get unique values from training data only\n",
    "        unique_values = train_features[col].unique()\n",
    "        \n",
    "        # Create one-hot encoding for both datasets\n",
    "        for value in unique_values:\n",
    "            new_col = f\"{col}_{value}\"\n",
    "            train_features[new_col] = (train_features[col] == value).astype(int)\n",
    "            test_features[new_col] = (test_features[col] == value).astype(int)\n",
    "    \n",
    "    # 4. FEATURE SELECTION (based on domain knowledge)\n",
    "    feature_columns = [\n",
    "        # Temporal features\n",
    "        'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos',\n",
    "        # Event features (most important)\n",
    "        'IsRamadan', 'IsEid', 'IsPreRamadan', 'IsPostRamadan', 'IsLast10Ramadan',\n",
    "        'IsDSF', 'IsSummerEvent', 'IsNationalDay', 'IsNewYear', 'IsMarathon',\n",
    "        'IsGITEX', 'IsFoodFestival', 'IsPreEvent', 'IsPostEvent',\n",
    "        # Lag features\n",
    "        'Revenue_7d_avg', 'Revenue_30d_avg',\n",
    "        # One-hot encoded features\n",
    "    ] + [col for col in train_features.columns if any(cat in col for cat in categorical_cols) and col.endswith(('_Breakfast', '_Dinner', '_Lunch', '_Normal', '_High', '_Low', '_Boost', '_Neutral', '_Decrease'))]\n",
    "    \n",
    "    # Ensure all features exist in both datasets\n",
    "    available_features = [col for col in feature_columns if col in train_features.columns and col in test_features.columns]\n",
    "    \n",
    "    print(f\"‚úÖ Feature Engineering Complete:\")\n",
    "    print(f\"  Total features available: {len(available_features)}\")\n",
    "    print(f\"  Training samples: {len(train_features)}\")\n",
    "    print(f\"  Test samples: {len(test_features)}\")\n",
    "    \n",
    "    return train_features[available_features + ['CheckTotal', 'Date', 'RevenueCenterID', 'MealPeriod']], \\\n",
    "           test_features[available_features + ['CheckTotal', 'Date', 'RevenueCenterID', 'MealPeriod']], \\\n",
    "           available_features\n",
    "\n",
    "# Execute feature engineering\n",
    "train_engineered, test_engineered, feature_list = create_robust_features(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4dc95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: CREATING PROPER TIME SERIES SEQUENCES\n",
      "================================================================================\n",
      "üìä Preparing sequences for Revenue Center 1\n",
      "  Training days: 395\n",
      "  Test days: 91\n",
      "  Features per day: 90\n",
      "  Target streams: 3\n",
      "\n",
      "‚úÖ Sequence Creation Complete:\n",
      "  X_train shape: (368, 21, 90)\n",
      "  y_train shape: (368, 7, 3)\n",
      "  X_test shape: (64, 21, 90)\n",
      "  y_test shape: (64, 7, 3)\n",
      "\n",
      "üìä Model Complexity Analysis:\n",
      "  Training sequences: 368\n",
      "  Input dimensions: 21 days √ó 90 features = 1890\n",
      "  Samples per feature: 4.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: PROPER SEQUENCE CREATION FOR TIME SERIES\n",
    "# ============================================================================\n",
    "\n",
    "def create_sequences_proper(train_df, test_df, feature_cols, \n",
    "                           lookback_days=21, forecast_days=7, \n",
    "                           revenue_center_id=1):\n",
    "    \"\"\"\n",
    "    Create sequences for CNN-LSTM with proper temporal structure\n",
    "    Focus on single revenue center first for simplicity\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 4: CREATING PROPER TIME SERIES SEQUENCES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    def prepare_center_data(df, center_id):\n",
    "        \"\"\"Prepare data for a specific revenue center\"\"\"\n",
    "        center_data = df[df['RevenueCenterID'] == center_id].copy()\n",
    "        \n",
    "        # Pivot to daily format (3 meals per day)\n",
    "        daily_data = center_data.pivot_table(\n",
    "            index='Date',\n",
    "            columns='MealPeriod',\n",
    "            values=['CheckTotal'] + feature_cols,\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Flatten column names\n",
    "        daily_data.columns = [f\"{col[1]}_{col[0]}\" for col in daily_data.columns]\n",
    "        daily_data = daily_data.reset_index()\n",
    "        \n",
    "        # Sort by date\n",
    "        daily_data = daily_data.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        return daily_data\n",
    "    \n",
    "    # Prepare data for specified revenue center\n",
    "    print(f\"üìä Preparing sequences for Revenue Center {revenue_center_id}\")\n",
    "    \n",
    "    train_daily = prepare_center_data(train_df, revenue_center_id)\n",
    "    test_daily = prepare_center_data(test_df, revenue_center_id)\n",
    "    \n",
    "    print(f\"  Training days: {len(train_daily)}\")\n",
    "    print(f\"  Test days: {len(test_daily)}\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    feature_cols_daily = [col for col in train_daily.columns if col != 'Date' and not col.startswith(('Breakfast_CheckTotal', 'Dinner_CheckTotal', 'Lunch_CheckTotal'))]\n",
    "    target_cols = ['Breakfast_CheckTotal', 'Dinner_CheckTotal', 'Lunch_CheckTotal']\n",
    "    \n",
    "    print(f\"  Features per day: {len(feature_cols_daily)}\")\n",
    "    print(f\"  Target streams: {len(target_cols)}\")\n",
    "    \n",
    "    # Create sequences\n",
    "    def create_sequences_from_daily(daily_data, features_cols, target_cols, lookback, forecast):\n",
    "        \"\"\"Create sequences from daily data\"\"\"\n",
    "        features = daily_data[features_cols].values\n",
    "        targets = daily_data[target_cols].values\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(lookback, len(daily_data) - forecast + 1):\n",
    "            # Features: past 'lookback' days\n",
    "            X.append(features[i-lookback:i])\n",
    "            \n",
    "            # Targets: next 'forecast' days\n",
    "            y.append(targets[i:i+forecast])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Create training sequences\n",
    "    X_train, y_train = create_sequences_from_daily(\n",
    "        train_daily, feature_cols_daily, target_cols, lookback_days, forecast_days\n",
    "    )\n",
    "    \n",
    "    # Create test sequences\n",
    "    X_test, y_test = create_sequences_from_daily(\n",
    "        test_daily, feature_cols_daily, target_cols, lookback_days, forecast_days\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sequence Creation Complete:\")\n",
    "    print(f\"  X_train shape: {X_train.shape}\")  # (sequences, lookback_days, features)\n",
    "    print(f\"  y_train shape: {y_train.shape}\")  # (sequences, forecast_days, revenue_streams)\n",
    "    print(f\"  X_test shape: {X_test.shape}\")\n",
    "    print(f\"  y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Calculate parameters per sample ratio\n",
    "    total_features = X_train.shape[1] * X_train.shape[2]\n",
    "    samples = X_train.shape[0]\n",
    "    \n",
    "    print(f\"\\nüìä Model Complexity Analysis:\")\n",
    "    print(f\"  Training sequences: {samples}\")\n",
    "    print(f\"  Input dimensions: {X_train.shape[1]} days √ó {X_train.shape[2]} features = {total_features}\")\n",
    "    print(f\"  Samples per feature: {samples / X_train.shape[2]:.1f}\")\n",
    "    \n",
    "    # Store original targets for evaluation\n",
    "    y_train_original = y_train.copy()\n",
    "    y_test_original = y_test.copy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_train_original, y_test_original, feature_cols_daily\n",
    "\n",
    "# Execute sequence creation\n",
    "X_train, X_test, y_train, y_test, y_train_orig, y_test_orig, features_used = create_sequences_proper(\n",
    "    train_engineered, test_engineered, feature_list, \n",
    "    lookback_days=21, forecast_days=7, revenue_center_id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85df32fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 5: CONSERVATIVE NORMALIZATION\n",
      "================================================================================\n",
      "üîß Normalizing features using RobustScaler...\n",
      "  ‚úÖ Features normalized using training data statistics\n",
      "üéØ Normalizing targets conservatively...\n",
      "  Original target ranges (training):\n",
      "    Breakfast: $0.00 - $8210.80 (mean: $808.29)\n",
      "    Dinner: $365.50 - $10052.50 (mean: $2519.56)\n",
      "    Lunch: $0.00 - $4504.00 (mean: $664.09)\n",
      "  üìä Applying log1p transformation to handle revenue ranges...\n",
      "  ‚úÖ Targets normalized using log1p + StandardScaler\n",
      "  Normalized target ranges:\n",
      "    Breakfast: -6.695 - 2.789\n",
      "    Dinner: -3.674 - 3.057\n",
      "    Lunch: -5.813 - 2.122\n",
      "  üíæ Scalers saved for denormalization\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: CONSERVATIVE NORMALIZATION (NO OUTLIER MANIPULATION)\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import joblib\n",
    "\n",
    "def normalize_data_conservatively(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Normalize data conservatively without outlier manipulation\n",
    "    Use robust scaling to handle natural revenue variations\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 5: CONSERVATIVE NORMALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. FEATURE NORMALIZATION using RobustScaler (less sensitive to outliers)\n",
    "    print(\"üîß Normalizing features using RobustScaler...\")\n",
    "    \n",
    "    # Reshape features for scaling\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Fit scaler on training data only\n",
    "    feature_scaler = RobustScaler()\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train_reshaped)\n",
    "    X_test_scaled = feature_scaler.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to original format\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "    \n",
    "    print(f\"  ‚úÖ Features normalized using training data statistics\")\n",
    "    \n",
    "    # 2. TARGET NORMALIZATION (conservative approach)\n",
    "    print(\"üéØ Normalizing targets conservatively...\")\n",
    "    \n",
    "    # Analyze target distribution first\n",
    "    y_train_flat = y_train.reshape(-1, y_train.shape[-1])\n",
    "    y_test_flat = y_test.reshape(-1, y_test.shape[-1])\n",
    "    \n",
    "    print(f\"  Original target ranges (training):\")\n",
    "    revenue_streams = ['Breakfast', 'Dinner', 'Lunch']\n",
    "    for i, stream in enumerate(revenue_streams):\n",
    "        stream_data = y_train_flat[:, i]\n",
    "        print(f\"    {stream}: ${stream_data.min():.2f} - ${stream_data.max():.2f} (mean: ${stream_data.mean():.2f})\")\n",
    "    \n",
    "    # Use log transformation for revenue (handles wide ranges naturally)\n",
    "    print(f\"  üìä Applying log1p transformation to handle revenue ranges...\")\n",
    "    \n",
    "    # Log transform (handles zeros and wide ranges)\n",
    "    y_train_log = np.log1p(y_train)  # log1p handles zeros naturally\n",
    "    y_test_log = np.log1p(y_test)\n",
    "    \n",
    "    # Then apply standard scaling to log-transformed data\n",
    "    y_train_log_reshaped = y_train_log.reshape(-1, y_train_log.shape[-1])\n",
    "    y_test_log_reshaped = y_test_log.reshape(-1, y_test_log.shape[-1])\n",
    "    \n",
    "    target_scaler = StandardScaler()\n",
    "    y_train_normalized = target_scaler.fit_transform(y_train_log_reshaped)\n",
    "    y_test_normalized = target_scaler.transform(y_test_log_reshaped)\n",
    "    \n",
    "    # Reshape back\n",
    "    y_train_normalized = y_train_normalized.reshape(y_train.shape)\n",
    "    y_test_normalized = y_test_normalized.reshape(y_test.shape)\n",
    "    \n",
    "    print(f\"  ‚úÖ Targets normalized using log1p + StandardScaler\")\n",
    "    print(f\"  Normalized target ranges:\")\n",
    "    for i, stream in enumerate(revenue_streams):\n",
    "        stream_data = y_train_normalized[:, :, i].flatten()\n",
    "        print(f\"    {stream}: {stream_data.min():.3f} - {stream_data.max():.3f}\")\n",
    "    \n",
    "    # Save scalers for later denormalization\n",
    "    joblib.dump(feature_scaler, 'feature_scaler_robust.pkl')\n",
    "    joblib.dump(target_scaler, 'target_scaler_conservative.pkl')\n",
    "    \n",
    "    print(f\"  üíæ Scalers saved for denormalization\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_normalized, y_test_normalized\n",
    "\n",
    "def denormalize_predictions_conservative(predictions_normalized, original_targets_for_reference):\n",
    "    \"\"\"\n",
    "    Denormalize predictions using the conservative approach\n",
    "    \"\"\"\n",
    "    # Load scalers\n",
    "    target_scaler = joblib.load('target_scaler_conservative.pkl')\n",
    "    \n",
    "    # Reshape for denormalization\n",
    "    pred_reshaped = predictions_normalized.reshape(-1, predictions_normalized.shape[-1])\n",
    "    \n",
    "    # Inverse standard scaling\n",
    "    pred_log = target_scaler.inverse_transform(pred_reshaped)\n",
    "    \n",
    "    # Inverse log transformation\n",
    "    pred_actual = np.expm1(pred_log)  # expm1 is inverse of log1p\n",
    "    \n",
    "    # Reshape back\n",
    "    pred_actual = pred_actual.reshape(predictions_normalized.shape)\n",
    "    \n",
    "    # Ensure no negative predictions (business constraint)\n",
    "    pred_actual = np.maximum(pred_actual, 1.0)\n",
    "    \n",
    "    return pred_actual\n",
    "\n",
    "# Execute normalization\n",
    "X_train_norm, X_test_norm, y_train_norm, y_test_norm = normalize_data_conservatively(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669359a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 6: BUILDING APPROPRIATELY SIZED MODEL\n",
      "================================================================================\n",
      "üìê Model specifications:\n",
      "  Input shape: (21, 90)\n",
      "  Output shape: (7, 3)\n",
      "  Complexity level: minimal\n",
      "  Training samples: 368\n",
      "  Recommended max parameters: 36\n",
      "\n",
      "üìä Model Architecture:\n",
      "  Actual parameters: 11,685\n",
      "  Parameters per sample: 31.8\n",
      "  Within recommended limit: ‚ùå\n",
      "  ‚ö†Ô∏è WARNING: Model may overfit with current data size\n",
      "\n",
      "üîß Compiling model with conservative settings...\n",
      "  ‚úÖ Model compiled with Huber loss and conservative Adam optimizer\n",
      "\n",
      "üìã Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,336</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         ‚îÇ            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,272</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization_1           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">357</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m16\u001b[0m)         ‚îÇ         \u001b[38;5;34m4,336\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m16\u001b[0m)         ‚îÇ            \u001b[38;5;34m64\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m16\u001b[0m)         ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ         \u001b[38;5;34m6,272\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ batch_normalization_1           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ           \u001b[38;5;34m128\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalization\u001b[0m)            ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ           \u001b[38;5;34m528\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             ‚îÇ           \u001b[38;5;34m357\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ reshape (\u001b[38;5;33mReshape\u001b[0m)               ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m3\u001b[0m)           ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,685</span> (45.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,685\u001b[0m (45.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,589</span> (45.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,589\u001b[0m (45.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: APPROPRIATELY SIZED CNN-LSTM MODEL\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "def build_appropriately_sized_model(input_shape, output_shape, complexity='minimal'):\n",
    "    \"\"\"\n",
    "    Build CNN-LSTM model with appropriate complexity for available data\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 6: BUILDING APPROPRIATELY SIZED MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"üìê Model specifications:\")\n",
    "    print(f\"  Input shape: {input_shape}\")\n",
    "    print(f\"  Output shape: {output_shape}\")\n",
    "    print(f\"  Complexity level: {complexity}\")\n",
    "    \n",
    "    # Calculate recommended model size based on data\n",
    "    samples = X_train_norm.shape[0]\n",
    "    max_params = samples // 10  # Conservative: 10 samples per parameter\n",
    "    \n",
    "    print(f\"  Training samples: {samples}\")\n",
    "    print(f\"  Recommended max parameters: {max_params:,}\")\n",
    "    \n",
    "    if complexity == 'minimal':\n",
    "        # Minimal model for limited data\n",
    "        model = Sequential([\n",
    "            # Single CNN layer for local pattern detection\n",
    "            Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            # Single LSTM layer for temporal dependencies\n",
    "            LSTM(32, return_sequences=False),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Dense layers for prediction\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(np.prod(output_shape), activation='linear'),\n",
    "            Reshape(output_shape)\n",
    "        ])\n",
    "        \n",
    "    elif complexity == 'moderate':\n",
    "        # Moderate model if we have more data\n",
    "        model = Sequential([\n",
    "            # Two CNN layers\n",
    "            Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            \n",
    "            # LSTM layer\n",
    "            LSTM(64, return_sequences=False),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Dense layers\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(np.prod(output_shape), activation='linear'),\n",
    "            Reshape(output_shape)\n",
    "        ])\n",
    "    \n",
    "    # Count actual parameters\n",
    "    model.compile(optimizer='adam', loss='mse')  # Temporary compilation to count params\n",
    "    actual_params = model.count_params()\n",
    "    \n",
    "    print(f\"\\nüìä Model Architecture:\")\n",
    "    print(f\"  Actual parameters: {actual_params:,}\")\n",
    "    print(f\"  Parameters per sample: {actual_params / samples:.1f}\")\n",
    "    print(f\"  Within recommended limit: {'‚úÖ' if actual_params <= max_params else '‚ùå'}\")\n",
    "    \n",
    "    if actual_params > max_params:\n",
    "        print(f\"  ‚ö†Ô∏è WARNING: Model may overfit with current data size\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model_conservatively(model):\n",
    "    \"\"\"\n",
    "    Compile model with conservative settings\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß Compiling model with conservative settings...\")\n",
    "    \n",
    "    # Conservative optimizer settings\n",
    "    optimizer = Adam(\n",
    "        learning_rate=0.001,  # Standard learning rate\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Use Huber loss (robust to outliers)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='huber',  # More robust than MSE\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    print(\"  ‚úÖ Model compiled with Huber loss and conservative Adam optimizer\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and compile model\n",
    "input_shape = (X_train_norm.shape[1], X_train_norm.shape[2])\n",
    "output_shape = (y_train_norm.shape[1], y_train_norm.shape[2])\n",
    "\n",
    "model = build_appropriately_sized_model(input_shape, output_shape, complexity='minimal')\n",
    "model = compile_model_conservatively(model)\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a06aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 7: ROBUST TRAINING SETUP\n",
      "================================================================================\n",
      "üìã Training Configuration:\n",
      "  batch_size: 8\n",
      "  epochs: 100\n",
      "  validation_split: 0.2\n",
      "  shuffle: True\n",
      "  verbose: 1\n",
      "\n",
      "üéØ Callbacks configured:\n",
      "  - Early stopping (patience: 20)\n",
      "  - Learning rate reduction (patience: 10)\n",
      "  - Model checkpointing\n",
      "\n",
      "üöÄ Starting robust training...\n",
      "Epoch 1/100\n",
      "\u001b[1m30/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.5514 - mae: 0.9382\n",
      "Epoch 1: val_loss improved from inf to 0.53404, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.5439 - mae: 0.9292 - val_loss: 0.5340 - val_mae: 0.9181 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4397 - mae: 0.8052\n",
      "Epoch 2: val_loss improved from 0.53404 to 0.52661, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4365 - mae: 0.8014 - val_loss: 0.5266 - val_mae: 0.9093 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3901 - mae: 0.7461\n",
      "Epoch 3: val_loss did not improve from 0.52661\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3870 - mae: 0.7427 - val_loss: 0.5333 - val_mae: 0.9164 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3497 - mae: 0.6992\n",
      "Epoch 4: val_loss did not improve from 0.52661\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3499 - mae: 0.6994 - val_loss: 0.5271 - val_mae: 0.9089 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m31/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3505 - mae: 0.6940\n",
      "Epoch 5: val_loss improved from 0.52661 to 0.52647, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3479 - mae: 0.6915 - val_loss: 0.5265 - val_mae: 0.9081 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3500 - mae: 0.6943\n",
      "Epoch 6: val_loss did not improve from 0.52647\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3485 - mae: 0.6926 - val_loss: 0.5272 - val_mae: 0.9088 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m30/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3299 - mae: 0.6729\n",
      "Epoch 7: val_loss did not improve from 0.52647\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3290 - mae: 0.6718 - val_loss: 0.5274 - val_mae: 0.9089 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m29/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3216 - mae: 0.6545\n",
      "Epoch 8: val_loss improved from 0.52647 to 0.51959, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3217 - mae: 0.6557 - val_loss: 0.5196 - val_mae: 0.8995 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3157 - mae: 0.6569\n",
      "Epoch 9: val_loss improved from 0.51959 to 0.51163, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3158 - mae: 0.6569 - val_loss: 0.5116 - val_mae: 0.8902 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3032 - mae: 0.6415\n",
      "Epoch 10: val_loss improved from 0.51163 to 0.50823, saving model to best_conservative_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3038 - mae: 0.6419 - val_loss: 0.5082 - val_mae: 0.8861 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3082 - mae: 0.6460\n",
      "Epoch 11: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3079 - mae: 0.6453 - val_loss: 0.5102 - val_mae: 0.8886 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m31/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3042 - mae: 0.6385\n",
      "Epoch 12: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3030 - mae: 0.6371 - val_loss: 0.5133 - val_mae: 0.8923 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m30/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2918 - mae: 0.6229\n",
      "Epoch 13: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2927 - mae: 0.6242 - val_loss: 0.5085 - val_mae: 0.8872 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3054 - mae: 0.6365\n",
      "Epoch 14: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3046 - mae: 0.6360 - val_loss: 0.5108 - val_mae: 0.8899 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m27/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3038 - mae: 0.6368\n",
      "Epoch 15: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3023 - mae: 0.6351 - val_loss: 0.5188 - val_mae: 0.8993 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m27/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3056 - mae: 0.6324\n",
      "Epoch 16: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3030 - mae: 0.6316 - val_loss: 0.5173 - val_mae: 0.8970 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m31/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3147 - mae: 0.6490\n",
      "Epoch 17: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3113 - mae: 0.6449 - val_loss: 0.5129 - val_mae: 0.8915 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2922 - mae: 0.6158\n",
      "Epoch 18: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2921 - mae: 0.6160 - val_loss: 0.5348 - val_mae: 0.9167 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3041 - mae: 0.6377\n",
      "Epoch 19: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.3024 - mae: 0.6355 - val_loss: 0.5332 - val_mae: 0.9158 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m28/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2850 - mae: 0.6150\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2853 - mae: 0.6149 - val_loss: 0.5600 - val_mae: 0.9449 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m31/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2840 - mae: 0.6113\n",
      "Epoch 21: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2849 - mae: 0.6125 - val_loss: 0.5379 - val_mae: 0.9210 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2804 - mae: 0.6090\n",
      "Epoch 22: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2815 - mae: 0.6102 - val_loss: 0.5393 - val_mae: 0.9225 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m31/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2763 - mae: 0.6058\n",
      "Epoch 23: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2784 - mae: 0.6077 - val_loss: 0.5398 - val_mae: 0.9233 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2913 - mae: 0.6180\n",
      "Epoch 24: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2903 - mae: 0.6169 - val_loss: 0.5387 - val_mae: 0.9221 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m30/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2825 - mae: 0.6091\n",
      "Epoch 25: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2822 - mae: 0.6087 - val_loss: 0.5575 - val_mae: 0.9423 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2998 - mae: 0.6263\n",
      "Epoch 26: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2974 - mae: 0.6240 - val_loss: 0.5626 - val_mae: 0.9476 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2746 - mae: 0.5996\n",
      "Epoch 27: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2757 - mae: 0.6009 - val_loss: 0.5714 - val_mae: 0.9567 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2767 - mae: 0.6054\n",
      "Epoch 28: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2770 - mae: 0.6057 - val_loss: 0.5734 - val_mae: 0.9589 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2881 - mae: 0.6158\n",
      "Epoch 29: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2881 - mae: 0.6158 - val_loss: 0.5568 - val_mae: 0.9413 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m30/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2764 - mae: 0.6037\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.50823\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2773 - mae: 0.6041 - val_loss: 0.5598 - val_mae: 0.9446 - learning_rate: 5.0000e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\n",
      "‚úÖ Training completed!\n",
      "‚úÖ Best model weights loaded\n",
      "\n",
      "üìà Training Summary:\n",
      "  Epochs completed: 30\n",
      "  Best validation loss: 0.5082\n",
      "  Final training loss: 0.2819\n",
      "  Overfitting check: 0.2263\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: ROBUST TRAINING WITH PROPER VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def setup_robust_training():\n",
    "    \"\"\"\n",
    "    Setup training with proper validation and conservative callbacks\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 7: ROBUST TRAINING SETUP\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Conservative callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,  # Generous patience for small dataset\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=10,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_conservative_model.h5',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Conservative training parameters\n",
    "    training_config = {\n",
    "        'batch_size': 8,  # Small batch size for limited data\n",
    "        'epochs': 100,    # Reasonable max with early stopping\n",
    "        'validation_split': 0.2,  # Use part of training for validation\n",
    "        'shuffle': True,\n",
    "        'verbose': 1\n",
    "    }\n",
    "    \n",
    "    print(f\"üìã Training Configuration:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Callbacks configured:\")\n",
    "    print(f\"  - Early stopping (patience: 20)\")\n",
    "    print(f\"  - Learning rate reduction (patience: 10)\")\n",
    "    print(f\"  - Model checkpointing\")\n",
    "    \n",
    "    return callbacks, training_config\n",
    "\n",
    "def train_model_robustly(model, X_train, y_train, callbacks, config):\n",
    "    \"\"\"\n",
    "    Train model with robust methodology\n",
    "    \"\"\"\n",
    "    print(\"\\nüöÄ Starting robust training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=config['batch_size'],\n",
    "        epochs=config['epochs'],\n",
    "        validation_split=config['validation_split'],\n",
    "        callbacks=callbacks,\n",
    "        shuffle=config['shuffle'],\n",
    "        verbose=config['verbose']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Training completed!\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_weights('best_conservative_model.h5')\n",
    "    print(\"‚úÖ Best model weights loaded\")\n",
    "    \n",
    "    # Training summary\n",
    "    final_epoch = len(history.history['loss'])\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    \n",
    "    print(f\"\\nüìà Training Summary:\")\n",
    "    print(f\"  Epochs completed: {final_epoch}\")\n",
    "    print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  Final training loss: {final_train_loss:.4f}\")\n",
    "    print(f\"  Overfitting check: {abs(final_train_loss - best_val_loss):.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Execute training\n",
    "callbacks, training_config = setup_robust_training()\n",
    "history = train_model_robustly(model, X_train_norm, y_train_norm, callbacks, training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cc4f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables in your namespace:\n",
      "Model variables: ['ModelCheckpoint', 'build_appropriately_sized_model', 'compile_model_conservatively', 'evaluate_model_auto_detect', 'evaluate_model_corrected', 'evaluate_model_honestly', 'model', 'train_model_robustly']\n",
      "Test variables: ['X_test', 'X_test_norm', 'test_df', 'test_engineered', 'y_test', 'y_test_norm', 'y_test_orig']\n",
      "Scaler variables: ['RobustScaler', 'StandardScaler']\n",
      "‚úì Found model: model\n",
      "‚úì Found X_test: X_test\n",
      "‚úì Found y_test: y_test_orig\n",
      "\n",
      "üöÄ Running auto-detected evaluation...\n",
      "================================================================================\n",
      "CORRECTED MODEL EVALUATION (AUTO-DETECTING SETUP)\n",
      "================================================================================\n",
      "üîç Auto-detecting your setup...\n",
      "  Model type: <class 'keras.src.models.sequential.Sequential'>\n",
      "  X_test shape: (64, 21, 90)\n",
      "  y_test_original shape: (64, 7, 3)\n",
      "üîÆ Generating predictions...\n",
      "  Raw prediction shape: (64, 7, 3)\n",
      "  Raw prediction range: -0.6010 - 0.8989\n",
      "üîÑ Denormalizing predictions...\n",
      "  ‚ö†Ô∏è  Predictions seem to be in different scale than actuals\n",
      "  Prediction mean: 0.23, Actual mean: 1701.30\n",
      "  Assuming predictions are normalized, attempting auto-denormalization...\n",
      "  ‚úÖ Predictions denormalized\n",
      "  Final prediction range: $665.99 - $3249.73\n",
      "\n",
      "üîç Data Quality Checks:\n",
      "  True values range: $0.00 - $9657.00\n",
      "  Any NaN in true values: False\n",
      "  Any NaN in predictions: False\n",
      "  Any negative true values: False\n",
      "  Any negative predictions: False\n",
      "\n",
      "üìä CORRECTED EVALUATION METRICS:\n",
      "  Clean samples: 1344 / 1344\n",
      "\n",
      "üéØ Overall Performance:\n",
      "  MAE: $1462.56\n",
      "  Robust MAPE: 76.4% (weighted, capped, >$100 only)\n",
      "  SMAPE: 83.2%\n",
      "  Correlation: 0.091\n",
      "  Revenue Accuracy: 76.8%\n",
      "\n",
      "üçΩÔ∏è Per-Stream Performance:\n",
      "  Breakfast:\n",
      "    MAE: $1355.14\n",
      "    Robust MAPE: 93.0%\n",
      "    SMAPE: 86.8%\n",
      "    Correlation: -0.022\n",
      "    Revenue Accuracy: 45.4%\n",
      "  Dinner:\n",
      "    MAE: $1612.37\n",
      "    Robust MAPE: 53.8%\n",
      "    SMAPE: 56.5%\n",
      "    Correlation: -0.006\n",
      "    Revenue Accuracy: 76.4%\n",
      "  Lunch:\n",
      "    MAE: $1420.17\n",
      "    Robust MAPE: 135.1%\n",
      "    SMAPE: 106.2%\n",
      "    Correlation: -0.009\n",
      "    Revenue Accuracy: -44.1%\n",
      "\n",
      "üíº Business Metrics:\n",
      "  Total revenue error: 23.2%\n",
      "  True total: $2,286,551.25\n",
      "  Predicted total: $2,816,346.00\n",
      "\n",
      "üìà Prediction Accuracy Distribution:\n",
      "  Within 10% error: 6.2%\n",
      "  Within 20% error: 12.4%\n",
      "  Within 30% error: 18.8%\n",
      "  Within 50% error: 31.5%\n",
      "\n",
      "üéØ Model Reliability Assessment:\n",
      "  ‚ùå NEEDS IMPROVEMENT (> 70% SMAPE)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECTED EVALUATION CODE - ADAPTS TO YOUR CURRENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_robust_mape(y_true, y_pred, min_threshold=50):\n",
    "    \"\"\"\n",
    "    Calculate MAPE that handles small values properly\n",
    "    \"\"\"\n",
    "    # Only calculate MAPE for values above threshold\n",
    "    mask = y_true >= min_threshold\n",
    "    \n",
    "    if np.sum(mask) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    true_filtered = y_true[mask]\n",
    "    pred_filtered = y_pred[mask]\n",
    "    \n",
    "    # Calculate MAPE with proper handling\n",
    "    mape_values = np.abs((true_filtered - pred_filtered) / true_filtered) * 100\n",
    "    \n",
    "    # Cap extreme values at 500% to prevent outliers from dominating\n",
    "    mape_values = np.minimum(mape_values, 500)\n",
    "    \n",
    "    # Weight by revenue size (larger revenues get more weight)\n",
    "    weights = true_filtered / np.sum(true_filtered)\n",
    "    weighted_mape = np.sum(mape_values * weights)\n",
    "    \n",
    "    return weighted_mape\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error - more robust than MAPE\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    mask = denominator > 1e-8\n",
    "    smape_values = np.zeros_like(numerator)\n",
    "    smape_values[mask] = (numerator[mask] / denominator[mask]) * 100\n",
    "    \n",
    "    return np.mean(smape_values)\n",
    "\n",
    "def denormalize_predictions_safe(y_pred_norm, scaler_or_stats):\n",
    "    \"\"\"\n",
    "    Safely denormalize predictions using either a scaler object or manual stats\n",
    "    \"\"\"\n",
    "    if hasattr(scaler_or_stats, 'inverse_transform'):\n",
    "        # It's a scaler object\n",
    "        original_shape = y_pred_norm.shape\n",
    "        y_pred_flat = y_pred_norm.reshape(-1, original_shape[-1])\n",
    "        y_pred_denorm_flat = scaler_or_stats.inverse_transform(y_pred_flat)\n",
    "        y_pred_denorm = y_pred_denorm_flat.reshape(original_shape)\n",
    "    elif isinstance(scaler_or_stats, dict):\n",
    "        # It's manual stats (mean, std)\n",
    "        y_pred_denorm = y_pred_norm * scaler_or_stats['std'] + scaler_or_stats['mean']\n",
    "    else:\n",
    "        # No scaling applied\n",
    "        y_pred_denorm = y_pred_norm\n",
    "    \n",
    "    return y_pred_denorm\n",
    "\n",
    "def evaluate_model_auto_detect(model, X_test, y_test_original, \n",
    "                              scaler_or_stats=None,\n",
    "                              stream_names=['Breakfast', 'Dinner', 'Lunch']):\n",
    "    \"\"\"\n",
    "    Auto-detecting evaluation that works with your current setup\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CORRECTED MODEL EVALUATION (AUTO-DETECTING SETUP)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Auto-detect variable types and shapes\n",
    "    print(\"üîç Auto-detecting your setup...\")\n",
    "    print(f\"  Model type: {type(model)}\")\n",
    "    print(f\"  X_test shape: {X_test.shape}\")\n",
    "    print(f\"  y_test_original shape: {y_test_original.shape}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"üîÆ Generating predictions...\")\n",
    "    y_pred_normalized = model.predict(X_test, verbose=0)\n",
    "    print(f\"  Raw prediction shape: {y_pred_normalized.shape}\")\n",
    "    print(f\"  Raw prediction range: {y_pred_normalized.min():.4f} - {y_pred_normalized.max():.4f}\")\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    print(\"üîÑ Denormalizing predictions...\")\n",
    "    if scaler_or_stats is not None:\n",
    "        y_pred_denorm = denormalize_predictions_safe(y_pred_normalized, scaler_or_stats)\n",
    "        print(f\"  Using provided scaler/stats for denormalization\")\n",
    "    else:\n",
    "        # Try to auto-detect if denormalization is needed\n",
    "        pred_mean = np.mean(y_pred_normalized)\n",
    "        actual_mean = np.mean(y_test_original)\n",
    "        \n",
    "        if abs(pred_mean - actual_mean) > actual_mean * 0.5:  # Predictions are in different scale\n",
    "            print(f\"  ‚ö†Ô∏è  Predictions seem to be in different scale than actuals\")\n",
    "            print(f\"  Prediction mean: {pred_mean:.2f}, Actual mean: {actual_mean:.2f}\")\n",
    "            print(f\"  Assuming predictions are normalized, attempting auto-denormalization...\")\n",
    "            \n",
    "            # Simple denormalization using actual data stats\n",
    "            actual_mean = np.mean(y_test_original)\n",
    "            actual_std = np.std(y_test_original)\n",
    "            y_pred_denorm = y_pred_normalized * actual_std + actual_mean\n",
    "        else:\n",
    "            y_pred_denorm = y_pred_normalized\n",
    "            print(f\"  Predictions appear to be in same scale as actuals\")\n",
    "    \n",
    "    # Ensure no negative predictions (revenue can't be negative)\n",
    "    y_pred_denorm = np.maximum(y_pred_denorm, 1.0)\n",
    "    \n",
    "    print(f\"  ‚úÖ Predictions denormalized\")\n",
    "    print(f\"  Final prediction range: ${y_pred_denorm.min():.2f} - ${y_pred_denorm.max():.2f}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(f\"\\nüîç Data Quality Checks:\")\n",
    "    print(f\"  True values range: ${y_test_original.min():.2f} - ${y_test_original.max():.2f}\")\n",
    "    print(f\"  Any NaN in true values: {np.isnan(y_test_original).any()}\")\n",
    "    print(f\"  Any NaN in predictions: {np.isnan(y_pred_denorm).any()}\")\n",
    "    print(f\"  Any negative true values: {(y_test_original < 0).any()}\")\n",
    "    print(f\"  Any negative predictions: {(y_pred_denorm < 0).any()}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # CORRECTED METRICS CALCULATION\n",
    "    # =====================================\n",
    "    \n",
    "    print(\"\\nüìä CORRECTED EVALUATION METRICS:\")\n",
    "    \n",
    "    # Flatten for overall metrics\n",
    "    y_true_flat = y_test_original.reshape(-1)\n",
    "    y_pred_flat = y_pred_denorm.reshape(-1)\n",
    "    \n",
    "    # Remove any remaining NaN/inf values\n",
    "    mask = np.isfinite(y_true_flat) & np.isfinite(y_pred_flat)\n",
    "    y_true_clean = y_true_flat[mask]\n",
    "    y_pred_clean = y_pred_flat[mask]\n",
    "    \n",
    "    print(f\"  Clean samples: {len(y_true_clean)} / {len(y_true_flat)}\")\n",
    "    \n",
    "    # Overall Performance\n",
    "    overall_mae = np.mean(np.abs(y_true_clean - y_pred_clean))\n",
    "    overall_rmse = np.sqrt(np.mean((y_true_clean - y_pred_clean) ** 2))\n",
    "    overall_corr = np.corrcoef(y_true_clean, y_pred_clean)[0, 1]\n",
    "    \n",
    "    # CORRECTED MAPE - Only for meaningful values\n",
    "    overall_mape_robust = calculate_robust_mape(y_true_clean, y_pred_clean, min_threshold=100)\n",
    "    overall_smape = calculate_smape(y_true_clean, y_pred_clean)\n",
    "    \n",
    "    # Revenue accuracy (business-meaningful)\n",
    "    total_true = np.sum(y_true_clean)\n",
    "    total_pred = np.sum(y_pred_clean)\n",
    "    revenue_accuracy = 100 * (1 - abs(total_true - total_pred) / total_true)\n",
    "    \n",
    "    print(f\"\\nüéØ Overall Performance:\")\n",
    "    print(f\"  MAE: ${overall_mae:.2f}\")\n",
    "    if not np.isnan(overall_mape_robust):\n",
    "        print(f\"  Robust MAPE: {overall_mape_robust:.1f}% (weighted, capped, >$100 only)\")\n",
    "    else:\n",
    "        print(f\"  Robust MAPE: N/A (insufficient high-value samples)\")\n",
    "    print(f\"  SMAPE: {overall_smape:.1f}%\")\n",
    "    print(f\"  Correlation: {overall_corr:.3f}\")\n",
    "    print(f\"  Revenue Accuracy: {revenue_accuracy:.1f}%\")\n",
    "    \n",
    "    # Per-stream evaluation with corrected metrics\n",
    "    print(f\"\\nüçΩÔ∏è Per-Stream Performance:\")\n",
    "    stream_results = {}\n",
    "    \n",
    "    for i, stream in enumerate(stream_names):\n",
    "        if y_test_original.ndim == 3:\n",
    "            stream_true = y_test_original[:, :, i].flatten()\n",
    "            stream_pred = y_pred_denorm[:, :, i].flatten()\n",
    "        else:\n",
    "            # Handle 2D case\n",
    "            stream_true = y_test_original[:, i].flatten()\n",
    "            stream_pred = y_pred_denorm[:, i].flatten()\n",
    "        \n",
    "        # Clean data\n",
    "        mask = np.isfinite(stream_true) & np.isfinite(stream_pred)\n",
    "        stream_true_clean = stream_true[mask]\n",
    "        stream_pred_clean = stream_pred[mask]\n",
    "        \n",
    "        # Calculate corrected metrics\n",
    "        stream_mae = np.mean(np.abs(stream_true_clean - stream_pred_clean))\n",
    "        stream_mape_robust = calculate_robust_mape(stream_true_clean, stream_pred_clean, min_threshold=50)\n",
    "        stream_smape = calculate_smape(stream_true_clean, stream_pred_clean)\n",
    "        stream_corr = np.corrcoef(stream_true_clean, stream_pred_clean)[0, 1]\n",
    "        \n",
    "        # Revenue accuracy for this stream\n",
    "        stream_total_true = np.sum(stream_true_clean)\n",
    "        stream_total_pred = np.sum(stream_pred_clean)\n",
    "        stream_revenue_acc = 100 * (1 - abs(stream_total_true - stream_total_pred) / stream_total_true)\n",
    "        \n",
    "        stream_results[stream] = {\n",
    "            'MAE': stream_mae,\n",
    "            'MAPE_Robust': stream_mape_robust,\n",
    "            'SMAPE': stream_smape,\n",
    "            'Correlation': stream_corr,\n",
    "            'Revenue_Accuracy': stream_revenue_acc,\n",
    "            'Sample_Count': len(stream_true_clean),\n",
    "            'Value_Range': f\"${stream_true_clean.min():.0f}-${stream_true_clean.max():.0f}\"\n",
    "        }\n",
    "        \n",
    "        print(f\"  {stream}:\")\n",
    "        print(f\"    MAE: ${stream_mae:.2f}\")\n",
    "        if not np.isnan(stream_mape_robust):\n",
    "            print(f\"    Robust MAPE: {stream_mape_robust:.1f}%\")\n",
    "        else:\n",
    "            print(f\"    Robust MAPE: N/A (low values)\")\n",
    "        print(f\"    SMAPE: {stream_smape:.1f}%\")\n",
    "        print(f\"    Correlation: {stream_corr:.3f}\")\n",
    "        print(f\"    Revenue Accuracy: {stream_revenue_acc:.1f}%\")\n",
    "    \n",
    "    # Business Metrics\n",
    "    print(f\"\\nüíº Business Metrics:\")\n",
    "    print(f\"  Total revenue error: {100 - revenue_accuracy:.1f}%\")\n",
    "    print(f\"  True total: ${total_true:,.2f}\")\n",
    "    print(f\"  Predicted total: ${total_pred:,.2f}\")\n",
    "    \n",
    "    # Prediction Accuracy Distribution (with capped errors)\n",
    "    percentage_errors = np.abs((y_true_clean - y_pred_clean) / (y_true_clean + 1e-8)) * 100\n",
    "    percentage_errors_capped = np.minimum(percentage_errors, 200)  # Cap at 200%\n",
    "    \n",
    "    print(f\"\\nüìà Prediction Accuracy Distribution:\")\n",
    "    print(f\"  Within 10% error: {(percentage_errors_capped <= 10).mean() * 100:.1f}%\")\n",
    "    print(f\"  Within 20% error: {(percentage_errors_capped <= 20).mean() * 100:.1f}%\")\n",
    "    print(f\"  Within 30% error: {(percentage_errors_capped <= 30).mean() * 100:.1f}%\")\n",
    "    print(f\"  Within 50% error: {(percentage_errors_capped <= 50).mean() * 100:.1f}%\")\n",
    "    \n",
    "    # Model Assessment\n",
    "    print(f\"\\nüéØ Model Reliability Assessment:\")\n",
    "    if overall_smape < 30:\n",
    "        assessment = \"‚úÖ EXCELLENT (< 30% SMAPE)\"\n",
    "    elif overall_smape < 50:\n",
    "        assessment = \"‚úÖ GOOD (< 50% SMAPE)\"\n",
    "    elif overall_smape < 70:\n",
    "        assessment = \"‚ö†Ô∏è MODERATE (< 70% SMAPE)\"\n",
    "    else:\n",
    "        assessment = \"‚ùå NEEDS IMPROVEMENT (> 70% SMAPE)\"\n",
    "    \n",
    "    print(f\"  {assessment}\")\n",
    "    \n",
    "    return {\n",
    "        'overall_mae': overall_mae,\n",
    "        'overall_mape_robust': overall_mape_robust,\n",
    "        'overall_smape': overall_smape,\n",
    "        'overall_correlation': overall_corr,\n",
    "        'revenue_accuracy': revenue_accuracy,\n",
    "        'stream_results': stream_results,\n",
    "        'predictions': y_pred_denorm,\n",
    "        'actuals': y_test_original,\n",
    "        'assessment': assessment\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE - REPLACE YOUR CURRENT EVALUATION CALL\n",
    "# ============================================================================\n",
    "\n",
    "# Check what variables you have available in your notebook\n",
    "print(\"Available variables in your namespace:\")\n",
    "available_vars = [var for var in dir() if not var.startswith('_')]\n",
    "model_vars = [var for var in available_vars if 'model' in var.lower()]\n",
    "test_vars = [var for var in available_vars if 'test' in var.lower()]\n",
    "scaler_vars = [var for var in available_vars if 'scaler' in var.lower()]\n",
    "\n",
    "print(f\"Model variables: {model_vars}\")\n",
    "print(f\"Test variables: {test_vars}\")\n",
    "print(f\"Scaler variables: {scaler_vars}\")\n",
    "\n",
    "# Try to auto-detect your variables and run evaluation\n",
    "try:\n",
    "    # Common variable name patterns\n",
    "    possible_models = ['model', 'best_model', 'trained_model', 'cnn_lstm_model']\n",
    "    possible_x_test = ['X_test', 'X_test_norm', 'X_test_normalized', 'test_X']\n",
    "    possible_y_test = ['y_test_orig', 'y_test_original', 'y_test_actual', 'test_y']\n",
    "    possible_scalers = ['target_scaler', 'y_scaler', 'scaler', 'output_scaler']\n",
    "    \n",
    "    # Find available variables\n",
    "    model_var = None\n",
    "    x_test_var = None\n",
    "    y_test_var = None\n",
    "    scaler_var = None\n",
    "    \n",
    "    for var in possible_models:\n",
    "        if var in globals():\n",
    "            model_var = globals()[var]\n",
    "            print(f\"‚úì Found model: {var}\")\n",
    "            break\n",
    "    \n",
    "    for var in possible_x_test:\n",
    "        if var in globals():\n",
    "            x_test_var = globals()[var]\n",
    "            print(f\"‚úì Found X_test: {var}\")\n",
    "            break\n",
    "    \n",
    "    for var in possible_y_test:\n",
    "        if var in globals():\n",
    "            y_test_var = globals()[var]\n",
    "            print(f\"‚úì Found y_test: {var}\")\n",
    "            break\n",
    "    \n",
    "    for var in possible_scalers:\n",
    "        if var in globals():\n",
    "            scaler_var = globals()[var]\n",
    "            print(f\"‚úì Found scaler: {var}\")\n",
    "            break\n",
    "    \n",
    "    # Run evaluation with detected variables\n",
    "    if model_var is not None and x_test_var is not None and y_test_var is not None:\n",
    "        print(\"\\nüöÄ Running auto-detected evaluation...\")\n",
    "        evaluation_results = evaluate_model_auto_detect(\n",
    "            model=model_var,\n",
    "            X_test=x_test_var,\n",
    "            y_test_original=y_test_var,\n",
    "            scaler_or_stats=scaler_var,  # Can be None\n",
    "            stream_names=['Breakfast', 'Dinner', 'Lunch']\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n‚ùå Could not auto-detect all required variables\")\n",
    "        print(\"Please run manually with your variable names:\")\n",
    "        print(\"\"\"\n",
    "        evaluation_results = evaluate_model_auto_detect(\n",
    "            model=YOUR_MODEL_VARIABLE,\n",
    "            X_test=YOUR_X_TEST_VARIABLE,\n",
    "            y_test_original=YOUR_Y_TEST_VARIABLE,\n",
    "            scaler_or_stats=YOUR_SCALER_OR_None,\n",
    "            stream_names=['Breakfast', 'Dinner', 'Lunch']\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Auto-detection failed: {e}\")\n",
    "    print(\"Please run manually with your specific variable names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39124d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
